[
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License.",
    "crumbs": [
      "License"
    ]
  },
  {
    "objectID": "parallel-python.html",
    "href": "parallel-python.html",
    "title": "Parallel processing in Python",
    "section": "",
    "text": "Python provides a variety of functionality for parallelization, including threaded operations (in particular for linear algebra), parallel looping and map statements, and parallelization across multiple machines. For the CPU, this material focuses on Python’s ipyparallel package and JAX, with some discussion of Dask and Ray. For the GPU, the material focuses on PyTorch and JAX, with a bit of discussion of CuPy.\nNote that all of the looping-based functionality discussed here applies only if the iterations/loops of your calculations can be done completely separately and do not depend on one another. This scenario is called an embarrassingly parallel computation. So coding up the evolution of a time series or a Markov chain is not possible using these tools. However, bootstrapping, random forests, simulation studies, cross-validation and many other statistical methods can be handled in this way.\n\n\n\n\n\nThe BLAS is the library of basic linear algebra operations (written in Fortran or C). A fast BLAS can greatly speed up linear algebra relative to the default BLAS on a machine. Some fast BLAS libraries are\n\nIntel’s MKL; may be available for educational use for free\nOpenBLAS; open source and free\nvecLib for Macs; provided with your Mac\n\nIn addition to being fast when used on a single core, all of these BLAS libraries are threaded - if your computer has multiple cores and there are free resources, your linear algebra will use multiple cores, provided your installed Python is linked against the threaded BLAS installed on your machine.\nTo use a fast, threaded BLAS, one approach is to use the Anaconda/Miniconda Python distribution. When you install numpy and scipy, these should be automatically linked against a fast, threaded BLAS (MKL). More generally, simply installing numpy from PyPI should make use of OpenBLAS.\n\n\n\nThreading in Python is limited to linear algebra (provided Python is linked against a threaded BLAS, except if using Dask or JAX or various other packages). Python has something called the Global Interpreter Lock that interferes with threading in Python (but not in threaded linear algebra packages called by Python).\nHere’s some linear algebra in Python that will use threading if numpy is linked against a threaded BLAS, though I don’t compare the timing for different numbers of threads here.\n\nimport numpy as np\nn = 5000\nx = np.random.normal(0, 1, size=(n, n))\nx = x.T @ x\nU = np.linalg.cholesky(x)\n\nIf you watch the Python process via the top command, you should see CPU usage above 100% if Python is linking to a threaded BLAS.\n\n\n\nIn general, threaded code will detect the number of cores available on a machine and make use of them. However, you can also explicitly control the number of threads available to a process.\nFor most threaded code (that based on the openMP protocol), the number of threads can be set by setting the OMP_NUM_THREADS environment variable. Note that under some circumstances you may need to use VECLIB_MAXIMUM_THREADS if on an (older, Intel-based) Mac or MKL_NUM_THREADS if numpy/scipy are linked against MKL.\nFor example, to set it for four threads in bash, do this before starting your Python session.\nexport OMP_NUM_THREADS=4\nAlternatively, you can set OMP_NUM_THREADS as you invoke your job, e.g.,\nOMP_NUM_THREADS=4 python job.py &gt; job.out\n\n\n\n\n\n\n\n\nFirst we’ll cover IPython Parallel (i.e., the ipyparallel package) functionality, which allows one to parallelize on a single machine (discussed here) or across multiple machines (see next section). In later sections, I’ll discuss other packages that can be used for parallelization.\nFirst we need to start our workers. As of ipyparallel version 7, we can start the workers from within Python.\n\n## In newer versions of ipyparallel (v. 7 and later)\nimport ipyparallel as ipp\n# Check the version:\nipp.__version__\nn = 4\ncluster = ipp.Cluster(n = n)\nc = cluster.start_and_connect_sync()\n\nStarting 4 engines with &lt;class 'ipyparallel.cluster.launcher.LocalEngineSetLauncher'&gt;\n\n\n\n\n\n\n\n\nLet’s verify that things seem set up ok and we can interact with all our workers:\n\n## Check that we have the number of workers expected:\nc.ids\n\n[0, 1, 2, 3]\n\n\n\n## Set up a direct view to interface with all the workers\ndview = c[:]\ndview\n\n&lt;DirectView [0, 1, 2, 3]&gt;\n\n\n\n## Set blocking so that we wait for the result of the parallel execution\ndview.block = True \ndview.apply(lambda : \"Hello, World\")\n\n['Hello, World', 'Hello, World', 'Hello, World', 'Hello, World']\n\n\ndview stands for a ‘direct view’, which is an interface to our cluster that allows us to ‘manually’ send tasks to the workers.\n\n\n\nNow let’s see an example of how we can use our workers to run code in parallel.\nWe’ll carry out a statistics/machine learning prediction method (random forest regression) with leave-one-out cross-validation, parallelizing over different held out data.\nFirst let’s set up packages, data and our main function on the workers:\n\ndview.execute('from sklearn.ensemble import RandomForestRegressor as rfr')\ndview.execute('import numpy as np')\n\ndef looFit(index, Ylocal, Xlocal):\n    rf = rfr(n_estimators=100)\n    fitted = rf.fit(np.delete(Xlocal, index, axis = 0), np.delete(Ylocal, index))\n    pred = rf.predict(np.array([Xlocal[index, :]]))\n    return(pred[0])\n\nimport numpy as np\nnp.random.seed(0)\nn = 200\np = 20\nX = np.random.normal(0, 1, size = (n, p))\nY = X[: , 0] + pow(abs(X[:,1] * X[:,2]), 0.5) + X[:,1] - X[:,2] + \\\n    np.random.normal(0, 1, n)\n\nmydict = dict(X = X, Y = Y, looFit = looFit)\ndview.push(mydict)\n\n[None, None, None, None]\n\n\n\n\n\nNow let’s set up a “load-balanced view”. With this type of interface, one submits the tasks and the controller decides how to divide up the tasks, ideally achieving good load balancing. A load-balanced computation is one that keeps all the workers busy throughout the computation\n\nlview = c.load_balanced_view()\nlview.block = True\n\n# need a wrapper function because map() only operates on one argument\ndef wrapper(i):\n    return(looFit(i, Y, X))\n\n# Now run the fitting, predicting on each held-out observation:\npred = lview.map(wrapper, range(n))\n# Check a few predictions:\npred[0:3]\n\n[1.8086526142545176, -0.46862439449018756, -0.5001718261880562]\n\n\n\n\n\nOne can also start the workers outside of Python. This was required in older versions of ipyparallel, before version 7.\n# In the bash shell:\nexport NWORKERS=4\nipcluster start -n ${NWORKERS} &\nNow in Python, we can connect to the running workers:\n\n# In python\nimport os\nimport ipyparallel as ipp\nc = ipp.Client()\nc.wait_for_engines(n = int(os.environ['NWORKERS']))\nc.ids\n# Now do your parallel computations\n\nFinally, stop the workers.\nipcluster stop\n\n\n\n\nOne can use ipyparallel in a context with multiple nodes, though the setup to get the worker processes started is a bit more involved when you have multiple nodes.\nIf we are using the SLURM scheduling software, here’s how we start up the worker processes:\n# In the bash shell (e.g., in your Slurm job script)\nipcontroller --ip='*' &\nsleep 60\n# Next start as many ipengines (workers) as we have Slurm tasks. \n# This works because srun is a Slurm command, \n# so it knows it is running within a Slurm allocation\nsrun ipengine &\nAt this point you should be able to connect to the running cluster using the syntax seen for single-node usage.\n\nWarning: Be careful to set the sleep period long enough that the controller starts before trying to start the workers and the workers start before trying to connect to the workers from within Python.\n\nAfter doing your computations and quitting your main Python session, shut down the cluster of workers:\nipcluster stop\nTo start the engines in a context outside of using Slurm (provided all machines share a filesystem), you should be able ssh to each machine and run ipengine & for as many worker processes as you want to start as follows. In some, but not all cases (depending on how the network is set up) you may not need the --location flag, but if you do, it should be set to the name of the machine you’re working on, e.g., by using the HOST environment variable. Here we start all the workers on a single other machine, “other_host”:\nipcontroller --ip='*' --location=${HOST} &\nsleep 60\nNWORKERS=4\nssh other_host \"for (( i = 0; i &lt; ${NWORKERS}; i++ )); do ipengine &; done\"\n\n\n\n\nDask and Ray are powerful packages for parallelization that allow one to parallelize tasks in similar fashion to ipyparallel. But they also provide additional useful functionality: Dask allows one to work with large datasets that are split up across multiple processes on (potentially) multiple nodes, providing Spark/Hadoop-like functionality. Ray allows one to develop complicated apps that execute in parallel using the notion of actors.\nFor more details on using distributed dataset with Dask, see this Dask tutorial. For more details on Ray’s actors, please see the Ray documentation.\n\n\nThere are various ways to do parallel loops in Dask, as discussed in detail in this Dask tutorial.\nHere’s an example of doing it with “delayed” calculations set up via list comprehension. First we’ll start workers on a single machine. One can also start workers on multiple machines, as discussed in the tutorial linked to just above.\n\nimport dask.multiprocessing\ndask.config.set(scheduler='processes', num_workers = 4)\n\nNow we’ll execute a set of tasks in parallel by wrapping the function of interest in dask.delayed to set up lazy evaluation that will be done in parallel using the workers already set up with the ‘processes’ scheduler above.\n\ndef calc_mean(i, n):\n    import numpy as np\n    rng = np.random.default_rng(i)\n    data = rng.normal(size = n)\n    return([np.mean(data), np.std(data)])\n\nn = 1000\np = 10\nfutures = [dask.delayed(calc_mean)(i, n) for i in range(p)]\nfutures  # This is an array of placeholders for the tasks to be carried out.\n# [Delayed('calc_mean-b07564ff-149a-4db7-ac3c-1cc89b898fe5'), \n# Delayed('calc_mean-f602cd67-97ad-4293-aeb8-e58be55a89d6'), \n# Delayed('calc_mean-d9448f54-b1db-46aa-b367-93a46e1c202a'), ...\n\n# Now ask for the output to trigger the lazy evaluation.\nresults = dask.compute(futures)\n\nExecution only starts when we call dask.compute.\nNote that we set a separate seed for each task to try to ensure indepenedent random numbers between tasks, but Section 5 discusses better ways to do this.\n\n\n\nWe’ll start up workers on a single machine. To run across multiple workers, see this tutorial or the Ray documentation.\n\nimport ray\nray.init(num_cpus = 4)\n\nTo run a computation in parallel, we decorate the function of interest with the remote tag:\n\n@ray.remote\ndef calc_mean(i, n):\n    import numpy as np\n    rng = np.random.default_rng(i)\n    data = rng.normal(size = n)\n    return([np.mean(data), np.std(data)])\n\nn = 1000\np = 10\nfutures = [calc_mean.remote(i, n) for i in range(p)]\nfutures  # This is an array of placeholders for the tasks to be carried out.\n# [ObjectRef(a67dc375e60ddd1affffffffffffffffffffffff0100000001000000), \n# ObjectRef(63964fa4841d4a2effffffffffffffffffffffff0100000001000000), ...\n\n# Now trigger the computation\nray.get(futures)\n\n\n\n\n\n\n\nThe key thing when thinking about random numbers in a parallel context is that you want to avoid having the same ‘random’ numbers occur on multiple processes. On a computer, random numbers are not actually random but are generated as a sequence of pseudo-random numbers designed to mimic true random numbers. The sequence is finite (but very long) and eventually repeats itself. When one sets a seed, one is choosing a position in that sequence to start from. Subsequent random numbers are based on that subsequence. All random numbers can be generated from one or more random uniform numbers, so we can just think about a sequence of values between 0 and 1.\nThe worst thing that could happen is that one sets things up in such a way that every process is using the same sequence of random numbers. This could happen if you mistakenly set the same seed in each process, e.g., using rng = np.random.default_rng(1) or np.random.seed(1) in Python for every worker.\nThe naive approach is to use a different seed for each process. E.g., if your processes are numbered id = 1,2,...,p with a variable id that is unique to a process, setting the seed to be the value of id on each process. This is likely not to cause problems, but raises the danger that two (or more) subsequences might overlap. For an algorithm with dependence on the full subsequence, such as an MCMC, this probably won’t cause big problems (though you likely wouldn’t know if it did), but for something like simple simulation studies, some of your ‘independent’ samples could be exact replicates of a sample on another process. Given the period length of the default generator in Python, this is actually quite unlikely, but it is a bit sloppy.\nTo avoid this problem, the key is to use an algorithm that ensures sequences that do not overlap.\n\n\n\nIn recent versions of numpy there has been attention paid to this problem and there are now multiple approaches to getting high-quality random number generation for parallel code.\nOne approach is to generate one random seed per task such that the blocks of random numbers avoid overlapping with high probability, as implemented in numpy’s SeedSequence approach.\nHere we use that approach within the context of an ipyparallel load-balanced view.\n\nimport numpy as np\nimport ipyparallel as ipp\nn = 4\ncluster = ipp.Cluster(n = n)\ncluster.start_cluster_sync()\n\nc = cluster.connect_client_sync()\nc.wait_for_engines(n)\nc.ids\n\nlview = c.load_balanced_view()\nlview.block = True\n\nn = 1000\np = 10\n\nseed = 1\nss = np.random.SeedSequence(seed)\nchild_seeds = ss.spawn(p)\n\ndef calc_mean(i, n, seed_i):\n    import numpy as np\n    rng = np.random.default_rng(seed_i)\n    data = rng.normal(size = n)\n    return([np.mean(data), np.std(data)])\n\n# need a wrapper function because map() only operates on one argument\ndef wrapper(i):\n    return(calc_mean(i, n, child_seeds[i]))\n\ndview = c[:]\ndview.block = True \nmydict = dict(calc_mean = calc_mean, n = n, child_seeds = child_seeds)\ndview.push(mydict)\n\nresults = lview.map(wrapper, range(p))\n\nA second approach is to advance the state of the random number generator as if a large number of random numbers had been drawn.\n\nseed = 1\npcg64 = np.random.PCG64(seed)\n\ndef calc_mean(i, n, rng):\n    import numpy as np\n    rng = np.random.Generator(pcg64.jumped(i))  ## jump in large steps, one jump per task\n    data = rng.normal(size = n)\n    return([np.mean(data), np.std(data)])\n\n# need a wrapper function because map() only operates on one argument\ndef wrapper(i):\n    return(calc_mean(i, n, rng))\n\ndview = c[:]\ndview.block = True \nmydict = dict(calc_mean = calc_mean, n = n, rng = rng)\ndview.push(mydict)\n\nresults = lview.map(wrapper, range(p))\n\nNote that above, I’ve done everything at the level of the computational tasks. One could presumably do this at the level of the workers, but one would need to figure out how to maintain the state of the generator from one task to the next for any given worker.\n\n\n\n\nPython is the go-to language used to run computations on a GPU. Some of the packages that can easily offload computations to the GPU include PyTorch, Tensorflow, JAX, and CuPy. (Of course PyTorch and Tensorflow are famously used for deep learning, but they’re also general numerical computing packages.) We’ll discuss some of these.\nThere are a couple key things to remember about using a GPU:\n\nThe GPU memory is separate from CPU memory, and transferring data from the CPU to GPU (or back) is often more costly than doing the computation on the GPU.\n\nIf possible, generate the data on the GPU or keep the data on the GPU when carrying out a sequence of operations.\n\nBy default GPU calculations are often doing using 32-bit (4-byte) floating point numbers rather than the standard of 64-bit (8-byte) when on the CPU.\n\nThis can affect speed comparisons between CPU and GPU if one doesn’t compare operations with the same types of floating point numbers.\n\nGPU operations are often asynchronous – they’ll continue in the background after they start, returning control of your Python session to you and potentially making it seem like the computation happened more quickly than it did.\n\nIn the examples below, note syntax that ensures the operation is done before timing concludes (e.g., cuda.synchronize for PyTorch and block_until_ready for JAX).\n\n\nNote that for this section, I’m pasting in the output when running the code separately on a machine with a GPU because this document is generated on a machine without a GPU.\n\n\nHere’s an example of doing some linear algebra (simply matrix multiplication) on the GPU using PyTorch.\nBy default PyTorch will use 32-bit numbers.\n\nimport torch\nimport time\n\nstart = torch.cuda.Event(enable_timing=True)\nend = torch.cuda.Event(enable_timing=True)\n\ngpu = torch.device(\"cuda:0\")\n\nn = 7000\n\ndef matmul_wrap(x, y):\n    z = torch.matmul(x, y)\n    return(z)\n    \n## Generate data on the CPU.    \nx = torch.randn(n,n)\ny = torch.randn(n,n)\n\n## Copy the objects to the GPU.\nx_gpu = x.cuda() # or: `x.to(\"cuda\")`\ny_gpu = y.cuda()\n    \ntorch.set_num_threads(1)\n    \nt0 = time.time()\nz = matmul_wrap(x, y)\nprint(time.time() - t0)  # 6.8 sec.\n\nstart.record()\nz_gpu = matmul_wrap(x_gpu, y_gpu)\ntorch.cuda.synchronize()\nend.record()\nprint(start.elapsed_time(end))  # 70 milliseconds (ms)\n\nSo we achieved a speedup of about 100-fold over a single CPU core using an A100 GPU in this case.\nLet’s consider the time for copying data to the GPU:\n\nx = torch.randn(n,n)\nstart.record()\nx_gpu = x.cuda()\ntorch.cuda.synchronize()\nend.record()\nprint(start.elapsed_time(end))  # 60 ms\n\nThis suggests that the time in copying the data is similar to that for doing the matrix multiplication.\nWe can generate data on the GPU like this:\n\nx_gpu = torch.randn(n,n, device=gpu)\n\n\n\n\nHere we’ll consider using the GPU for vectorized calculations. We’ll compare using numpy, CPU-based PyTorch, and GPU-based PyTorch, again with 32-bit numbers.\n\nimport torch\nimport numpy as np\nimport time\n\nstart = torch.cuda.Event(enable_timing=True)\nend = torch.cuda.Event(enable_timing=True)\n\ngpu = torch.device(\"cuda:0\")\n\ndef myfun_np(x):\n    y = np.exp(x) + 3 * np.sin(x)\n    return(y)\n\ndef myfun_torch(x):\n    y = torch.exp(x) + 3 * torch.sin(x)\n    return(y)\n    \n    \nn = 250000000\nx = torch.randn(n)\nx_gpu = x.cuda() # or: `x.to(\"cuda\")`\ntmp = np.random.normal(size = n)\nx_np = tmp.astype(np.float32)  # for fair comparison\n\n## numpy\nt0 = time.time()\ny_np = myfun_np(x_np)\ntime.time()-t0   # 1.2 sec.\n\n## CPU-based torch (1 thread)\ntorch.set_num_threads(1)\nstart.record()\ny = myfun_torch(x)\nend.record()\nprint(start.elapsed_time(end))  # 2200 ms (2.2 sec.)\n\n## GPU-based torch\nstart.record()\ny_gpu = myfun_torch(x_gpu)\ntorch.cuda.synchronize()\nend.record()\nprint(start.elapsed_time(end))   # 9 ms\n\nSo using the GPU speeds things up by 150-fold (compared to numpy) and 250-fold (compared to CPU-based PyTorch).\nOne can also have PyTorch “fuse” the operations in the loop, which avoids having the different vectorized operations in myfun being done in separate loops under the hood. For an overview of loop fusion, see this discussion in the context of Julia.\nTo fuse the operations, we need to have the function in a module. In this case I defined myfun_torch in myfun_torch.py, and we need to compile the code using torch.jit.script.\n\nfrom myfun_torch import myfun_torch as myfun_torch_tmp\nmyfun_torch_compiled = torch.jit.script(myfun_torch_tmp)\n\n## CPU plus loop fusion\nstart.record()\ny = myfun_torch_compiled(x)\nend.record()\nprint(start.elapsed_time(end))   # 1000 ms (1 sec.)\n\n## GPU plus loop fusion\nstart.record()\ny_gpu = myfun_torch_compiled(x_gpu)\ntorch.cuda.synchronize()\nend.record()\nprint(start.elapsed_time(end))   # 3.5 ms\n\nSo that seems to give a 2-3 fold speedup compared to without loop fusion.\n\n\n\nOne can also use PyTorch to run computations on the GPU that comes with Apple’s M2 chips.\nThe “backend” is called “MPS”, where “M” stands for “Metal”, which is what Apple calls its GPU framework.\n\nimport torch\nimport time\n\nstart = torch.mps.Event(enable_timing=True)\nend = torch.mps.Event(enable_timing=True)\n\nmps_device = torch.device(\"mps\")\n\nn = 10000\nx = torch.randn(n,n)\ny = torch.randn(n,n) \n\nx_mps = x.to(\"mps\")\ny_mps = y.to(\"mps\")\n    \n## On the CPU    \ntorch.set_num_threads(1)\n\nt0 = time.time()\nz = matmul_wrap(x, y)\nprint(time.time() - t0)   # 1.8 sec (1800 ms)\n\n## On the M2 GPU\nstart.record()\nz_mps = matmul_wrap(x_mps, y_mps)\ntorch.mps.synchronize()\nend.record()\nprint(start.elapsed_time(end)) # 950 ms\n\nSo there is about a two-fold speed up, which isn’t impressive compared to the speedup on a standard GPU.\nLet’s see how much time is involved in transferring the data.\n\nx = torch.randn(n,n)\n\nstart.record()\nx_mps = x.to(\"mps\")\ntorch.mps.synchronize()\nend.record()\nprint(start.elapsed_time(end))  # 35 ms.\n\nSo it looks like the transfer time is pretty small compared to the computation time (and to the savings involved in using the M2 GPU).\nWe can generate data on the GPU like this:\n\nx_mps = torch.randn(n,n, device=mps_device)\n\n\n\n\n\nYou can think of JAX as a version of numpy enabled to use the GPU (or automatically parallelize on CPU threads) and provide automatic differentiation.\nOne can also use just-in-time (JIT) compilation with JAX. Behind the scenes, the instructions are compiled to machine code for different backends (e.g., CPU and GPU) using XLA.\n\n\nLet’s first consider running a vectorized calculation using JAX on the CPU, which will use multiple threads, each thread running on a separate CPU core on our computer.\n\nimport time\nimport numpy as np\nimport jax.numpy as jnp\n\ndef myfun_np(x):\n    y = np.exp(x) + 3 * np.sin(x)\n    return(y)\n    \ndef myfun_jnp(x):\n    y = jnp.exp(x) + 3 * jnp.sin(x)\n    return(y)\n\nn = 250000000\n\nx = np.random.normal(size = n).astype(np.float32)  # for consistency\nx_jax = jnp.array(x)  # 32-bit by default\nprint(x_jax.platform())\n\ncpu\n\n\n\nt0 = time.time()\nz = myfun_np(x)\nt1 = time.time() - t0\n\nt0 = time.time()\nz_jax = myfun_jnp(x_jax).block_until_ready()\nt2 = time.time() - t0\n\nprint(f\"numpy time: {round(t1,3)}\\njax time: {round(t2,3)}\")\n\nnumpy time: 3.148\njax time: 1.555\n\n\nThere’s a nice speedup compared to numpy.\nSince JAX will often execute computations asynchronously (in particular when using the GPU), the block_until_ready invocation ensures that the computation finishes before we stop timing.\nBy default the JAX floating point type is 32-bit so we forced the use of 32-bit numbers for numpy for comparability. One could have JAX use 64-bit numbers like this:\n\nimport jax\njax.config.update(\"jax_enable_x64\", True)  \n\nNext let’s consider JIT compiling it, which should fuse the vectorized operations and avoid temporary objects. The JAX docs have a nice discussion of when JIT compilation will be beneficial.\n\nimport jax\nmyfun_jnp_jit = jax.jit(myfun_jnp)\n\nt0 = time.time()\nz_jax_jit = myfun_jnp_jit(x_jax).block_until_ready()\nt3 = time.time() - t0\nprint(f\"jitted jax time: {round(t3,3)}\")\n\njitted jax time: 0.824\n\n\nSo that gives another almost 2x speedup.\n\n\n\nLinear algebra in JAX will use multiple threads (as discussed for numpy). Here we’ll compare 64-bit calculation, since matrix decompositions sometimes need more precision.\n\nn = 7000\nx = np.random.normal(0, 1, size=(n, n))\n\nt0 = time.time()\nmat = x.T @ x\nprint(\"numpy time:\")\nprint(round(time.time() - t0,3))\n\nt0 = time.time()\nU = np.linalg.cholesky(mat) \nprint(round(time.time() - t0,3))\n\nnumpy time:\n3.544\n1.561\n\n\n\nimport jax\njax.config.update(\"jax_enable_x64\", True)  \n\nx_jax = jnp.array(x, dtype = jnp.float64)\nprint(f\"JAX dtype is {x_jax.dtype}\")\n\nt0 = time.time()\nmat_jax = jnp.matmul(x_jax.transpose(), x_jax)\nprint(\"jax time:\")\nprint(round(time.time() - t0,3))\n\nt0 = time.time()\nU_jax = jnp.linalg.cholesky(mat_jax)\nprint(round(time.time() - t0,3))\n\nJAX dtype is float64\njax time:\n9.018\n2.004\n\n\nSo here the matrix multiplication is slower using JAX with 64-bit numbers but the Cholesky is a bit faster. If one uses 32-bit numbers, JAX is faster for both (not shown).\nIn general, the JAX speedups are not huge, which is not surprising given both approaches are using multiple threads to carry out the linear algebra. At the least it indicates one can move a numpy workflow to JAX without worrying about losing the threaded BLAS speed of numpy.\n\n\n\nGetting threaded CPU computation automatically is nice, but the real benefit of JAX comes in offloading computations to the GPU (and in providing automatic differentiation, not discussed in this tutorial). If a GPU is available and a GPU-enabled JAX is installed, JAX will generally try to use the GPU.\nNote my general comments about using the GPU in the PyTorch section.\nNote that for this section, I’m pasting in the output when running the code separately on a machine with a GPU because this document is generated on a machine without a GPU.\nWe’ll just repeat the experiments we ran earlier comparing numpy- and JAX-based calculations, but on a machine with an A100 GPU.\n\nimport time\nimport numpy as np\nimport jax.numpy as jnp\n\ndef myfun_np(x):\n    y = np.exp(x) + 3 * np.sin(x)\n    return(y)\n    \ndef myfun_jnp(x):\n    y = jnp.exp(x) + 3 * jnp.sin(x)\n    return(y)\n\nn = 250000000\n\nx = np.random.normal(size = n).astype(np.float32)  # for consistency\nx_jax = jnp.array(x)  # 32-bit by default\nprint(x_jax.platform())    # gpu\n\nt0 = time.time()\nz = myfun_np(x)\nprint(time.time() - t0)    # 1.15 s.\n\nt0 = time.time()\nz_jax = myfun_jnp(x_jax).block_until_ready()\nprint(time.time() - t0)    # 0.0099 s.\n\nSo that gives a speedup of more than 100x.\n\nimport jax\nmyfun_jnp_jit = jax.jit(myfun_jnp)\n\nt0 = time.time()\nz_jax_jit = myfun_jnp_jit(x_jax).block_until_ready()  # 0.0052 s.\nprint(time.time() - t0)\n\nJIT compilation helps a bit (about 2x).\nFinally, here’s the linear algebra example on the GPU.\n\nn = 7000\nx = np.random.normal(0, 1, size=(n, n)).astype(np.float32) # for consistency\n\nt0 = time.time()\nmat = x.T @ x\nprint(time.time() - t0)    # 3.7 s.\n\nt0 = time.time()\nU = np.linalg.cholesky(mat)  # 3.3 s.\nprint(time.time() - t0)\n\n\nx_jax = jnp.array(x)\n\nt0 = time.time()\nmat_jax = jnp.matmul(x_jax.transpose(), x_jax).block_until_ready()\nprint(time.time() - t0)    # 0.025 sec.\n\nt0 = time.time()\nU_jax = jnp.linalg.cholesky(mat_jax).block_until_ready()\nprint(time.time() - t0)   # 0.08 s.\n\nAgain we get a very impressive speedup.\n\n\n\nAs discussed elsewhere in this tutorial, it takes time to transfer data to and from the GPU, so it’s best to generate values on the GPU and keep objects on the GPU when possible.\nAlso, JAX objects are designed to be manipulated as objects, rather than manipulating individual values.\n\nx_jax[0,0] = 3.17\n\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/scratch/users/paciorek/conda/envs/jax-test/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py\", line 278, in _unimplemented_setitem\n    raise TypeError(msg.format(type(self)))\nTypeError: '&lt;class 'jaxlib.xla_extension.ArrayImpl'&gt;' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\n\n\n\nWe can use JAX’s vmap to automatically vectorize a map operation. Unlike numpy’s vectorize or apply_along_axis, which are just handy syntax (“syntactic sugar”) and don’t actually speed anything up (because the looping is still done in Python), vmap actually vectorizes the loop. Behind the scenes it generates a vectorized version of the code that can run in parallel on CPU or GPU.\nIn general, one would use this to automatically iterate over the dimension(s) of one or more arrays. This is convenient from a coding perspective (compared to explicitly writing a loop) and potentially speeds up the computation based on parallelization and by avoiding the overhead of looping at the Python level.\nHere we’ll standardize each column of an array using vmap rather than writing a loop over the columns.\n\nimport jax\nimport jax.numpy as jnp\nimport time\n\nnr = 10000\nnc = 10000\nx = np.random.normal(size = (nr,nc)).astype(np.float32)  # for consistency\nx_jax = jnp.array(x) \n\ndef f(x):\n    ## Standardize a vector by its range.\n    return x / (np.max(x) - np.min(x))\n\ndef f_jax(x):\n    return x / (jnp.max(x) - jnp.min(x))\n\n# Standardize each column.\n\nt0 = time.time()\nout = np.apply_along_axis(f, 0, x)  \nt1 = time.time() - t0\n\n# JAX vmap numbers axes in reverse order of numpy, apparently.\nf_jax_vmap = jax.vmap(f_jax, in_axes=1, out_axes=1)\n\nt0 = time.time()\nout_jax = f_jax_vmap(x_jax).block_until_ready()     \nt2 = time.time() - t0\nprint(f\"numpy time: {round(t1,3)}\\njax vmap time: {round(t2,3)}\")\n\nnumpy time: 3.512\njax vmap time: 1.269\n\n\nThat gives a nice speedup. Let’s also try JIT’ing it. That gives a further speedup.\n\nf_jax_vmap_jit = jax.jit(f_jax_vmap)\n\nt0 = time.time()\nout_jax_jit = f_jax_vmap_jit(x_jax).block_until_ready()\nt3 = time.time() - t0\nprint(f\"jitted jax vmap time: {round(t3,3)}\")\n\njitted jax vmap time: 0.318\n\n\nIt would make sense to explore the benefits of using a GPU here, though I haven’t done so.\nvmap has a lot of flexibility to operate on various axes of its input arguments (and structure the output axes). Suppose we want to do the same standardization but using the columns of a different array as what to standardize based on.\n\ny = np.random.normal(size = (nr,nc)).astype(np.float32)\ny_jax = jnp.array(y) \n\ndef f2_jax(x, y):\n    return x / (jnp.max(y) - jnp.min(y))\n\nout2 = jax.vmap(f2_jax, in_axes=(1,1), out_axes=1)(x_jax, y_jax)\nf2_jax_jit = jax.jit(jax.vmap(f2_jax, in_axes=(1,1), out_axes=1)) \nout3 = f2_jax_jit(x_jax, y_jax)\n\nFinally, note that pmap is a function with a similar-sounding name that allows one to parallelize a map operation over multiple devices (e.g., multiple GPUs).\n\n\n\n\nCuPy is another package allowing one to execute numpy-type calculations on the GPU (Nvidia only). It has some similarity to JAX.\nHere’s a basic illustration, where we get a 175x speedup for generating a random matrix and matrix multiplication when using an A100 GPU.\n\nimport cupy\nimport numpy as np\nimport time\n\ndef matmul_np(n):\n    x = np.random.normal(size=(n,n))\n    z = np.matmul(x,x)\n    return(z)\n\ndef matmul_cupy(n):\n    x = cupy.random.normal(size=(n,n))\n    z = cupy.matmul(x,x)\n    return(z)\n\n\nn = 7000\n\nt0 = time.time()\nz = matmul_np(n)\nprint(time.time() - t0)   # 8.8 s.\n\nt0 = time.time()\nz_cupy = matmul_cupy(n)\ncupy.cuda.stream.get_current_stream().synchronize()\nprint(time.time() - t0)   # .05 s.\n\nYou can also use cupy.RawKernel to execute a GPU kernel written in CUDA C/C++ directly from Python. That’s a bit beyond our scope here, so I won’t show an example.",
    "crumbs": [
      "Parallel Python"
    ]
  },
  {
    "objectID": "parallel-python.html#overview",
    "href": "parallel-python.html#overview",
    "title": "Parallel processing in Python",
    "section": "",
    "text": "Python provides a variety of functionality for parallelization, including threaded operations (in particular for linear algebra), parallel looping and map statements, and parallelization across multiple machines. For the CPU, this material focuses on Python’s ipyparallel package and JAX, with some discussion of Dask and Ray. For the GPU, the material focuses on PyTorch and JAX, with a bit of discussion of CuPy.\nNote that all of the looping-based functionality discussed here applies only if the iterations/loops of your calculations can be done completely separately and do not depend on one another. This scenario is called an embarrassingly parallel computation. So coding up the evolution of a time series or a Markov chain is not possible using these tools. However, bootstrapping, random forests, simulation studies, cross-validation and many other statistical methods can be handled in this way.",
    "crumbs": [
      "Parallel Python"
    ]
  },
  {
    "objectID": "parallel-python.html#threading",
    "href": "parallel-python.html#threading",
    "title": "Parallel processing in Python",
    "section": "",
    "text": "The BLAS is the library of basic linear algebra operations (written in Fortran or C). A fast BLAS can greatly speed up linear algebra relative to the default BLAS on a machine. Some fast BLAS libraries are\n\nIntel’s MKL; may be available for educational use for free\nOpenBLAS; open source and free\nvecLib for Macs; provided with your Mac\n\nIn addition to being fast when used on a single core, all of these BLAS libraries are threaded - if your computer has multiple cores and there are free resources, your linear algebra will use multiple cores, provided your installed Python is linked against the threaded BLAS installed on your machine.\nTo use a fast, threaded BLAS, one approach is to use the Anaconda/Miniconda Python distribution. When you install numpy and scipy, these should be automatically linked against a fast, threaded BLAS (MKL). More generally, simply installing numpy from PyPI should make use of OpenBLAS.\n\n\n\nThreading in Python is limited to linear algebra (provided Python is linked against a threaded BLAS, except if using Dask or JAX or various other packages). Python has something called the Global Interpreter Lock that interferes with threading in Python (but not in threaded linear algebra packages called by Python).\nHere’s some linear algebra in Python that will use threading if numpy is linked against a threaded BLAS, though I don’t compare the timing for different numbers of threads here.\n\nimport numpy as np\nn = 5000\nx = np.random.normal(0, 1, size=(n, n))\nx = x.T @ x\nU = np.linalg.cholesky(x)\n\nIf you watch the Python process via the top command, you should see CPU usage above 100% if Python is linking to a threaded BLAS.\n\n\n\nIn general, threaded code will detect the number of cores available on a machine and make use of them. However, you can also explicitly control the number of threads available to a process.\nFor most threaded code (that based on the openMP protocol), the number of threads can be set by setting the OMP_NUM_THREADS environment variable. Note that under some circumstances you may need to use VECLIB_MAXIMUM_THREADS if on an (older, Intel-based) Mac or MKL_NUM_THREADS if numpy/scipy are linked against MKL.\nFor example, to set it for four threads in bash, do this before starting your Python session.\nexport OMP_NUM_THREADS=4\nAlternatively, you can set OMP_NUM_THREADS as you invoke your job, e.g.,\nOMP_NUM_THREADS=4 python job.py &gt; job.out",
    "crumbs": [
      "Parallel Python"
    ]
  },
  {
    "objectID": "parallel-python.html#basic-parallelized-loopsmapsapply-using-ipyparallel",
    "href": "parallel-python.html#basic-parallelized-loopsmapsapply-using-ipyparallel",
    "title": "Parallel processing in Python",
    "section": "",
    "text": "First we’ll cover IPython Parallel (i.e., the ipyparallel package) functionality, which allows one to parallelize on a single machine (discussed here) or across multiple machines (see next section). In later sections, I’ll discuss other packages that can be used for parallelization.\nFirst we need to start our workers. As of ipyparallel version 7, we can start the workers from within Python.\n\n## In newer versions of ipyparallel (v. 7 and later)\nimport ipyparallel as ipp\n# Check the version:\nipp.__version__\nn = 4\ncluster = ipp.Cluster(n = n)\nc = cluster.start_and_connect_sync()\n\nStarting 4 engines with &lt;class 'ipyparallel.cluster.launcher.LocalEngineSetLauncher'&gt;\n\n\n\n\n\n\n\n\nLet’s verify that things seem set up ok and we can interact with all our workers:\n\n## Check that we have the number of workers expected:\nc.ids\n\n[0, 1, 2, 3]\n\n\n\n## Set up a direct view to interface with all the workers\ndview = c[:]\ndview\n\n&lt;DirectView [0, 1, 2, 3]&gt;\n\n\n\n## Set blocking so that we wait for the result of the parallel execution\ndview.block = True \ndview.apply(lambda : \"Hello, World\")\n\n['Hello, World', 'Hello, World', 'Hello, World', 'Hello, World']\n\n\ndview stands for a ‘direct view’, which is an interface to our cluster that allows us to ‘manually’ send tasks to the workers.\n\n\n\nNow let’s see an example of how we can use our workers to run code in parallel.\nWe’ll carry out a statistics/machine learning prediction method (random forest regression) with leave-one-out cross-validation, parallelizing over different held out data.\nFirst let’s set up packages, data and our main function on the workers:\n\ndview.execute('from sklearn.ensemble import RandomForestRegressor as rfr')\ndview.execute('import numpy as np')\n\ndef looFit(index, Ylocal, Xlocal):\n    rf = rfr(n_estimators=100)\n    fitted = rf.fit(np.delete(Xlocal, index, axis = 0), np.delete(Ylocal, index))\n    pred = rf.predict(np.array([Xlocal[index, :]]))\n    return(pred[0])\n\nimport numpy as np\nnp.random.seed(0)\nn = 200\np = 20\nX = np.random.normal(0, 1, size = (n, p))\nY = X[: , 0] + pow(abs(X[:,1] * X[:,2]), 0.5) + X[:,1] - X[:,2] + \\\n    np.random.normal(0, 1, n)\n\nmydict = dict(X = X, Y = Y, looFit = looFit)\ndview.push(mydict)\n\n[None, None, None, None]\n\n\n\n\n\nNow let’s set up a “load-balanced view”. With this type of interface, one submits the tasks and the controller decides how to divide up the tasks, ideally achieving good load balancing. A load-balanced computation is one that keeps all the workers busy throughout the computation\n\nlview = c.load_balanced_view()\nlview.block = True\n\n# need a wrapper function because map() only operates on one argument\ndef wrapper(i):\n    return(looFit(i, Y, X))\n\n# Now run the fitting, predicting on each held-out observation:\npred = lview.map(wrapper, range(n))\n# Check a few predictions:\npred[0:3]\n\n[1.8086526142545176, -0.46862439449018756, -0.5001718261880562]\n\n\n\n\n\nOne can also start the workers outside of Python. This was required in older versions of ipyparallel, before version 7.\n# In the bash shell:\nexport NWORKERS=4\nipcluster start -n ${NWORKERS} &\nNow in Python, we can connect to the running workers:\n\n# In python\nimport os\nimport ipyparallel as ipp\nc = ipp.Client()\nc.wait_for_engines(n = int(os.environ['NWORKERS']))\nc.ids\n# Now do your parallel computations\n\nFinally, stop the workers.\nipcluster stop\n\n\n\n\nOne can use ipyparallel in a context with multiple nodes, though the setup to get the worker processes started is a bit more involved when you have multiple nodes.\nIf we are using the SLURM scheduling software, here’s how we start up the worker processes:\n# In the bash shell (e.g., in your Slurm job script)\nipcontroller --ip='*' &\nsleep 60\n# Next start as many ipengines (workers) as we have Slurm tasks. \n# This works because srun is a Slurm command, \n# so it knows it is running within a Slurm allocation\nsrun ipengine &\nAt this point you should be able to connect to the running cluster using the syntax seen for single-node usage.\n\nWarning: Be careful to set the sleep period long enough that the controller starts before trying to start the workers and the workers start before trying to connect to the workers from within Python.\n\nAfter doing your computations and quitting your main Python session, shut down the cluster of workers:\nipcluster stop\nTo start the engines in a context outside of using Slurm (provided all machines share a filesystem), you should be able ssh to each machine and run ipengine & for as many worker processes as you want to start as follows. In some, but not all cases (depending on how the network is set up) you may not need the --location flag, but if you do, it should be set to the name of the machine you’re working on, e.g., by using the HOST environment variable. Here we start all the workers on a single other machine, “other_host”:\nipcontroller --ip='*' --location=${HOST} &\nsleep 60\nNWORKERS=4\nssh other_host \"for (( i = 0; i &lt; ${NWORKERS}; i++ )); do ipengine &; done\"",
    "crumbs": [
      "Parallel Python"
    ]
  },
  {
    "objectID": "parallel-python.html#dask-and-ray",
    "href": "parallel-python.html#dask-and-ray",
    "title": "Parallel processing in Python",
    "section": "",
    "text": "Dask and Ray are powerful packages for parallelization that allow one to parallelize tasks in similar fashion to ipyparallel. But they also provide additional useful functionality: Dask allows one to work with large datasets that are split up across multiple processes on (potentially) multiple nodes, providing Spark/Hadoop-like functionality. Ray allows one to develop complicated apps that execute in parallel using the notion of actors.\nFor more details on using distributed dataset with Dask, see this Dask tutorial. For more details on Ray’s actors, please see the Ray documentation.\n\n\nThere are various ways to do parallel loops in Dask, as discussed in detail in this Dask tutorial.\nHere’s an example of doing it with “delayed” calculations set up via list comprehension. First we’ll start workers on a single machine. One can also start workers on multiple machines, as discussed in the tutorial linked to just above.\n\nimport dask.multiprocessing\ndask.config.set(scheduler='processes', num_workers = 4)\n\nNow we’ll execute a set of tasks in parallel by wrapping the function of interest in dask.delayed to set up lazy evaluation that will be done in parallel using the workers already set up with the ‘processes’ scheduler above.\n\ndef calc_mean(i, n):\n    import numpy as np\n    rng = np.random.default_rng(i)\n    data = rng.normal(size = n)\n    return([np.mean(data), np.std(data)])\n\nn = 1000\np = 10\nfutures = [dask.delayed(calc_mean)(i, n) for i in range(p)]\nfutures  # This is an array of placeholders for the tasks to be carried out.\n# [Delayed('calc_mean-b07564ff-149a-4db7-ac3c-1cc89b898fe5'), \n# Delayed('calc_mean-f602cd67-97ad-4293-aeb8-e58be55a89d6'), \n# Delayed('calc_mean-d9448f54-b1db-46aa-b367-93a46e1c202a'), ...\n\n# Now ask for the output to trigger the lazy evaluation.\nresults = dask.compute(futures)\n\nExecution only starts when we call dask.compute.\nNote that we set a separate seed for each task to try to ensure indepenedent random numbers between tasks, but Section 5 discusses better ways to do this.\n\n\n\nWe’ll start up workers on a single machine. To run across multiple workers, see this tutorial or the Ray documentation.\n\nimport ray\nray.init(num_cpus = 4)\n\nTo run a computation in parallel, we decorate the function of interest with the remote tag:\n\n@ray.remote\ndef calc_mean(i, n):\n    import numpy as np\n    rng = np.random.default_rng(i)\n    data = rng.normal(size = n)\n    return([np.mean(data), np.std(data)])\n\nn = 1000\np = 10\nfutures = [calc_mean.remote(i, n) for i in range(p)]\nfutures  # This is an array of placeholders for the tasks to be carried out.\n# [ObjectRef(a67dc375e60ddd1affffffffffffffffffffffff0100000001000000), \n# ObjectRef(63964fa4841d4a2effffffffffffffffffffffff0100000001000000), ...\n\n# Now trigger the computation\nray.get(futures)",
    "crumbs": [
      "Parallel Python"
    ]
  },
  {
    "objectID": "parallel-python.html#random-number-generation-rng-in-parallel",
    "href": "parallel-python.html#random-number-generation-rng-in-parallel",
    "title": "Parallel processing in Python",
    "section": "",
    "text": "The key thing when thinking about random numbers in a parallel context is that you want to avoid having the same ‘random’ numbers occur on multiple processes. On a computer, random numbers are not actually random but are generated as a sequence of pseudo-random numbers designed to mimic true random numbers. The sequence is finite (but very long) and eventually repeats itself. When one sets a seed, one is choosing a position in that sequence to start from. Subsequent random numbers are based on that subsequence. All random numbers can be generated from one or more random uniform numbers, so we can just think about a sequence of values between 0 and 1.\nThe worst thing that could happen is that one sets things up in such a way that every process is using the same sequence of random numbers. This could happen if you mistakenly set the same seed in each process, e.g., using rng = np.random.default_rng(1) or np.random.seed(1) in Python for every worker.\nThe naive approach is to use a different seed for each process. E.g., if your processes are numbered id = 1,2,...,p with a variable id that is unique to a process, setting the seed to be the value of id on each process. This is likely not to cause problems, but raises the danger that two (or more) subsequences might overlap. For an algorithm with dependence on the full subsequence, such as an MCMC, this probably won’t cause big problems (though you likely wouldn’t know if it did), but for something like simple simulation studies, some of your ‘independent’ samples could be exact replicates of a sample on another process. Given the period length of the default generator in Python, this is actually quite unlikely, but it is a bit sloppy.\nTo avoid this problem, the key is to use an algorithm that ensures sequences that do not overlap.\n\n\n\nIn recent versions of numpy there has been attention paid to this problem and there are now multiple approaches to getting high-quality random number generation for parallel code.\nOne approach is to generate one random seed per task such that the blocks of random numbers avoid overlapping with high probability, as implemented in numpy’s SeedSequence approach.\nHere we use that approach within the context of an ipyparallel load-balanced view.\n\nimport numpy as np\nimport ipyparallel as ipp\nn = 4\ncluster = ipp.Cluster(n = n)\ncluster.start_cluster_sync()\n\nc = cluster.connect_client_sync()\nc.wait_for_engines(n)\nc.ids\n\nlview = c.load_balanced_view()\nlview.block = True\n\nn = 1000\np = 10\n\nseed = 1\nss = np.random.SeedSequence(seed)\nchild_seeds = ss.spawn(p)\n\ndef calc_mean(i, n, seed_i):\n    import numpy as np\n    rng = np.random.default_rng(seed_i)\n    data = rng.normal(size = n)\n    return([np.mean(data), np.std(data)])\n\n# need a wrapper function because map() only operates on one argument\ndef wrapper(i):\n    return(calc_mean(i, n, child_seeds[i]))\n\ndview = c[:]\ndview.block = True \nmydict = dict(calc_mean = calc_mean, n = n, child_seeds = child_seeds)\ndview.push(mydict)\n\nresults = lview.map(wrapper, range(p))\n\nA second approach is to advance the state of the random number generator as if a large number of random numbers had been drawn.\n\nseed = 1\npcg64 = np.random.PCG64(seed)\n\ndef calc_mean(i, n, rng):\n    import numpy as np\n    rng = np.random.Generator(pcg64.jumped(i))  ## jump in large steps, one jump per task\n    data = rng.normal(size = n)\n    return([np.mean(data), np.std(data)])\n\n# need a wrapper function because map() only operates on one argument\ndef wrapper(i):\n    return(calc_mean(i, n, rng))\n\ndview = c[:]\ndview.block = True \nmydict = dict(calc_mean = calc_mean, n = n, rng = rng)\ndview.push(mydict)\n\nresults = lview.map(wrapper, range(p))\n\nNote that above, I’ve done everything at the level of the computational tasks. One could presumably do this at the level of the workers, but one would need to figure out how to maintain the state of the generator from one task to the next for any given worker.",
    "crumbs": [
      "Parallel Python"
    ]
  },
  {
    "objectID": "parallel-python.html#using-the-gpu-via-pytorch",
    "href": "parallel-python.html#using-the-gpu-via-pytorch",
    "title": "Parallel processing in Python",
    "section": "",
    "text": "Python is the go-to language used to run computations on a GPU. Some of the packages that can easily offload computations to the GPU include PyTorch, Tensorflow, JAX, and CuPy. (Of course PyTorch and Tensorflow are famously used for deep learning, but they’re also general numerical computing packages.) We’ll discuss some of these.\nThere are a couple key things to remember about using a GPU:\n\nThe GPU memory is separate from CPU memory, and transferring data from the CPU to GPU (or back) is often more costly than doing the computation on the GPU.\n\nIf possible, generate the data on the GPU or keep the data on the GPU when carrying out a sequence of operations.\n\nBy default GPU calculations are often doing using 32-bit (4-byte) floating point numbers rather than the standard of 64-bit (8-byte) when on the CPU.\n\nThis can affect speed comparisons between CPU and GPU if one doesn’t compare operations with the same types of floating point numbers.\n\nGPU operations are often asynchronous – they’ll continue in the background after they start, returning control of your Python session to you and potentially making it seem like the computation happened more quickly than it did.\n\nIn the examples below, note syntax that ensures the operation is done before timing concludes (e.g., cuda.synchronize for PyTorch and block_until_ready for JAX).\n\n\nNote that for this section, I’m pasting in the output when running the code separately on a machine with a GPU because this document is generated on a machine without a GPU.\n\n\nHere’s an example of doing some linear algebra (simply matrix multiplication) on the GPU using PyTorch.\nBy default PyTorch will use 32-bit numbers.\n\nimport torch\nimport time\n\nstart = torch.cuda.Event(enable_timing=True)\nend = torch.cuda.Event(enable_timing=True)\n\ngpu = torch.device(\"cuda:0\")\n\nn = 7000\n\ndef matmul_wrap(x, y):\n    z = torch.matmul(x, y)\n    return(z)\n    \n## Generate data on the CPU.    \nx = torch.randn(n,n)\ny = torch.randn(n,n)\n\n## Copy the objects to the GPU.\nx_gpu = x.cuda() # or: `x.to(\"cuda\")`\ny_gpu = y.cuda()\n    \ntorch.set_num_threads(1)\n    \nt0 = time.time()\nz = matmul_wrap(x, y)\nprint(time.time() - t0)  # 6.8 sec.\n\nstart.record()\nz_gpu = matmul_wrap(x_gpu, y_gpu)\ntorch.cuda.synchronize()\nend.record()\nprint(start.elapsed_time(end))  # 70 milliseconds (ms)\n\nSo we achieved a speedup of about 100-fold over a single CPU core using an A100 GPU in this case.\nLet’s consider the time for copying data to the GPU:\n\nx = torch.randn(n,n)\nstart.record()\nx_gpu = x.cuda()\ntorch.cuda.synchronize()\nend.record()\nprint(start.elapsed_time(end))  # 60 ms\n\nThis suggests that the time in copying the data is similar to that for doing the matrix multiplication.\nWe can generate data on the GPU like this:\n\nx_gpu = torch.randn(n,n, device=gpu)\n\n\n\n\nHere we’ll consider using the GPU for vectorized calculations. We’ll compare using numpy, CPU-based PyTorch, and GPU-based PyTorch, again with 32-bit numbers.\n\nimport torch\nimport numpy as np\nimport time\n\nstart = torch.cuda.Event(enable_timing=True)\nend = torch.cuda.Event(enable_timing=True)\n\ngpu = torch.device(\"cuda:0\")\n\ndef myfun_np(x):\n    y = np.exp(x) + 3 * np.sin(x)\n    return(y)\n\ndef myfun_torch(x):\n    y = torch.exp(x) + 3 * torch.sin(x)\n    return(y)\n    \n    \nn = 250000000\nx = torch.randn(n)\nx_gpu = x.cuda() # or: `x.to(\"cuda\")`\ntmp = np.random.normal(size = n)\nx_np = tmp.astype(np.float32)  # for fair comparison\n\n## numpy\nt0 = time.time()\ny_np = myfun_np(x_np)\ntime.time()-t0   # 1.2 sec.\n\n## CPU-based torch (1 thread)\ntorch.set_num_threads(1)\nstart.record()\ny = myfun_torch(x)\nend.record()\nprint(start.elapsed_time(end))  # 2200 ms (2.2 sec.)\n\n## GPU-based torch\nstart.record()\ny_gpu = myfun_torch(x_gpu)\ntorch.cuda.synchronize()\nend.record()\nprint(start.elapsed_time(end))   # 9 ms\n\nSo using the GPU speeds things up by 150-fold (compared to numpy) and 250-fold (compared to CPU-based PyTorch).\nOne can also have PyTorch “fuse” the operations in the loop, which avoids having the different vectorized operations in myfun being done in separate loops under the hood. For an overview of loop fusion, see this discussion in the context of Julia.\nTo fuse the operations, we need to have the function in a module. In this case I defined myfun_torch in myfun_torch.py, and we need to compile the code using torch.jit.script.\n\nfrom myfun_torch import myfun_torch as myfun_torch_tmp\nmyfun_torch_compiled = torch.jit.script(myfun_torch_tmp)\n\n## CPU plus loop fusion\nstart.record()\ny = myfun_torch_compiled(x)\nend.record()\nprint(start.elapsed_time(end))   # 1000 ms (1 sec.)\n\n## GPU plus loop fusion\nstart.record()\ny_gpu = myfun_torch_compiled(x_gpu)\ntorch.cuda.synchronize()\nend.record()\nprint(start.elapsed_time(end))   # 3.5 ms\n\nSo that seems to give a 2-3 fold speedup compared to without loop fusion.\n\n\n\nOne can also use PyTorch to run computations on the GPU that comes with Apple’s M2 chips.\nThe “backend” is called “MPS”, where “M” stands for “Metal”, which is what Apple calls its GPU framework.\n\nimport torch\nimport time\n\nstart = torch.mps.Event(enable_timing=True)\nend = torch.mps.Event(enable_timing=True)\n\nmps_device = torch.device(\"mps\")\n\nn = 10000\nx = torch.randn(n,n)\ny = torch.randn(n,n) \n\nx_mps = x.to(\"mps\")\ny_mps = y.to(\"mps\")\n    \n## On the CPU    \ntorch.set_num_threads(1)\n\nt0 = time.time()\nz = matmul_wrap(x, y)\nprint(time.time() - t0)   # 1.8 sec (1800 ms)\n\n## On the M2 GPU\nstart.record()\nz_mps = matmul_wrap(x_mps, y_mps)\ntorch.mps.synchronize()\nend.record()\nprint(start.elapsed_time(end)) # 950 ms\n\nSo there is about a two-fold speed up, which isn’t impressive compared to the speedup on a standard GPU.\nLet’s see how much time is involved in transferring the data.\n\nx = torch.randn(n,n)\n\nstart.record()\nx_mps = x.to(\"mps\")\ntorch.mps.synchronize()\nend.record()\nprint(start.elapsed_time(end))  # 35 ms.\n\nSo it looks like the transfer time is pretty small compared to the computation time (and to the savings involved in using the M2 GPU).\nWe can generate data on the GPU like this:\n\nx_mps = torch.randn(n,n, device=mps_device)",
    "crumbs": [
      "Parallel Python"
    ]
  },
  {
    "objectID": "parallel-python.html#using-jax-for-cpu-and-gpu",
    "href": "parallel-python.html#using-jax-for-cpu-and-gpu",
    "title": "Parallel processing in Python",
    "section": "",
    "text": "You can think of JAX as a version of numpy enabled to use the GPU (or automatically parallelize on CPU threads) and provide automatic differentiation.\nOne can also use just-in-time (JIT) compilation with JAX. Behind the scenes, the instructions are compiled to machine code for different backends (e.g., CPU and GPU) using XLA.\n\n\nLet’s first consider running a vectorized calculation using JAX on the CPU, which will use multiple threads, each thread running on a separate CPU core on our computer.\n\nimport time\nimport numpy as np\nimport jax.numpy as jnp\n\ndef myfun_np(x):\n    y = np.exp(x) + 3 * np.sin(x)\n    return(y)\n    \ndef myfun_jnp(x):\n    y = jnp.exp(x) + 3 * jnp.sin(x)\n    return(y)\n\nn = 250000000\n\nx = np.random.normal(size = n).astype(np.float32)  # for consistency\nx_jax = jnp.array(x)  # 32-bit by default\nprint(x_jax.platform())\n\ncpu\n\n\n\nt0 = time.time()\nz = myfun_np(x)\nt1 = time.time() - t0\n\nt0 = time.time()\nz_jax = myfun_jnp(x_jax).block_until_ready()\nt2 = time.time() - t0\n\nprint(f\"numpy time: {round(t1,3)}\\njax time: {round(t2,3)}\")\n\nnumpy time: 3.148\njax time: 1.555\n\n\nThere’s a nice speedup compared to numpy.\nSince JAX will often execute computations asynchronously (in particular when using the GPU), the block_until_ready invocation ensures that the computation finishes before we stop timing.\nBy default the JAX floating point type is 32-bit so we forced the use of 32-bit numbers for numpy for comparability. One could have JAX use 64-bit numbers like this:\n\nimport jax\njax.config.update(\"jax_enable_x64\", True)  \n\nNext let’s consider JIT compiling it, which should fuse the vectorized operations and avoid temporary objects. The JAX docs have a nice discussion of when JIT compilation will be beneficial.\n\nimport jax\nmyfun_jnp_jit = jax.jit(myfun_jnp)\n\nt0 = time.time()\nz_jax_jit = myfun_jnp_jit(x_jax).block_until_ready()\nt3 = time.time() - t0\nprint(f\"jitted jax time: {round(t3,3)}\")\n\njitted jax time: 0.824\n\n\nSo that gives another almost 2x speedup.\n\n\n\nLinear algebra in JAX will use multiple threads (as discussed for numpy). Here we’ll compare 64-bit calculation, since matrix decompositions sometimes need more precision.\n\nn = 7000\nx = np.random.normal(0, 1, size=(n, n))\n\nt0 = time.time()\nmat = x.T @ x\nprint(\"numpy time:\")\nprint(round(time.time() - t0,3))\n\nt0 = time.time()\nU = np.linalg.cholesky(mat) \nprint(round(time.time() - t0,3))\n\nnumpy time:\n3.544\n1.561\n\n\n\nimport jax\njax.config.update(\"jax_enable_x64\", True)  \n\nx_jax = jnp.array(x, dtype = jnp.float64)\nprint(f\"JAX dtype is {x_jax.dtype}\")\n\nt0 = time.time()\nmat_jax = jnp.matmul(x_jax.transpose(), x_jax)\nprint(\"jax time:\")\nprint(round(time.time() - t0,3))\n\nt0 = time.time()\nU_jax = jnp.linalg.cholesky(mat_jax)\nprint(round(time.time() - t0,3))\n\nJAX dtype is float64\njax time:\n9.018\n2.004\n\n\nSo here the matrix multiplication is slower using JAX with 64-bit numbers but the Cholesky is a bit faster. If one uses 32-bit numbers, JAX is faster for both (not shown).\nIn general, the JAX speedups are not huge, which is not surprising given both approaches are using multiple threads to carry out the linear algebra. At the least it indicates one can move a numpy workflow to JAX without worrying about losing the threaded BLAS speed of numpy.\n\n\n\nGetting threaded CPU computation automatically is nice, but the real benefit of JAX comes in offloading computations to the GPU (and in providing automatic differentiation, not discussed in this tutorial). If a GPU is available and a GPU-enabled JAX is installed, JAX will generally try to use the GPU.\nNote my general comments about using the GPU in the PyTorch section.\nNote that for this section, I’m pasting in the output when running the code separately on a machine with a GPU because this document is generated on a machine without a GPU.\nWe’ll just repeat the experiments we ran earlier comparing numpy- and JAX-based calculations, but on a machine with an A100 GPU.\n\nimport time\nimport numpy as np\nimport jax.numpy as jnp\n\ndef myfun_np(x):\n    y = np.exp(x) + 3 * np.sin(x)\n    return(y)\n    \ndef myfun_jnp(x):\n    y = jnp.exp(x) + 3 * jnp.sin(x)\n    return(y)\n\nn = 250000000\n\nx = np.random.normal(size = n).astype(np.float32)  # for consistency\nx_jax = jnp.array(x)  # 32-bit by default\nprint(x_jax.platform())    # gpu\n\nt0 = time.time()\nz = myfun_np(x)\nprint(time.time() - t0)    # 1.15 s.\n\nt0 = time.time()\nz_jax = myfun_jnp(x_jax).block_until_ready()\nprint(time.time() - t0)    # 0.0099 s.\n\nSo that gives a speedup of more than 100x.\n\nimport jax\nmyfun_jnp_jit = jax.jit(myfun_jnp)\n\nt0 = time.time()\nz_jax_jit = myfun_jnp_jit(x_jax).block_until_ready()  # 0.0052 s.\nprint(time.time() - t0)\n\nJIT compilation helps a bit (about 2x).\nFinally, here’s the linear algebra example on the GPU.\n\nn = 7000\nx = np.random.normal(0, 1, size=(n, n)).astype(np.float32) # for consistency\n\nt0 = time.time()\nmat = x.T @ x\nprint(time.time() - t0)    # 3.7 s.\n\nt0 = time.time()\nU = np.linalg.cholesky(mat)  # 3.3 s.\nprint(time.time() - t0)\n\n\nx_jax = jnp.array(x)\n\nt0 = time.time()\nmat_jax = jnp.matmul(x_jax.transpose(), x_jax).block_until_ready()\nprint(time.time() - t0)    # 0.025 sec.\n\nt0 = time.time()\nU_jax = jnp.linalg.cholesky(mat_jax).block_until_ready()\nprint(time.time() - t0)   # 0.08 s.\n\nAgain we get a very impressive speedup.\n\n\n\nAs discussed elsewhere in this tutorial, it takes time to transfer data to and from the GPU, so it’s best to generate values on the GPU and keep objects on the GPU when possible.\nAlso, JAX objects are designed to be manipulated as objects, rather than manipulating individual values.\n\nx_jax[0,0] = 3.17\n\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/scratch/users/paciorek/conda/envs/jax-test/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py\", line 278, in _unimplemented_setitem\n    raise TypeError(msg.format(type(self)))\nTypeError: '&lt;class 'jaxlib.xla_extension.ArrayImpl'&gt;' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\n\n\n\nWe can use JAX’s vmap to automatically vectorize a map operation. Unlike numpy’s vectorize or apply_along_axis, which are just handy syntax (“syntactic sugar”) and don’t actually speed anything up (because the looping is still done in Python), vmap actually vectorizes the loop. Behind the scenes it generates a vectorized version of the code that can run in parallel on CPU or GPU.\nIn general, one would use this to automatically iterate over the dimension(s) of one or more arrays. This is convenient from a coding perspective (compared to explicitly writing a loop) and potentially speeds up the computation based on parallelization and by avoiding the overhead of looping at the Python level.\nHere we’ll standardize each column of an array using vmap rather than writing a loop over the columns.\n\nimport jax\nimport jax.numpy as jnp\nimport time\n\nnr = 10000\nnc = 10000\nx = np.random.normal(size = (nr,nc)).astype(np.float32)  # for consistency\nx_jax = jnp.array(x) \n\ndef f(x):\n    ## Standardize a vector by its range.\n    return x / (np.max(x) - np.min(x))\n\ndef f_jax(x):\n    return x / (jnp.max(x) - jnp.min(x))\n\n# Standardize each column.\n\nt0 = time.time()\nout = np.apply_along_axis(f, 0, x)  \nt1 = time.time() - t0\n\n# JAX vmap numbers axes in reverse order of numpy, apparently.\nf_jax_vmap = jax.vmap(f_jax, in_axes=1, out_axes=1)\n\nt0 = time.time()\nout_jax = f_jax_vmap(x_jax).block_until_ready()     \nt2 = time.time() - t0\nprint(f\"numpy time: {round(t1,3)}\\njax vmap time: {round(t2,3)}\")\n\nnumpy time: 3.512\njax vmap time: 1.269\n\n\nThat gives a nice speedup. Let’s also try JIT’ing it. That gives a further speedup.\n\nf_jax_vmap_jit = jax.jit(f_jax_vmap)\n\nt0 = time.time()\nout_jax_jit = f_jax_vmap_jit(x_jax).block_until_ready()\nt3 = time.time() - t0\nprint(f\"jitted jax vmap time: {round(t3,3)}\")\n\njitted jax vmap time: 0.318\n\n\nIt would make sense to explore the benefits of using a GPU here, though I haven’t done so.\nvmap has a lot of flexibility to operate on various axes of its input arguments (and structure the output axes). Suppose we want to do the same standardization but using the columns of a different array as what to standardize based on.\n\ny = np.random.normal(size = (nr,nc)).astype(np.float32)\ny_jax = jnp.array(y) \n\ndef f2_jax(x, y):\n    return x / (jnp.max(y) - jnp.min(y))\n\nout2 = jax.vmap(f2_jax, in_axes=(1,1), out_axes=1)(x_jax, y_jax)\nf2_jax_jit = jax.jit(jax.vmap(f2_jax, in_axes=(1,1), out_axes=1)) \nout3 = f2_jax_jit(x_jax, y_jax)\n\nFinally, note that pmap is a function with a similar-sounding name that allows one to parallelize a map operation over multiple devices (e.g., multiple GPUs).",
    "crumbs": [
      "Parallel Python"
    ]
  },
  {
    "objectID": "parallel-python.html#using-cupy",
    "href": "parallel-python.html#using-cupy",
    "title": "Parallel processing in Python",
    "section": "",
    "text": "CuPy is another package allowing one to execute numpy-type calculations on the GPU (Nvidia only). It has some similarity to JAX.\nHere’s a basic illustration, where we get a 175x speedup for generating a random matrix and matrix multiplication when using an A100 GPU.\n\nimport cupy\nimport numpy as np\nimport time\n\ndef matmul_np(n):\n    x = np.random.normal(size=(n,n))\n    z = np.matmul(x,x)\n    return(z)\n\ndef matmul_cupy(n):\n    x = cupy.random.normal(size=(n,n))\n    z = cupy.matmul(x,x)\n    return(z)\n\n\nn = 7000\n\nt0 = time.time()\nz = matmul_np(n)\nprint(time.time() - t0)   # 8.8 s.\n\nt0 = time.time()\nz_cupy = matmul_cupy(n)\ncupy.cuda.stream.get_current_stream().synchronize()\nprint(time.time() - t0)   # .05 s.\n\nYou can also use cupy.RawKernel to execute a GPU kernel written in CUDA C/C++ directly from Python. That’s a bit beyond our scope here, so I won’t show an example.",
    "crumbs": [
      "Parallel Python"
    ]
  },
  {
    "objectID": "parallel-matlab.html",
    "href": "parallel-matlab.html",
    "title": "Parallel processing in MATLAB",
    "section": "",
    "text": "MATLAB provides a variety of functionality for parallelization, including threaded operations (linear algebra and more), parallel for loops, and parallelization across multiple machines. This material will just scratch the surface of what is possible.\n\n\n\nMany MATLAB functions are automatically threaded (not just linear algebra), so you don’t need to do anything special in your code to take advantage of this. So if you’re running MATLAB and monitoring CPU usage (e.g., using top on Linux or OS X), you may see a process using more than 100% of CPU.\nHowever worker tasks within a parfor use only a single thread.\nMATLAB uses MKL for (threaded) linear algebra.\nThreading in MATLAB can be controlled in two ways. From within your MATLAB code you can set the number of threads, e.g., to four in this case:\nmaxNumCompThreads(4);\nTo use only a single thread, you can use 1 instead of 4 above, or you can start MATLAB with the singleCompThread flag:\nmatlab -singleCompThread ...\n\n\n\nTo run a loop in parallel in MATLAB, you can use the parfor construction. Before running the parfor you need to start up a set of workers using parpool. MATLAB will use only one thread per worker. Here is some demo code:\nnslots = 4; % to set manually\nmypool = parpool(nslots) \n\nn = 3000;\nnIts = 500;\nc = zeros(n, nIts);\nparfor i = 1:nIts\n     c(:,i) = eig(rand(n)); \nend\n\ndelete(mypool)\n\n% delete(gcp) works if you forget to name your pool by assigning the output of parpool to a variable\nMATLAB has a default limit on the number of workers in a pool, but you can modify your MATLAB settings as follows to increase that limit (in this case to allow up to 32 workers). It should work to run the following code once in a MATLAB session, which will modify the settings for future MATLAB sessions.\ncl = parcluster();\ncl.NumWorkers = 32;\nsaveProfile(cl);\nBy default MATLAB uses a single thread (core) per worker. However you can also use multiple threads. Here is how you can set that up.\ncl = parcluster();\ncl.NumThreads = 2;  % 2 threads per worker\ncl.parpool(4);      % 4 workers\n\n\n\nMATLAB also uses the Mersenne-Twister. We can set the seed as: rng(seed), with seed being a non-negative integer.\nHappily, like in R, we can set up independent streams, using either of the Combined Multiple Recursive (‘mrg32k3a’) and the Multiplicative Lagged Fibonacci (‘mlfg6331_64’) generators. Here’s an example, where we create the second of the 5 streams, as if we were using this code in the second of our parallel processes. The 'Seed',0 part is not actually needed as that is the default.\nthisStream = 2;\ntotalNumStreams = 5;\nseed = 0;\ncmrg1 = RandStream.create('mrg32k3a', 'NumStreams', totalNumStreams, \n   'StreamIndices', thisStream, 'Seed', seed); \nRandStream.setGlobalStream(cmrg1);\nrandn(5, 1)\n\n\n\nIn addition to using parfor in MATLAB, you can also explicitly program parallelization, managing the individual parallelized tasks. Here is some template code for doing this. We’ll submit our jobs to a pool of workers so that we have control over how many jobs are running at once. Note that here I submit 6 jobs that call the same function, but the different jobs could call different functions and have varying inputs and outputs. MATLAB will run as many jobs as available workers in the pool and will queue the remainder, starting them as workers in the pool become available. Here is some demo code\nfeature('numThreads', 1); \nncores = 4;\npool = parpool(ncores); \n% assume you have test.m with a function, test, taking two inputs \n% (n and seed) and returning 1 output\nn = 10000000;\njob = cell(1,6); \njob{1} = parfeval(pool, @test, 1, n, 1);  \njob{2} = parfeval(pool, @test, 1, n, 2);  \njob{3} = parfeval(pool, @test, 1, n, 3);  \njob{4} = parfeval(pool, @test, 1, n, 4);  \njob{5} = parfeval(pool, @test, 1, n, 5);  \njob{6} = parfeval(pool, @test, 1, n, 6);  \n\n% wait for outputs, in order\noutput = cell(1, 6);\nfor idx = 1:6\n  output{idx} = fetchOutputs(job{idx});\nend \n\n% alternative way to loop over jobs:\nfor idx = 1:6\n  jobs(idx) = parfeval(pool, @test, 1, n, idx); \nend \n\n% wait for outputs as they finish\noutput = cell(1, 6);\nfor idx = 1:6\n  [completedIdx, value] = fetchNext(jobs);\n  output{completedIdx} = value;\nend \n\ndelete(pool);\nAnd if you want to run threaded code in a given job, you can do that by setting the number of threads within the function called by parfeval. See the testThread.m file for the testThread function.\nncores = 8;\nn = 5000;\nnJobs = 4;\npool = parpool(nJobs);\n% pass number of threads as number of slots divided by number of jobs\n% testThread function should then do: \n% feature('numThreads', nThreads);\n% where nThreads is the name of the relevant argument to testThread\njobt1 = parfeval(pool, @testThread, 1, n, 1, nCores/nJobs);\njobt2 = parfeval(pool, @testThread, 1, n, 2, nCores/nJobs);\njobt3 = parfeval(pool, @testThread, 1, n, 3, nCores/nJobs);\njobt4 = parfeval(pool, @testThread, 1, n, 4, nCores/nJobs);\n\noutput1 = fetchOutputs(jobt1);\noutput2 = fetchOutputs(jobt2);\noutput3 = fetchOutputs(jobt3);\noutput4 = fetchOutputs(jobt4);\n\ndelete(pool);\n\n\n\nTo use MATLAB across multiple nodes, you need to have the MATLAB Parallel Server, which often requires an additional license. If it is installed, one can set up MATLAB so that parfor will distribute its work across multiple nodes. Details may vary depending on how Parallel Server is set up on your system.",
    "crumbs": [
      "Parallel MATLAB"
    ]
  },
  {
    "objectID": "parallel-matlab.html#overview",
    "href": "parallel-matlab.html#overview",
    "title": "Parallel processing in MATLAB",
    "section": "",
    "text": "MATLAB provides a variety of functionality for parallelization, including threaded operations (linear algebra and more), parallel for loops, and parallelization across multiple machines. This material will just scratch the surface of what is possible.",
    "crumbs": [
      "Parallel MATLAB"
    ]
  },
  {
    "objectID": "parallel-matlab.html#threading",
    "href": "parallel-matlab.html#threading",
    "title": "Parallel processing in MATLAB",
    "section": "",
    "text": "Many MATLAB functions are automatically threaded (not just linear algebra), so you don’t need to do anything special in your code to take advantage of this. So if you’re running MATLAB and monitoring CPU usage (e.g., using top on Linux or OS X), you may see a process using more than 100% of CPU.\nHowever worker tasks within a parfor use only a single thread.\nMATLAB uses MKL for (threaded) linear algebra.\nThreading in MATLAB can be controlled in two ways. From within your MATLAB code you can set the number of threads, e.g., to four in this case:\nmaxNumCompThreads(4);\nTo use only a single thread, you can use 1 instead of 4 above, or you can start MATLAB with the singleCompThread flag:\nmatlab -singleCompThread ...",
    "crumbs": [
      "Parallel MATLAB"
    ]
  },
  {
    "objectID": "parallel-matlab.html#parallel-for-loops-on-one-machine",
    "href": "parallel-matlab.html#parallel-for-loops-on-one-machine",
    "title": "Parallel processing in MATLAB",
    "section": "",
    "text": "To run a loop in parallel in MATLAB, you can use the parfor construction. Before running the parfor you need to start up a set of workers using parpool. MATLAB will use only one thread per worker. Here is some demo code:\nnslots = 4; % to set manually\nmypool = parpool(nslots) \n\nn = 3000;\nnIts = 500;\nc = zeros(n, nIts);\nparfor i = 1:nIts\n     c(:,i) = eig(rand(n)); \nend\n\ndelete(mypool)\n\n% delete(gcp) works if you forget to name your pool by assigning the output of parpool to a variable\nMATLAB has a default limit on the number of workers in a pool, but you can modify your MATLAB settings as follows to increase that limit (in this case to allow up to 32 workers). It should work to run the following code once in a MATLAB session, which will modify the settings for future MATLAB sessions.\ncl = parcluster();\ncl.NumWorkers = 32;\nsaveProfile(cl);\nBy default MATLAB uses a single thread (core) per worker. However you can also use multiple threads. Here is how you can set that up.\ncl = parcluster();\ncl.NumThreads = 2;  % 2 threads per worker\ncl.parpool(4);      % 4 workers",
    "crumbs": [
      "Parallel MATLAB"
    ]
  },
  {
    "objectID": "parallel-matlab.html#parallel-random-number-generation",
    "href": "parallel-matlab.html#parallel-random-number-generation",
    "title": "Parallel processing in MATLAB",
    "section": "",
    "text": "MATLAB also uses the Mersenne-Twister. We can set the seed as: rng(seed), with seed being a non-negative integer.\nHappily, like in R, we can set up independent streams, using either of the Combined Multiple Recursive (‘mrg32k3a’) and the Multiplicative Lagged Fibonacci (‘mlfg6331_64’) generators. Here’s an example, where we create the second of the 5 streams, as if we were using this code in the second of our parallel processes. The 'Seed',0 part is not actually needed as that is the default.\nthisStream = 2;\ntotalNumStreams = 5;\nseed = 0;\ncmrg1 = RandStream.create('mrg32k3a', 'NumStreams', totalNumStreams, \n   'StreamIndices', thisStream, 'Seed', seed); \nRandStream.setGlobalStream(cmrg1);\nrandn(5, 1)",
    "crumbs": [
      "Parallel MATLAB"
    ]
  },
  {
    "objectID": "parallel-matlab.html#manually-parallelizing-individual-tasks",
    "href": "parallel-matlab.html#manually-parallelizing-individual-tasks",
    "title": "Parallel processing in MATLAB",
    "section": "",
    "text": "In addition to using parfor in MATLAB, you can also explicitly program parallelization, managing the individual parallelized tasks. Here is some template code for doing this. We’ll submit our jobs to a pool of workers so that we have control over how many jobs are running at once. Note that here I submit 6 jobs that call the same function, but the different jobs could call different functions and have varying inputs and outputs. MATLAB will run as many jobs as available workers in the pool and will queue the remainder, starting them as workers in the pool become available. Here is some demo code\nfeature('numThreads', 1); \nncores = 4;\npool = parpool(ncores); \n% assume you have test.m with a function, test, taking two inputs \n% (n and seed) and returning 1 output\nn = 10000000;\njob = cell(1,6); \njob{1} = parfeval(pool, @test, 1, n, 1);  \njob{2} = parfeval(pool, @test, 1, n, 2);  \njob{3} = parfeval(pool, @test, 1, n, 3);  \njob{4} = parfeval(pool, @test, 1, n, 4);  \njob{5} = parfeval(pool, @test, 1, n, 5);  \njob{6} = parfeval(pool, @test, 1, n, 6);  \n\n% wait for outputs, in order\noutput = cell(1, 6);\nfor idx = 1:6\n  output{idx} = fetchOutputs(job{idx});\nend \n\n% alternative way to loop over jobs:\nfor idx = 1:6\n  jobs(idx) = parfeval(pool, @test, 1, n, idx); \nend \n\n% wait for outputs as they finish\noutput = cell(1, 6);\nfor idx = 1:6\n  [completedIdx, value] = fetchNext(jobs);\n  output{completedIdx} = value;\nend \n\ndelete(pool);\nAnd if you want to run threaded code in a given job, you can do that by setting the number of threads within the function called by parfeval. See the testThread.m file for the testThread function.\nncores = 8;\nn = 5000;\nnJobs = 4;\npool = parpool(nJobs);\n% pass number of threads as number of slots divided by number of jobs\n% testThread function should then do: \n% feature('numThreads', nThreads);\n% where nThreads is the name of the relevant argument to testThread\njobt1 = parfeval(pool, @testThread, 1, n, 1, nCores/nJobs);\njobt2 = parfeval(pool, @testThread, 1, n, 2, nCores/nJobs);\njobt3 = parfeval(pool, @testThread, 1, n, 3, nCores/nJobs);\njobt4 = parfeval(pool, @testThread, 1, n, 4, nCores/nJobs);\n\noutput1 = fetchOutputs(jobt1);\noutput2 = fetchOutputs(jobt2);\noutput3 = fetchOutputs(jobt3);\noutput4 = fetchOutputs(jobt4);\n\ndelete(pool);",
    "crumbs": [
      "Parallel MATLAB"
    ]
  },
  {
    "objectID": "parallel-matlab.html#using-matlab-across-multiple-nodes",
    "href": "parallel-matlab.html#using-matlab-across-multiple-nodes",
    "title": "Parallel processing in MATLAB",
    "section": "",
    "text": "To use MATLAB across multiple nodes, you need to have the MATLAB Parallel Server, which often requires an additional license. If it is installed, one can set up MATLAB so that parfor will distribute its work across multiple nodes. Details may vary depending on how Parallel Server is set up on your system.",
    "crumbs": [
      "Parallel MATLAB"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Parallel processing in Python, R, Julia, MATLAB, and C/C++",
    "section": "",
    "text": "This tutorial covers the use of parallelization (on either one machine or multiple machines/nodes) in Python, R, Julia, MATLAB and C/C++ and use of the GPU in Python and Julia. Please click on the links above for material specific to each language.\nYou should be able to replicate much of what is covered here provided you have the relevant software on your computer, but some of the parallelization approaches may not work on Windows, and of course the GPU-based pieces require a GPU.\nThis tutorial assumes you have a working knowledge of the relevant language, but not necessarily knowledge of parallelization.\nMaterials for this tutorial, including the Markdown files and associated code files that were used to create these documents are available on GitHub in the gh-pages branch. You can download the files by doing a git clone from a terminal window on a UNIX-like machine, as follows:\ngit clone https://github.com/berkeley-scf/tutorial-parallelization\nThis tutorial by Christopher Paciorek of the UC Berkeley Statistical Computing Facility is licensed under a Creative Commons Attribution 3.0 Unported License.\n\n\n\n\n\nEveryday personal computers usually have more than one processor (more than one chip) and on a given processor, often have more than one core (multi-core). A multi-core processor has multiple processors on a single computer chip. On personal computers, all the processors and cores share the same memory. For the purpose of this tutorial, there is little practical distinction between multi-processor and multi-core situations. The main issue is whether processes share memory or not. In general, I won’t distinguish between cores and processors. We’ll just focus on the number of cores on given personal computer or a given node in a cluster.\nSupercomputers and computer clusters generally have tens, hundreds, or thousands of ‘nodes’, linked by a fast local network. Each node is essentially a computer with its own cores and memory. Memory is local to each node (distributed memory). One basic principle is that communication between a processor and its memory is much faster than communication across nodes between processors accessing different memory.\n\n\n\n\ncores: We’ll use this term to mean the different processing units available on a single machine or node.\nnodes: We’ll use this term to mean the different computers, each with their own distinct memory, that make up a cluster or supercomputer.\nprocesses: instances of a program executing on a machine; multiple processes may be executing at once. A given executable (e.g., Python or R) may start up multiple processes at once. Ideally we have no more user processes than cores on a node.\nworkers: the individual processes that are carrying out the (parallelized) computation. We’ll use worker and process interchangeably.\ntasks: This term gets used in various ways (including in place of ‘processes’ in the context of Slurm and MPI), but we’ll use it to refer to the individual computational items you want to complete - e.g., one task per cross-validation fold or one task per simulation replicate/iteration.\nthreads: multiple paths of execution within a single process; the OS sees the threads as a single process, but one can think of them as ‘lightweight’ processes. Ideally when considering the processes and their threads, we would have the number of total threads across all processes not exceed the number of cores on a node.\nforking: child processes are spawned that are identical to the parent, but with different process IDs and their own memory. In some cases if objects are not changed, the objects in the child process may refer back to the original objects in the original process, avoiding making copies.\nsockets: some of R’s parallel functionality involves creating new R processes (e.g., starting processes via Rscript) and communicating with them via a communication technology called sockets.\nscheduler: a program that manages users’ jobs on a cluster.\nload-balanced: when all the cores that are part of a computation are busy for the entire period of time the computation is running.\n\n\n\n\n\nThere are two basic flavors of parallel processing (leaving aside GPUs): shared memory (single machine) and distributed memory (multiple machines). With shared memory, multiple processors (which I’ll call cores) share the same memory. With distributed memory, you have multiple nodes, each with their own memory. You can think of each node as a separate computer connected by a fast network.\n\n\nFor shared memory parallelism, each core is accessing the same memory so there is no need to pass information (in the form of messages) between different machines.\nHowever, except for certain special situations (involving software threads or forked processes), the different worker processes on a given machine do not share objects in memory. So most often, one has multiple copies of the same objects, one per worker process.\n\n\nThreads are multiple paths of execution within a single process. If you are monitoring CPU usage (such as with top in Linux or Mac) and watching a job that is executing threaded code, you’ll see the process using more than 100% of CPU. When this occurs, the process is using multiple cores, although it appears as a single process rather than as multiple processes.\nNote that this is a different notion than a processor that is hyperthreaded. With hyperthreading a single core appears as two cores to the operating system.\nThreads generally do share objects in memory, thereby allowing us to have a single copy of objects instead of one per thread.\nOne very common use of threading is for linear algebra, using threaded linear alebra packages accessed from Python, R, MATLAB, or C/C++.\n\n\n\n\nParallel programming for distributed memory parallelism requires passing messages containing information (code, data, etc.) between the different nodes.\nA standard protocol for passing messages is MPI, of which there are various versions, including openMPI.\nTools such as ipyparallel, Dask and Ray in Python and R’s future package all manage the work of moving information between nodes for you (and don’t generally use MPI).\n\n\n\nGPUs (Graphics Processing Units) are processing units originally designed for rendering graphics on a computer quickly. This is done by having a large number of simple processing units for massively parallel calculation. The idea of general purpose GPU (GPGPU) computing is to exploit this capability for general computation.\nMost researchers don’t program for a GPU directly but rather use software (often machine learning software such as Tensorflow or PyTorch) that has been programmed to take advantage of a GPU if one is available.\n\n\n\nSpark and Hadoop are systems for implementing computations in a distributed memory environment, using the MapReduce approach.\nWe won’t cover this topic in this tutorial.\nNote that Dask provides a lot of the same functionality as Spark, allowing one to create distributed datasets where pieces of the dataset live on different machines but can be treated as a single dataset from the perspective of the user.\n\n\n\n\nSome of the considerations that apply when thinking about how effective a given parallelization approach will be include:\n\nthe amount of memory that will be used by the various processes,\nthe amount of communication that needs to happen – how much data will need to be passed between processes,\nthe latency of any communication - how much delay/lag is there in sending data between processes or starting up a worker process, and\nto what extent do processes have to wait for other processes to finish before they can do their next step.\n\nThe following are some basic principles/suggestions for how to parallelize your computation.\n\nShould I use one machine/node or many machines/nodes? - If you can do your computation on the cores of a single node using shared memory, that will be faster than using the same number of cores (or even somewhat more cores) across multiple nodes. Similarly, jobs with a lot of data/high memory requirements that one might think of as requiring Spark or Hadoop may in some cases be much faster if you can find a single machine with a lot of memory. - That said, if you would run out of memory on a single node, then you’ll need to use distributed memory.\nWhat level or dimension should I parallelize over?\n\nIf you have nested loops, you generally only want to parallelize at one level of the code. That said, in this unit we’ll see some tools for parallelizing at multiple levels. Keep in mind whether your linear algebra is being threaded. Often you will want to parallelize over a loop and not use threaded linear algebra within the iterations of the loop.\nOften it makes sense to parallelize the outer loop when you have nested loops.\nYou generally want to parallelize in such a way that your code is load-balanced and does not involve too much communication.\n\nHow do I balance communication overhead with keeping my cores busy?\n\nIf you have very few tasks, particularly if the tasks take different amounts of time, often some processors will be idle and your code poorly load-balanced.\nIf you have very many tasks and each one takes little time, the overhead of starting and stopping the tasks will reduce efficiency.\n\nShould multiple tasks be pre-assigned (statically assigned) to a process (i.e., a worker) (sometimes called prescheduling) or should tasks be assigned dynamically as previous tasks finish?\n\nTo illustrate the difference, suppose you have 6 tasks and 3 workers. If the tasks are pre-assigned, worker 1 might be assigned tasks 1 and 4 at the start, worker 2 assigned tasks 2 and 5, and worker 3 assigned tasks 3 and 6. If the tasks are dynamically assigned, worker 1 would be assigned task 1, worker 2 task 2, and worker 3 task 3. Then whichever worker finishes their task first (it woudn’t necessarily be worker 1) would be assigned task 4 and so on.\nBasically if you have many tasks that each take similar time, you want to preschedule the tasks to reduce communication. If you have few tasks or tasks with highly variable completion times, you don’t want to preschedule, to improve load-balancing.\nFor R in particular, some of R’s parallel functions allow you to say whether the tasks should be prescheduled. In the future package, future_lapply has arguments future.scheduling and future.chunk.size. Similarly, there is the mc.preschedule argument in mclapply().",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#this-tutorial",
    "href": "index.html#this-tutorial",
    "title": "Parallel processing in Python, R, Julia, MATLAB, and C/C++",
    "section": "",
    "text": "This tutorial covers the use of parallelization (on either one machine or multiple machines/nodes) in Python, R, Julia, MATLAB and C/C++ and use of the GPU in Python and Julia. Please click on the links above for material specific to each language.\nYou should be able to replicate much of what is covered here provided you have the relevant software on your computer, but some of the parallelization approaches may not work on Windows, and of course the GPU-based pieces require a GPU.\nThis tutorial assumes you have a working knowledge of the relevant language, but not necessarily knowledge of parallelization.\nMaterials for this tutorial, including the Markdown files and associated code files that were used to create these documents are available on GitHub in the gh-pages branch. You can download the files by doing a git clone from a terminal window on a UNIX-like machine, as follows:\ngit clone https://github.com/berkeley-scf/tutorial-parallelization\nThis tutorial by Christopher Paciorek of the UC Berkeley Statistical Computing Facility is licensed under a Creative Commons Attribution 3.0 Unported License.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#some-useful-terminology",
    "href": "index.html#some-useful-terminology",
    "title": "Parallel processing in Python, R, Julia, MATLAB, and C/C++",
    "section": "",
    "text": "Everyday personal computers usually have more than one processor (more than one chip) and on a given processor, often have more than one core (multi-core). A multi-core processor has multiple processors on a single computer chip. On personal computers, all the processors and cores share the same memory. For the purpose of this tutorial, there is little practical distinction between multi-processor and multi-core situations. The main issue is whether processes share memory or not. In general, I won’t distinguish between cores and processors. We’ll just focus on the number of cores on given personal computer or a given node in a cluster.\nSupercomputers and computer clusters generally have tens, hundreds, or thousands of ‘nodes’, linked by a fast local network. Each node is essentially a computer with its own cores and memory. Memory is local to each node (distributed memory). One basic principle is that communication between a processor and its memory is much faster than communication across nodes between processors accessing different memory.\n\n\n\n\ncores: We’ll use this term to mean the different processing units available on a single machine or node.\nnodes: We’ll use this term to mean the different computers, each with their own distinct memory, that make up a cluster or supercomputer.\nprocesses: instances of a program executing on a machine; multiple processes may be executing at once. A given executable (e.g., Python or R) may start up multiple processes at once. Ideally we have no more user processes than cores on a node.\nworkers: the individual processes that are carrying out the (parallelized) computation. We’ll use worker and process interchangeably.\ntasks: This term gets used in various ways (including in place of ‘processes’ in the context of Slurm and MPI), but we’ll use it to refer to the individual computational items you want to complete - e.g., one task per cross-validation fold or one task per simulation replicate/iteration.\nthreads: multiple paths of execution within a single process; the OS sees the threads as a single process, but one can think of them as ‘lightweight’ processes. Ideally when considering the processes and their threads, we would have the number of total threads across all processes not exceed the number of cores on a node.\nforking: child processes are spawned that are identical to the parent, but with different process IDs and their own memory. In some cases if objects are not changed, the objects in the child process may refer back to the original objects in the original process, avoiding making copies.\nsockets: some of R’s parallel functionality involves creating new R processes (e.g., starting processes via Rscript) and communicating with them via a communication technology called sockets.\nscheduler: a program that manages users’ jobs on a cluster.\nload-balanced: when all the cores that are part of a computation are busy for the entire period of time the computation is running.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#types-of-parallel-processing",
    "href": "index.html#types-of-parallel-processing",
    "title": "Parallel processing in Python, R, Julia, MATLAB, and C/C++",
    "section": "",
    "text": "There are two basic flavors of parallel processing (leaving aside GPUs): shared memory (single machine) and distributed memory (multiple machines). With shared memory, multiple processors (which I’ll call cores) share the same memory. With distributed memory, you have multiple nodes, each with their own memory. You can think of each node as a separate computer connected by a fast network.\n\n\nFor shared memory parallelism, each core is accessing the same memory so there is no need to pass information (in the form of messages) between different machines.\nHowever, except for certain special situations (involving software threads or forked processes), the different worker processes on a given machine do not share objects in memory. So most often, one has multiple copies of the same objects, one per worker process.\n\n\nThreads are multiple paths of execution within a single process. If you are monitoring CPU usage (such as with top in Linux or Mac) and watching a job that is executing threaded code, you’ll see the process using more than 100% of CPU. When this occurs, the process is using multiple cores, although it appears as a single process rather than as multiple processes.\nNote that this is a different notion than a processor that is hyperthreaded. With hyperthreading a single core appears as two cores to the operating system.\nThreads generally do share objects in memory, thereby allowing us to have a single copy of objects instead of one per thread.\nOne very common use of threading is for linear algebra, using threaded linear alebra packages accessed from Python, R, MATLAB, or C/C++.\n\n\n\n\nParallel programming for distributed memory parallelism requires passing messages containing information (code, data, etc.) between the different nodes.\nA standard protocol for passing messages is MPI, of which there are various versions, including openMPI.\nTools such as ipyparallel, Dask and Ray in Python and R’s future package all manage the work of moving information between nodes for you (and don’t generally use MPI).\n\n\n\nGPUs (Graphics Processing Units) are processing units originally designed for rendering graphics on a computer quickly. This is done by having a large number of simple processing units for massively parallel calculation. The idea of general purpose GPU (GPGPU) computing is to exploit this capability for general computation.\nMost researchers don’t program for a GPU directly but rather use software (often machine learning software such as Tensorflow or PyTorch) that has been programmed to take advantage of a GPU if one is available.\n\n\n\nSpark and Hadoop are systems for implementing computations in a distributed memory environment, using the MapReduce approach.\nWe won’t cover this topic in this tutorial.\nNote that Dask provides a lot of the same functionality as Spark, allowing one to create distributed datasets where pieces of the dataset live on different machines but can be treated as a single dataset from the perspective of the user.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#parallelization-strategies",
    "href": "index.html#parallelization-strategies",
    "title": "Parallel processing in Python, R, Julia, MATLAB, and C/C++",
    "section": "",
    "text": "Some of the considerations that apply when thinking about how effective a given parallelization approach will be include:\n\nthe amount of memory that will be used by the various processes,\nthe amount of communication that needs to happen – how much data will need to be passed between processes,\nthe latency of any communication - how much delay/lag is there in sending data between processes or starting up a worker process, and\nto what extent do processes have to wait for other processes to finish before they can do their next step.\n\nThe following are some basic principles/suggestions for how to parallelize your computation.\n\nShould I use one machine/node or many machines/nodes? - If you can do your computation on the cores of a single node using shared memory, that will be faster than using the same number of cores (or even somewhat more cores) across multiple nodes. Similarly, jobs with a lot of data/high memory requirements that one might think of as requiring Spark or Hadoop may in some cases be much faster if you can find a single machine with a lot of memory. - That said, if you would run out of memory on a single node, then you’ll need to use distributed memory.\nWhat level or dimension should I parallelize over?\n\nIf you have nested loops, you generally only want to parallelize at one level of the code. That said, in this unit we’ll see some tools for parallelizing at multiple levels. Keep in mind whether your linear algebra is being threaded. Often you will want to parallelize over a loop and not use threaded linear algebra within the iterations of the loop.\nOften it makes sense to parallelize the outer loop when you have nested loops.\nYou generally want to parallelize in such a way that your code is load-balanced and does not involve too much communication.\n\nHow do I balance communication overhead with keeping my cores busy?\n\nIf you have very few tasks, particularly if the tasks take different amounts of time, often some processors will be idle and your code poorly load-balanced.\nIf you have very many tasks and each one takes little time, the overhead of starting and stopping the tasks will reduce efficiency.\n\nShould multiple tasks be pre-assigned (statically assigned) to a process (i.e., a worker) (sometimes called prescheduling) or should tasks be assigned dynamically as previous tasks finish?\n\nTo illustrate the difference, suppose you have 6 tasks and 3 workers. If the tasks are pre-assigned, worker 1 might be assigned tasks 1 and 4 at the start, worker 2 assigned tasks 2 and 5, and worker 3 assigned tasks 3 and 6. If the tasks are dynamically assigned, worker 1 would be assigned task 1, worker 2 task 2, and worker 3 task 3. Then whichever worker finishes their task first (it woudn’t necessarily be worker 1) would be assigned task 4 and so on.\nBasically if you have many tasks that each take similar time, you want to preschedule the tasks to reduce communication. If you have few tasks or tasks with highly variable completion times, you don’t want to preschedule, to improve load-balancing.\nFor R in particular, some of R’s parallel functions allow you to say whether the tasks should be prescheduled. In the future package, future_lapply has arguments future.scheduling and future.chunk.size. Similarly, there is the mc.preschedule argument in mclapply().",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "parallel-julia.html",
    "href": "parallel-julia.html",
    "title": "Parallel processing in Julia",
    "section": "",
    "text": "Julia provides built-in support for various kinds of parallel processing on one or more machines. This material focuses on some standard approaches that are (mostly) analogous to functionality in Python and R. However there is other functionality available, including the ability to control tasks and sending data between processes in a fine-grained way.\nIn addition to parallelization, the second to last section discusses some issues related to efficiency with for loops, in particular fused operations. This is not directly related to parallelization but given the focus on loops in this document, it’s useful and interesting to know about.\nFinally, the last section discussed offloading computation to the GPU, i.e., massive parallelization on many GPU cores.",
    "crumbs": [
      "Parallel Julia"
    ]
  },
  {
    "objectID": "parallel-julia.html#overview",
    "href": "parallel-julia.html#overview",
    "title": "Parallel processing in Julia",
    "section": "",
    "text": "Julia provides built-in support for various kinds of parallel processing on one or more machines. This material focuses on some standard approaches that are (mostly) analogous to functionality in Python and R. However there is other functionality available, including the ability to control tasks and sending data between processes in a fine-grained way.\nIn addition to parallelization, the second to last section discusses some issues related to efficiency with for loops, in particular fused operations. This is not directly related to parallelization but given the focus on loops in this document, it’s useful and interesting to know about.\nFinally, the last section discussed offloading computation to the GPU, i.e., massive parallelization on many GPU cores.",
    "crumbs": [
      "Parallel Julia"
    ]
  },
  {
    "objectID": "parallel-julia.html#threading",
    "href": "parallel-julia.html#threading",
    "title": "Parallel processing in Julia",
    "section": "2 Threading",
    "text": "2 Threading\nThreaded calculations are done in parallel on software threads.\nThreads share objects in memory with the parent process, which is useful for avoiding copies but raises the danger of a “race condition”, where different threads modify data that other threads are using and cause errors..\n\n2.1 Threaded linear algebra\nAs with Python and R, Julia uses BLAS, a standard library of basic linear algebra operations (written in Fortran or C), for linear algebra operations. A fast BLAS can greatly speed up linear algebra relative to the default BLAS on a machine. Julia uses a fast, open source, free BLAS library called OpenBLAS. In addition to being fast when used on a single core, the openBLAS library is threaded - if your computer has multiple cores and there are free resources, your linear algebra will use multiple cores\nHere’s an example.\n\nusing BenchmarkTools\n\nusing LinearAlgebra\nusing Distributions\nn = 7000\nx = rand(Uniform(0,1), n,n);\n\nprintln(BLAS.get_num_threads())\n\n4\n\n\n\nfunction chol_xtx(x)\n    z = x'*x   ## z is positive definite\n    C = cholesky(z) \nend\n\nBLAS.set_num_threads(4)\n@btime chol = chol_xtx(x);  \n\n  9.747 s (5 allocations: 747.68 MiB)\n\n\n\nBLAS.set_num_threads(1)\n@btime chol = chol_xtx(x);  \n\n  11.055 s (5 allocations: 747.68 MiB)\n\n\nWe see that using four threads is faster than one, but in this case we don’t get a four-fold speedup.\n\nNumber of threads\nBy default, Julia will set the number of threads for linear algebra equal to the number of processors on your machine.\nAs seen above, you can check the number of threads being used with:\n\nBLAS.get_num_threads()\n\n1\n\n\nOther ways to control the number of threads used for linear algebra include:\n\nsetting the OMP_NUM_THREADS environment variable in the shell before starting Julia, and\nusing BLAS.set_num_threads(n).\n\n\n\n\n2.2 Threaded for loops\nIn Julia, you can directly set up software threads to use for parallel processing.\nHere we’ll see some examples of running a for loop in parallel, both acting on a single object and used as a parallel map operation.\nHere we can operate on a vector in parallel:\n\nusing Base.Threads\n\nn = 50000000;\nx = rand(n);\n\n@threads for i in 1:length(x)\n    x[i] = exp(x[i]) + sin(x[i]);\nend\n\nWe could also threads to carry out a parallel map operation, implemented as a for loop.\n\nn = 1000\n\nfunction test(n)\n    x = rand(Uniform(0,1), n,n)\n    z = x'*x \n    C = cholesky(z)\n    return(C.U[1,1])\nend\n\na = zeros(12)\n@threads for i in 1:12\n    a[i] = test(n)\nend\n\n\n\n2.3 Spawning tasks on threads\nYou can also create (aka ‘spawn’) individual tasks on threads, with the tasks running in parallel.\nLet’s see an example (taken from here of sorting a vector in parallel, by sorting subsets of the vector in separate threads.\n\nimport Base.Threads.@spawn\n\n# sort the elements of `v` in place, from indices `lo` to `hi` inclusive\nfunction psort!(v, lo::Int=1, hi::Int=length(v))\n    println(current_task(), ' ', lo, ' ', hi)\n    if lo &gt;= hi                       # 1 or 0 elements; nothing to do\n        return v\n    end\n    if hi - lo &lt; 100000               # below some cutoff, run in serial\n        sort!(view(v, lo:hi), alg = MergeSort)\n        return v\n    end\n\n    mid = (lo+hi)&gt;&gt;&gt;1                 # find the midpoint\n\n    ### Set up parallelization here ###\n\n    ## Sort two halves in parallel, one in current call and one in a new task\n    ## in a separate thread:\n    \n    half = @spawn psort!(v, lo, mid)  # task to sort the lower half\n    psort!(v, mid+1, hi)              # sort the upper half in the current call\n    \n    wait(half)                        # wait for the lower half to finish\n\n    temp = v[lo:mid]                  # workspace for merging\n\n    i, k, j = 1, lo, mid+1            # merge the two sorted sub-arrays\n    @inbounds while k &lt; j &lt;= hi\n        if v[j] &lt; temp[i]\n            v[k] = v[j]\n            j += 1\n        else\n            v[k] = temp[i]\n            i += 1\n        end\n        k += 1\n    end\n    @inbounds while k &lt; j\n        v[k] = temp[i]\n        k += 1\n        i += 1\n    end\n\n    return v\nend\n\npsort! (generic function with 3 methods)\n\n\nHow does this work? Let’s consider an example where we sort a vector of length 250000.\nThe vector gets split into elements 1:125000 (run in task #1) and 125001:250000 (run in the main call). Then the elements 1:125000 are split into 1:62500 (run in task #2) and 62501:125000 (run in task #1), while the elements 125001:250000 are split into 125001:187500 (run in task #3) and 187501:250000 (run in the main call). No more splitting occurs because vectors of length less than 100000 are run in serial.\nAssuming we have at least four threads (including the main process), each of the tasks will run in a separate thread, and all four sorts on the vector subsets will run in parallel.\n\nx = rand(250000);\npsort!(x);\n\nTask (runnable) @0x00007fe59f8bed60 1 250000\nTask (runnable) @0x00007fe59f8bed60 125001 250000\nTask (runnable) @0x00007fe59f8bed60 187501 250000\nTask (runnable) @0x00007fe59f116270 1 125000\nTask (runnable) @0x00007fe59f116270 62501 125000\nTask (runnable) @0x00007fe59f116400 125001 187500\nTask (runnable) @0x00007fe59f116590 1 62500\n\n\nWe see that the output from current_task() shows that the task labels correspond with what I stated above.\nThe number of tasks running in parallel will be at most the number of threads set in the Julia session.\n\n\n2.4 Controlling the number of threads\nYou can see the number of threads available:\n\nThreads.nthreads()\n\n1\n\n\nYou can control the number of threads used for threading in Julia (apart from linear algebra) either by:\n\nsetting the JULIA_NUM_THREADS environment variable in the shell before starting Julia, or\nstarting Julia with the -t (or --threads) flag, e.g.: julia -t 4.\n\nNote that we can’t use OMP_NUM_THREADS as the Julia threading is not based on openMP.",
    "crumbs": [
      "Parallel Julia"
    ]
  },
  {
    "objectID": "parallel-julia.html#multi-process-parallelization",
    "href": "parallel-julia.html#multi-process-parallelization",
    "title": "Parallel processing in Julia",
    "section": "3 Multi-process parallelization",
    "text": "3 Multi-process parallelization\n\n3.1 Parallel map operations\nWe can use pmap to run a parallel map operation across multiple Julia processes (on one or more machines). pmap is good for cases where each task takes a non-negligible amount of time, as there is overhead (latency) in starting the tasks.\nHere we’ll carry out multiple computationally expensive calculations in the map.\nWe need to import packages and create the function on each of the worker processes using @everywhere.\n\nusing Distributed\n\nif nprocs() == 1\n    addprocs(4)\nend\n\nnprocs()\n\nWARNING: using Distributed.@spawn in module Main conflicts with an existing identifier.\n\n\n5\n\n\n\n@everywhere begin\n    using Distributions\n    using LinearAlgebra\n    function test(n)\n        x = rand(Uniform(0,1), n,n)\n        z = transpose(x)*x \n        C = cholesky(z)\n        return C.U[2,3]\n    end\nend\n\nresult = pmap(test, repeat([5000],12))\n\n12-element Vector{Float64}:\n 11.707986495393065\n 11.751989936203428\n 11.322395638771171\n 11.156564928552134\n 11.800798513392417\n 11.721407323554907\n 12.029028566182074\n 11.831160210476378\n 11.085769679699528\n 11.706260089760217\n 11.742702280483638\n 11.469567375730398\n\n\nOne can use static allocation (prescheduling) with the batch_size argument, thereby assigning that many tasks to each worker to reduce latentcy.\n\n\n3.2 Parallel for loops\nOne can execute for loops in parallel across multiple worker processes as follows. This is particularly handy for cases where one uses a reduction operator (e.g., the + here) so that little data needs to be copied back to the main process. (And in this case we don’t copy any data to the workers either.)\nHere we’ll sum over a large number of random numbers with chunks done on each of the workers, comparing the time to a basic for loop.\n\nfunction forfun(n)\n    sum = 0.0\n    for i in 1:n\n        sum += rand(1)[1]\n    end\n    return(sum)\nend\n\nfunction pforfun(n)\n   out = @sync @distributed (+) for i = 1:n\n       rand(1)[1]\n   end\n   return(out)\nend\n\nn=50000000\n@time forfun(n);\n\n  3.005737 seconds (50.01 M allocations: 2.981 GiB, 13.08% gc time, 0.61% compilation time)\n\n\n\n@time pforfun(n); \n\n  1.823901 seconds (498.54 k allocations: 33.225 MiB, 25.26% compilation time)\n\n\nThe use of @sync causes the operation to block until the result is available so we can get the correct timing.\nWithout a reduction operation, one would generally end up passing a lot of data back to the main process, and this could take a lot of time. For such calculations, one would generally be better off using threaded for loops in order to take advantage of shared memory.\nWe’d have to look into how the random number seed is set on each worker to better understand any issues that might arise from parallel random number generation, but I believe that each worker has a different seed (but note that this does not explicitly ensure that the random number streams on the workers are distinct, as is the case if one uses the L’Ecuyer algorithm).\n\n\n3.3 Passing data to the workers\nWith multiple workers, particularly on more than one machine, one generally wants to be careful about having to copy large data objects to each worker, as that could make up a substantial portion of the time involved in the computation.\nOne can explicitly copy a variable to the workers in an @everywhere block by using Julia’s interpolation syntax:\n\n@everywhere begin\n    x = $x  # copy to workers using interpolation syntax\n    println(pointer_from_objref(x), ' ', x[1])  \nend\n\nPtr{Nothing} @0x00007fe563e15cf0 5.917418973822031e-6\n      From worker 4:    Ptr{Nothing} @0x00007fb18b934280 5.917418973822031e-6\n      From worker 5:    Ptr{Nothing} @0x00007fe520494280 5.917418973822031e-6\n      From worker 3:    Ptr{Nothing} @0x00007f93da238280 5.917418973822031e-6\n      From worker 2:    Ptr{Nothing} @0x00007f9b72938280 5.917418973822031e-6\n\n\nWe see based on pointer_from_objref that each copy of x is stored at a distinct location in memory, even when processes are on the same machine.\nAlso note that if one creates a variable within an @everywhere block, that variable is available to all tasks run on the worker, so it is global’ with respect to those tasks. Note the repeated values in the result here.\n\n@everywhere begin\n    x = rand(5)\n    function test(i)\n        return sum(x)\n    end\nend\n\nresult = pmap(test, 1:12, batch_size = 3)\n\n12-element Vector{Float64}:\n 2.2946829791594854\n 2.990563473199111\n 2.566086910330041\n 2.8483385660494345\n 2.2946829791594854\n 2.990563473199111\n 2.566086910330041\n 2.8483385660494345\n 2.2946829791594854\n 2.990563473199111\n 2.566086910330041\n 2.8483385660494345\n\n\nIf one wants to have multiple processes all work on the same object, without copying it, one can consider using Julia’s SharedArray (one machine) or DArray from the DistributedArrays package (multiple machines) types, which break up arrays into pieces, with different pieces stored locally on different processes.\n\n\n3.4 Spawning tasks\nOne can use the Distributed.@spawnat macro to run tasks on processes, in a fashion similar to using Threads.@spawn. More details can be found here.\n\n\n3.5 Using multiple machines\nIn addition to using processes on one machine, one can use processes across multiple machines. One can either start the processes when you start the main Julia session or you can start them from within the Julia session. In both cases you’ll need to have the ability to ssh to the other machines without entering your password.\nTo start the processes when starting Julia, create a “machinefile” that lists the names of the machines and the number of worker processes to start on each machine.\nHere’s an example machinefile:\narwen.berkeley.edu\narwen.berkeley.edu\ngandalf.berkeley.edu\ngandalf.berkeley.edu\nNote that if you’re using Slurm on a Linux cluster, you could generate that file in the shell from within your Slurm allocation like this:\nsrun hostname &gt; machines\nThen start Julia like this:\njulia --machine-file machines\nFrom within Julia, you can add processes like this (first we’ll remove the existing worker processes started using addprocs() previously):\n\nrmprocs(workers())\n\naddprocs([(\"arwen\", 2), (\"gandalf\", 2)])\n\n4-element Vector{Int64}:\n 6\n 7\n 8\n 9\n\n\nTo check on the number of processes:\n\nnprocs()\n\n5",
    "crumbs": [
      "Parallel Julia"
    ]
  },
  {
    "objectID": "parallel-julia.html#loops-and-fused-operations",
    "href": "parallel-julia.html#loops-and-fused-operations",
    "title": "Parallel processing in Julia",
    "section": "4 Loops and fused operations",
    "text": "4 Loops and fused operations\nConsider the following vectorized code that you might run in a variety of languages (e.g., Julia, Python, R).\nx = tan(x) + 3*sin(x)\nIf run as vectorized code, this has downsides. First, it will use additional memory (temporary arrays will be created to store tan(x), sin(x), 3*sin(x)). Second, multiple for loops will have to get executed when the vectorized code is run, looping over the elements of x to calculate tan(x), sin(x), etc. (For example in R or Python/numpy, multiple for loops would get run in the underlying C code.)\nContrast that to running directly as a for loop (e.g., in Julia or in C/C++):\n\nfor i in 1:length(x)\n    x[i] = tan(x[i]) + 3*sin(x[i])\nend\n\nHere temporary arrays don’t need to be allocated and there is only a single for loop.\nCombining loops is called ‘fusing’ and is an important optimization that Julia can do. (It’s also a key optimization done by XLA, a compiler used with Tensorflow.)\nOf course you might ask why use vectorized code at all given that Julia will JIT compile the for loop above and run it really quickly. That’s true, but reading and writing vectorized code is easier than writing for loops.\nLet’s compare the speed of the following approaches. We’ll put everything into functions as generally recommended when timing Julia code to avoid global variables that incur a performance penalty because their type can change.\nFirst, let’s find the time when directly using a for loop, as a baseline.\n\nn = 50000000\ny = Array{Float64}(undef, n);\nx = rand(n);\n\n\nfunction direct_for_loop_calc(x, y)\n    for i in 1:length(x)\n        y[i] = exp(x[i]) + sin(x[i])\n    end\nend\n\nusing BenchmarkTools\n@benchmark direct_for_loop_calc(x, y)\n\n\nBenchmarkTools.Trial: 8 samples with 1 evaluation.\n Range (min … max):  645.995 ms … 770.652 ms  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     672.480 ms               ┊ GC (median):    0.00%\n Time  (mean ± σ):   689.201 ms ±  44.517 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%\n  ▁  ▁   █          ▁       ▁                 ▁               ▁  \n  █▁▁█▁▁▁█▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁\n  646 ms           Histogram: frequency by time          771 ms &lt;\n Memory estimate: 0 bytes, allocs estimate: 0.\n\n\n\nNotice the lack of additional memory use.\nNow let’s try a basic vectorized calculation (for which we need the various periods to get vectorization), without fusing. We’ll reassign the result to the allocated y vector for comparability to the for loop implementation above.\n\nfunction basic_vectorized_calc(x, y)\n     y .= exp.(x) + 3 * sin.(x)\nend\n\nusing BenchmarkTools\n@benchmark basic_vectorized_calc(x, y)\n\n\nBenchmarkTools.Trial: 4 samples with 1 evaluation.\n Range (min … max):  1.400 s …   1.564 s  ┊ GC (min … max):  5.34% … 14.34%\n Time  (median):     1.490 s              ┊ GC (median):    10.63%\n Time  (mean ± σ):   1.486 s ± 85.872 ms  ┊ GC (mean ± σ):  11.23% ±  6.02%\n  █       █                                             █ █  \n  █▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁█ ▁\n  1.4 s          Histogram: frequency by time        1.56 s &lt;\n Memory estimate: 1.49 GiB, allocs estimate: 8.\n\n\n\nThe original x array is 400 MB; notice the additional memory allocation and that this takes almost twice as long as the original for loop.\nHere’s a fused version of the vectorized calculation, where the @. causes the loops to be fused.\n\nfunction fused_vectorized_calc(x, y)\n    y .= @. tan(x) + 3 * sin(x)\nend\n\n@benchmark fused_vectorized_calc(x, y)\n\n\nBenchmarkTools.Trial: 5 samples with 1 evaluation.\n Range (min … max):  1.077 s …   1.146 s  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     1.095 s              ┊ GC (median):    0.00%\n Time  (mean ± σ):   1.105 s ± 28.250 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%\n  █       █      █                    █                   █  \n  █▁▁▁▁▁▁▁█▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁\n  1.08 s         Histogram: frequency by time        1.15 s &lt;\n Memory estimate: 0 bytes, allocs estimate: 0.\n\n\n\nWe see that the time and (lack of) memory allocation are essentially the same as the original basic for loop.\nFinally one can achieve the same fusion by having the function just compute scalar quantities and then using the vectorized version of the function (by using scalar_calc.() instead of scalar_calc()), which also does the fusion.\n\nfunction scalar_calc(x)\n    return(tan(x) + 3 * sin(x))\nend\n\n@benchmark y .= scalar_calc.(x)\n\n\nBenchmarkTools.Trial: 5 samples with 1 evaluation.\n Range (min … max):  1.032 s …   1.091 s  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     1.047 s              ┊ GC (median):    0.00%\n Time  (mean ± σ):   1.052 s ± 22.440 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%\n  █          █  █ █                                       █  \n  █▁▁▁▁▁▁▁▁▁▁█▁▁█▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁\n  1.03 s         Histogram: frequency by time        1.09 s &lt;\n Memory estimate: 16 bytes, allocs estimate: 1.",
    "crumbs": [
      "Parallel Julia"
    ]
  },
  {
    "objectID": "parallel-julia.html#using-the-gpu---basic-offloading",
    "href": "parallel-julia.html#using-the-gpu---basic-offloading",
    "title": "Parallel processing in Julia",
    "section": "5 Using the GPU - basic offloading",
    "text": "5 Using the GPU - basic offloading\nWe can use CUDA.jl to offload computations to the GPU. Here we’ll explore matrix multiplication and vectorized calculations. For this, Julia will take care, behind the scenes, of converting our Julia code to code that can run on the GPU.\nThere are a couple key things to remember about using a GPU:\n\nThe GPU memory is separate from CPU memory, and transferring data from the CPU to GPU (or back) is often more costly than doing the computation on the GPU.\n\nIf possible, generate the data on the GPU or keep the data on the GPU when carrying out a sequence of operations.\n\nBy default GPU calculations are often doing using 32-bit (4-byte) floating point numbers rather than the standard of 64-bit (8-byte) when on the CPU.\n\nThis can affect speed comparisons between CPU and GPU.\n\n\nNote that for this section, I’m pasting in the output when running the code separately on a machine with a GPU because this document is generated on a machine without a GPU.\n\n5.1 Matrix multiplication\nLet’s first consider basic matrix multiplication. In this case since we generate the matrices on the CPU, they are 64-bit.\n\nusing BenchmarkTools\nusing CUDA\nusing LinearAlgebra\n\nfunction matmult(x, y)\n    z = x * y\n    return z\nend\n\nn = 7000\n\nx = randn(n, n);\ny = randn(n, n);\nx_gpu = CuArray(x);\ny_gpu = CuArray(y);\n\n## These use 64-bit numbers:\ntypeof(x)\n# Matrix{Float64} (alias for Array{Float64, 2})\ntypeof(x_gpu)\n# CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}\n\nLinearAlgebra.BLAS.set_num_threads(1);\n\n@benchmark z = matmult(x, y) \n# BenchmarkTools.Trial: 1 sample with 1 evaluation.\n#  Single result which took 17.271 s (0.00% GC) to evaluate,\n#  with a memory estimate of 373.84 MiB, over 2 allocations.\n\n@benchmark CUDA.@sync z_gpu = matmult(x_gpu, y_gpu)\n# BenchmarkTools.Trial: 65 samples with 1 evaluation.\n#  Range (min … max):  53.172 ms … 90.679 ms  ┊ GC (min … max): 0.00% … 0.00%\n#  Time  (median):     76.419 ms              ┊ GC (median):    0.00%\n#  Time  (mean ± σ):   77.404 ms ±  4.092 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%\n\nClearly the the GPU calculation is much faster, taking about 75 milliseconds, compared to 17 seconds on the CPU (albeit using a single thread).\nLet’s compare that to the time of copying the data to the GPU:\n\n@benchmark CUDA.@sync tmp = CuArray(x)\n# BenchmarkTools.Trial: 59 samples with 1 evaluation.\n#  Range (min … max):  83.766 ms … 137.849 ms  ┊ GC (min … max): 0.00% … 0.00%\n#  Time  (median):     84.684 ms               ┊ GC (median):    0.00%\n#  Time  (mean ± σ):   85.696 ms ±   7.011 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%\n\nThis suggests that the time in copying the data is similar to that for doing the computation.\nIf we count the time of transferring the data to and from the GPU, that ends up being a substantial part of the time, compared to the 75 ms for simply doing the matrix multiplication.\n\nfunction matmult_with_transfer(x, y)\n    xc = CuArray(x)\n    yc = CuArray(y)\n    z = xc * yc\n    return Array(z)\nend\n\n@benchmark CUDA.@sync z = matmult_with_transfer(x, y) \n# BenchmarkTools.Trial: 20 samples with 1 evaluation.\n#  Range (min … max):  251.578 ms … 258.017 ms  ┊ GC (min … max): 0.00% … 0.57%\n#  Time  (median):     253.886 ms               ┊ GC (median):    0.00%\n#  Time  (mean ± σ):   254.228 ms ±   1.708 ms  ┊ GC (mean ± σ):  4.33% ± 6.78%\n\nAs a sidenote, we can force use of 64-bit numbers on the GPU (in this case when generating values on the GPU) like this.\n\nx_gpu = CUDA.randn(Float64, n, n);\n\nFinally, let’s consider whether the matrix multiplication is faster using 32-bit numbers.\n\nx = randn(Float32, n, n);\ny = randn(Float32, n, n);\nx_gpu = CuArray(x);\ny_gpu = CuArray(y);\ntypeof(x_gpu)\n\n@benchmark z = matmult(x, y) \n# BenchmarkTools.Trial: 1 sample with 1 evaluation.\n#  Single result which took 8.671 s (0.00% GC) to evaluate,\n@benchmark CUDA.@sync z_gpu = matmult(x_gpu, y_gpu)\n# BenchmarkTools.Trial: 91 samples with 1 evaluation.\n#  Range (min … max):  41.174 ms … 70.491 ms  ┊ GC (min … max): 0.00% … 0.00%\n#  Time  (median):     54.420 ms              ┊ GC (median):    0.00%\n#  Time  (mean ± σ):   55.363 ms ±  2.912 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%\n\nSo that’s faster, though I’m not sure why the CPU implementation is about twice as fast (which makes sense in that it is working with numbers taking up half as much space) while the GPU implementation does not achieve that speedup (54 ms. with 32-bit compared to 75 ms. with 64-bit).\n\n\n5.2 Vectorized calculations\nHere we’ll consider using the GPU for vectorized calculations, noting that earlier we talked about using . to vectorize and @ to fuse loops in the context of CPU-based calculations.\n\n# Scalar function to do something.\nfunction myfun(x)\n    y = tan(x) + 3 * sin(x)\n    return y\nend\n\n# Vectorized version that modifies `y` in place.\nfunction myfun_vec(x, y)\n    y .= myfun.(x)\n    return \nend\n\nn = 250000000; \ny = Vector{Float64}(undef, n);\nx = rand(n);\n\nx_gpu = CuArray(x);\ny_gpu = CuArray(y);\n\n@benchmark myfun_vec(x, y)   # 3.5 sec.\n\n@benchmark CUDA.@sync myfun_vec(x_gpu, y_gpu)  # 6 ms.\n\nHere we have a massive 500x speedup of 6 ms. compared to 3.5 seconds.\nOf course, as in the matrix multiplication example above, if you need to copy the data to and from the GPU, that will add substantial time.",
    "crumbs": [
      "Parallel Julia"
    ]
  },
  {
    "objectID": "parallel-julia.html#using-the-gpu---writing-gpu-kernels",
    "href": "parallel-julia.html#using-the-gpu---writing-gpu-kernels",
    "title": "Parallel processing in Julia",
    "section": "6 Using the GPU - writing GPU kernels",
    "text": "6 Using the GPU - writing GPU kernels\nNext we’ll explore writing our own GPU kernels. Kernels are functions that encode the core computational operations that are executed in parallel.\nIn other languages, the basic mode of operation with a GPU when you are writing your own GPU code is to write a kernel using CUDA (basically C) code and then call the kernel in parallel via C, R, or Python code. In Julia, we can write the kernel using Julia syntax (though many operations (particularly non-numerical ones) will not run on the GPU…).\n\n6.1 Basic example\nHere’s a basic example in which we’ll do a calculation in place. We run 1000 scalar calculations using 1000 threads.\nWe use @cuda to compile and run the kernel.\n\nfunction my_kernel(x)\n  idx = threadIdx().x;   # What thread am I?\n  if idx &lt;= length(x)\n    x[idx] = tan(x[idx]) + 3*sin(x[idx]);\n  end\n  return\nend\n\nn = 1000;\nx_gpu = CUDA.randn(n);\nArray(x_gpu)[n]\n# -1.5321726f0\n@cuda threads=n my_kernel(x_gpu);\nArray(x_gpu)[n]   # Check the computation was done by checking last element.\n# -28.875708f0\n\nThere are limits on the number of threads we can use.\n\nn = 2000;\nx_gpu = CUDA.randn(n);\n@cuda threads=n my_kernel(x_gpu);\n# ERROR: Number of threads in x-dimension exceeds device limit (2000 &gt; 1024).\n\n\n6.1.1 Multiple blocks\nWe need to use at least as many threads as computations, and in addition to only being able to use 1024 threads in the x dimension, we can have at most 1024 threads in a block on the A100 GPU we’re using. So we’ll need multiple blocks.\n\nfunction my_kernel(x)\n  i = threadIdx().x;  # What thread am I within the block?\n  j = blockIdx().x;   # What block am I in?\n  idx =  (j-1)*blockDim().x + i;\n  if idx &lt;= length(x)\n    x[idx] = tan(x[idx]) + 3*sin(x[idx]);\n  end\n  return\nend\n\nn = 2000;\nnthreads = 1024;\nx_gpu = CUDA.randn(n);\ninitial = Array(x_gpu)[n]\nnblocks = Int(ceil(n/nthreads));\n\n@cuda threads=nthreads blocks=nblocks my_kernel(x_gpu);\n(initial, Array(x_gpu)[n])  # Check that calculation was done.\n\nLet’s do a smaller test run in which we can check on the thread and block indexing.\n\nfunction my_kernel_print(x)\n  i = threadIdx().x;  # What thread am I within the block?\n  j = blockIdx().x;   # What block am I in?\n  idx =  (j-1)*blockDim().x + i;\n  if idx &lt;= length(x)\n    x[idx] = tan(x[idx]) + 3*sin(x[idx]);\n    @cuprintln idx, i, j, blockDim().x, blockDim().y;\n  end\n  return\nend\n\nn = 200;\nx_gpu = CUDA.randn(n);\nnthreads = 100;\nnblocks = Int(ceil(n/nthreads));\n@cuda threads=nthreads blocks=nblocks my_kernel_print(x_gpu);\n\nWhen we run this, notice the output seems to be grouped based on warps of 32 threads (apart from the last set since n=200 is not a multiple of 32).\n\n\n6.1.2 Larger computations\nIn many cases we’ll have more tasks than the total number of GPU cores. As long as we don’t exceed the maximum size of a block or grid, we can just ask for as many threads as we have tasks and rely on the GPU to manage assigning the tasks to the GPU cores.\nWe’d want to check that the number/dimension of the block here does not exceed the maximum block size. I didn’t do that, but it ran, so it must have been ok!\nHere we’ll run the computation we ran earlier when we did not write our own kernel and just relied on Julia to offload to the GPU behind the scene.\n\nn = 250000000;\nx_gpu = CUDA.randn(n);\nnthreads = 1024;\nnblocks = Int(ceil(n/nthreads));\nArray(x_gpu)[n]\n\n# Run it once to flush out any compilation/transformation time.\ny_gpu = CUDA.randn(5);\nCUDA.@sync @cuda threads=nthreads blocks=nblocks my_kernel(y_gpu);\n\nCUDA.@time CUDA.@sync @cuda threads=nthreads blocks=nblocks my_kernel(x_gpu);\n# 0.002003 seconds (45 CPU allocations: 2.719 KiB)\nArray(x_gpu)[n]\n\nThe 2.0 ms is reasonably comparable to the 3.7 ms when we just had Julia run the vectorized computation on the GPU (from the last time we ran it). That used 64-bit floats. When I reran the code above using 64-bit floats, the time was 5.2 ms.\n\n\n\n6.2 Efficient memory access\nWe’ll explore two final topics related to efficiently accessing data in memory: first accessing global GPU memory efficiently and second making use of shared GPU memory.\n\n6.2.1 Coalesced access to global memory\nIf adjacent threads in a block access adjacent memory locations, a chunk of data can be obtained in a single access to global memory.\nWe’ll implement element-wise summing of two matrices. Obviously one can just do this directly with CuArrays in Julia, but if we implement it ourselves, it illustrates that reading a matrix by column is much more efficient than reading by row. Here a thread block either handles part of a column (good) or part of a row (bad). The x-dimension of the blocks in the grid then handles multiple thread blocks within each column (or row; bad) and the y-dimension of the blocks in the grid handles the different columns (or rows; bad).\n\nn = 10000;\nX_gpu = CUDA.randn(n,n);\nY_gpu = CUDA.randn(n,n);\nout_gpu = CUDA.zeros(n,n);\n\nX_gpu_small = CUDA.randn(5,5);\nY_gpu_small = CUDA.randn(5,5);\nout_gpu_small = CUDA.zeros(5,5);\n\n# Good: Adjacent threads process elements in a column.\nfunction kernel_sum_bycol!(X, Y, output)\n    row_idx = threadIdx().x + (blockIdx().x - 1)*blockDim().x;\n    col_idx = blockIdx().y;\n    \n    if row_idx &lt;= size(X, 1) && col_idx &lt;= size(Y, 2)\n        output[row_idx, col_idx] = X[row_idx, col_idx] + Y[row_idx, col_idx]\n    end\n    return nothing\nend\n\nnthreads = 1024;\n# x dim of grid is number of thread blocks in a column.\n# y dim of grid is number of columns.\nnblocks = (Int(ceil(n/nthreads)), n);\n\n\n# Flush out any compilation time.\nCUDA.@sync @cuda threads=nthreads blocks=nblocks kernel_sum_bycol!(X_gpu_small, Y_gpu_small, out_gpu_small);\n\n@btime CUDA.@sync @cuda threads=nthreads blocks=nblocks kernel_sum_bycol!(X_gpu, Y_gpu, out_gpu);\n# 2.153 ms (47 allocations: 1.30 KiB)\n\n# Bad: Adjacent threads process elements in a row.\nfunction kernel_sum_byrow!(X, Y, output)\n    row_idx = blockIdx().y;\n    col_idx = threadIdx().x + (blockIdx().x - 1)*blockDim().x;\n    \n    if row_idx &lt;= size(X, 1) && col_idx &lt;= size(Y, 2)\n        output[row_idx, col_idx] = X[row_idx, col_idx] + Y[row_idx, col_idx]\n    end\n    return nothing\nend\n\n\n# Flush out any compilation time.\nCUDA.@sync @cuda threads=nthreads blocks=nblocks kernel_sum_byrow!(X_gpu_small, Y_gpu_small, out_gpu_small);\n\n@btime CUDA.@sync @cuda threads=nthreads blocks=nblocks kernel_sum_byrow!(X_gpu, Y_gpu, out_gpu);\n# 10.500 ms (47 allocations: 1.30 KiB)\n\n\n\n6.2.2 Using shared memory\nAccessing global GPU memory is much slower than doing computation on the GPU. So we’d like to avoid repeated access to global memory (e.g., a bad scenario would be a ratio of one arithmetic calculation per retrieval from global memory). One strategy is for multiple threads in a block to cooperate to load data from global memory into shared memory accessible by all the threads in the block. The computation can then be done on the data in shared memory.\nHere’s a simplified example that shows how to load the data into shared memory. There’s no actual computation coded here, but one could imagine that each thread would then each do a computation that uses the entire chunk of data in shared memory.\n\nfunction kernel_reader_bycol(x::CuDeviceArray{T}) where T\n  i = threadIdx().x;  # What thread am I within the block?\n  j = blockIdx().x;   # What block am I in?\n  idx =  (j-1)*blockDim().x + i;\n  dims = size(x);\n  \n  # Setup shared memory for the subset of data.\n  shared_data = CuDynamicSharedArray(T, (blockDim().x, dims[2]));\n\n  for chunk_start = 1:blockDim().x:dims[1]\n    chunk_size = min(blockDim().x, dims[1] - chunk_start + 1);\n    ## Transfer a chunk of rows in parallel, one row per thread.\n    if i &lt;= chunk_size\n      for col in 1:dims[2]\n        shared_data[i, col] = x[chunk_start + i - 1, col];\n      end\n    end\n    sync_threads()\n    \n    # At this point we'd insert code to do the actual computation, based on `idx`.\n    # Each thread now has the opportunity to compute on all the data in the chunk in\n    # `shared_data`.\n    \n  end\n\n  return\nend\n\nn = 10000000;\nm = 10;\nx_gpu = CUDA.randn(n, m);\nx_gpu_small = CUDA.randn(5, m);\n\nnthreads = 1024;\nnblocks = 100;  # This is arbitrary in this example as we are not doing an actual computation.\n\nmemsize = nthreads * m * 4;\n\nCUDA.@sync @cuda threads=nthreads blocks=nblocks shmem=memsize kernel_reader_bycol(x_gpu_small);\n\nCUDA.@time @cuda threads=nthreads blocks=nblocks shmem=memsize kernel_reader_bycol(x_gpu);\n# 0.138480 seconds (24 CPU allocations: 752 bytes)\n\nIf m gets much bigger, we get an error “ERROR: Amount of dynamic shared memory exceeds device limit (400.000 KiB &gt; 48.000 KiB).” So for larger m we’d need to rework how we manipulate the data.\nLet’s close by seeing if the memory access patterns make a difference in this example. Instead of accessing by column, we’ll access by row, but with the matrix transposed so it is very wide instead of very long.\nMy initial thought was that accessing by row would be slower because adjacent threads are not reading from adjacent locations in global memory.\n\nfunction kernel_reader_byrow(x::CuDeviceArray{T}) where T\n  i = threadIdx().x;  # What thread am I within the block?\n  j = blockIdx().x;   # What block am I in?\n  idx =  (j-1)*blockDim().x + i;\n  dims = size(x);\n  \n  # Setup shared memory for the subset of data.\n  shared_data = CuDynamicSharedArray(T, (dims[1], blockDim().x));\n\n  for chunk_start = 1:blockDim().x:dims[2]\n    chunk_size = min(blockDim().x, dims[2] - chunk_start + 1);\n    ## Transfer a chunk of rows in parallel, one column per thread.\n    if i &lt;= chunk_size\n      for row in 1:dims[1]\n        shared_data[row, i] = x[row, chunk_start + i - 1];\n      end\n    end\n    sync_threads()\n    \n    # At this point we'd insert code to do the actual computation, based on `idx`.\n    # Each thread now has the opportunity to compute on all the data in the chunk in\n    # `shared_data`.\n    \n  end\n\n  return\nend\n\nn = 10000000;\nm = 10;\nx_gpu = CUDA.randn(m, n);\nx_gpu_small = CUDA.randn(m, 5);\n\nnthreads = 1024;\nnblocks = 100;  # This is arbitrary in this example as we are not doing an actual computation.\n\nmemsize = nthreads * m * 4;\n\nCUDA.@sync @cuda threads=nthreads blocks=nblocks shmem=memsize kernel_reader_byrow(x_gpu);\n\nCUDA.@time CUDA.@sync @cuda threads=nthreads blocks=nblocks shmem=memsize kernel_reader_byrow(x_gpu);\n# 0.105434 seconds (25 CPU allocations: 1008 bytes)\n\nWe see that the access by row here is (a bit) faster. I think this is because the entire chunk of data in the wide matrix lives in a small area of global memory, while in the long matrix, each column in the chunk has adjacent values but separate columns are very far apart because the matrix is so long.\nWe might be able to improve efficiency with the wide matrix by operating by column within the wide matrix. This would involve more work to manage the indexing because we wouldn’t just have each thread manage a column (unless we used very few threads, which would presumably reduce efficiency).\n\n\n\n6.3 Using atomics for reduction operations\nOne thing we haven’t seen so far is being able to have different threads write to the same memory location (e.g., to a scalar or to an element of an array). One can easily imagine needing to do this to carry out reduction operations (e.g., calculating a sum or a max or min).\nThe obvious danger is that two threads might write to the memory location at the same time and somehow cause the location not to be properly updated.\nSuppose we want to calculate the log-likelihood (or some other loss function) across independent observations. We’d like to do the summation on the GPU to avoid passing all the log-likelihood values from GPU to CPU and then having to do the sum on the CPU.\n\nusing BenchmarkTools\nusing Distributions\n\nn = 100_000_000;   # Formatted for readability.\nnorm_dist = Normal(0,1)\nsamples = rand(norm_dist, n);\n\nfunction loglik_kernel(x, result)\n  i = threadIdx().x;  # What thread am I within the block?\n  j = blockIdx().x;   # What block am I in?\n  idx =  (j-1)*blockDim().x + i;\n  if idx &lt;= length(x)\n    # logpdf(norm_dist, x[idx]) # Doesn't compile.\n    CUDA.@atomic result[1] += -0.5*x[idx]^2;            # Experimental, but nicer interface.\n    #CUDA.atomic_add!(pointer(result), -0.5*x[idx]^2);  # Stable low-level API.\n  end\n  return\nend\n\nnthreads = 1024;\nnblocks = Int(ceil(n/nthreads));\n\nsamples_gpu = CuArray(samples);\nresult = CUDA.zeros(typeof(samples[1]), 1);\n@btime CUDA.@sync @cuda threads=nthreads blocks=nblocks loglik_kernel(samples_gpu, result);\n# 161.352 ms (34 allocations: 944 bytes)\nArray(result)[1] -n*log(2*pi)/2  # Adjust for normalizing constant as scalar computation, not on GPU.\n\n@btime sum(logpdf.(norm_dist, samples))\n# 1.410 s (5 allocations: 762.94 MiB)\n\nSo we got about a 12-fold speedup, which is less than we’ve been getting for some of our other comparisons.\nI was curious how much time is spent handling the reduction operation (presumably there is some loss in efficiency from having all the threads write to the same memory location). When I changed result to be a vector of length equal to that of samples and just assign the individual PDF evaluations to the corresponding elements of result without the atomic operation, the time was 3 milliseconds (compared to 161 above), so there is a performance degradation from the atomic operation.\n\n6.3.1 Using shared memory to reduce the cost of atomic operations\nOne solution to the performance degradation is to not have all of the summing make use of the same location in memory to accumulate the result.\nInstead we can use shared memory to more efficiently do the reduction within each thread block before doing the final reduction across blocks. Here’s an approach using a tree-like operation (as suggested by a ChatBot, but requiring some debugging on my part) to compute the partial sum within each thread block before using the atomic operation to compute the sum of the partial sums:\n\nfunction loglik_kernel_shmem(x::CuDeviceArray{T}, result::CuDeviceArray{T}) where T\n  i = threadIdx().x;  # What thread am I within the block?\n  j = blockIdx().x;   # What block am I in?\n  idx =  (j-1)*blockDim().x + i;\n\n  shared_data = CuDynamicSharedArray(T, (blockDim().x));\n  # First do the core calculation and store in shared memory.\n  if idx &lt;= length(x)\n     shared_data[i] = -0.5*x[idx]^2;\n  else\n     shared_data[i] = 0.0; \n  end\n\n  # Tree-like partial sum within the thread block,\n  # summing pairs until the sum within the block\n  # is contained in `shared_data[1]`.\n  s = blockDim().x ÷ 2;   # `÷` ensures `s` is Int.\n  while s &gt;= 1\n    if i &lt;= s\n      shared_data[i] += shared_data[i + s];\n    end\n    sync_threads()\n    s ÷= 2;\n  end\n    \n  # The first thread in the block writes the partial sum to global memory.\n  if i == 1\n    CUDA.@atomic result[1] += shared_data[1];\n  end\n  return\nend\n\nmemsize = nthreads * sizeof(samples[1]);\n\nresult2 = CUDA.zeros(typeof(samples[1]), 1);\n@btime CUDA.@sync @cuda threads=nthreads blocks=nblocks shmem=memsize loglik_kernel_shmem(samples_gpu, result2);\n# 6.317 ms (34 allocations: 944 bytes)\n\nArray(result2)[1] -n*log(2*pi)/2 \n\n\n\n\n6.4 Debugging kernel code\nIt can be much harder to debug kernel code than regular Julia code. If the syntax doesn’t produce valid compiled code that can run on the GPU, it may not be obvious from the error messsage what the problem is.\nAs an example in the code in the previous section, I originally had s = blockDim().x / 2; and s /= 2;. I didn’t realize that even with integer inputs, that this produced a float output type for s and that as a result using s for indexing in shared_data[i + s]; wouldn’t work. The error message said there was a problem with the LLVM/IR code produced from the kernel, but didn’t say where and it took a binary search on my part to figure out that shared_data[i + s]; was the problematic piece of code and that was caused by s being a float.\nOn an only somewhat related point, the ChatBot originally gave me while s &gt;= 0, which is a bug that doesn’t prevent the code from running, but does give incorrect numerical results, so we still need to be careful with what we get from ChatBots.",
    "crumbs": [
      "Parallel Julia"
    ]
  },
  {
    "objectID": "parallel-R.html",
    "href": "parallel-R.html",
    "title": "Parallel processing in R",
    "section": "",
    "text": "R provides a variety of functionality for parallelization, including threaded operations (linear algebra), parallel for loops and lapply-type statements, and parallelization across multiple machines. This material focuses on R’s future package, a flexible and powerful approach to parallelization in R.\n\n\n\nThreading in R is limited to linear algebra, provided R is linked against a threaded BLAS.\n\n\nThe BLAS is the library of basic linear algebra operations (written in Fortran or C). A fast BLAS can greatly speed up linear algebra relative to the default BLAS on a machine. Some fast BLAS libraries are\n\nIntel’s MKL; may be available for educational use for free\nOpenBLAS; open source and free\nvecLib for Macs; provided with your Mac (both newer Apple Silicon and older Intel Macs)\n\nIn addition to being fast when used on a single core, all of these BLAS libraries are threaded - if your computer has multiple cores and there are free resources, your linear algebra will use multiple cores, provided your installed R is linked against the threaded BLAS installed on your machine.\nYou can use an optimized BLAS on your own machine(s).\n\n\n\nHere’s some code that illustrates the speed of using a threaded BLAS:\n\nlibrary(RhpcBLASctl)  ## package that controls number of threads from within R\n\nx &lt;- matrix(rnorm(5000^2), 5000)\n\n## Control number of threads from within R. See next section for details.\nblas_set_num_threads(4)\nsystem.time({\n   x &lt;- crossprod(x)\n   U &lt;- chol(x)\n})\n\n#   user  system elapsed \n# 14.104   5.403   6.752 \n\nblas_set_num_threads(1)\nsystem.time({\n   x &lt;- crossprod(x)\n   U &lt;- chol(x)\n})\n\n#   user  system elapsed \n# 12.393   0.055  12.344 \n\nHere the elapsed time indicates that using four threads gave us a two times (2x) speedup in terms of real time, while the user time indicates that the threaded calculation took a bit more total processing time (combining time across all processors) because of the overhead of using multiple threads. So the threading helps, but it’s not the 4x linear speedup we would hope for.\n\n\n\nIn general, threaded code will detect the number of cores available on a machine and make use of them. However, you can also explicitly control the number of threads available to a process.\nFor most threaded code (that based on the openMP protocol), the number of threads can be set by setting the OMP_NUM_THREADS environment variable. Note that under some circumstances you may need to use VECLIB_MAXIMUM_THREADS if on an Intel (older) Mac or MKL_NUM_THREADS if R is linked against MKL (which can be seen by running sessionInfo). For information relevant for newer Apple Silicon (M1 and M2) based Macs see below.\nFor example, to set it for four threads in bash:\nexport OMP_NUM_THREADS=4\nDo this before starting your R or Python session or before running your compiled executable.\nAlternatively, you can set OMP_NUM_THREADS as you invoke your job, e.g., here with R:\nOMP_NUM_THREADS=4 R CMD BATCH --no-save job.R job.out\nFinally, the R package, RhpcBLASctl, allows you to control the number of threads from within R, as already seen in the example in the previous subsection.\n\nlibrary(RhpcBLASctl)\nblas_set_num_threads(4)\n# now run your linear algebra\n\n\n\n\nNote that newer Macs (Apple Silicon-based M1 and M2 Macs) also provide the Accelerate (vecLib) BLAS, but apparently they use the Mac’s AMX co-processor (details are hard to find online). This gives fast computation, but the calculations are not using the regular CPU cores and so one doesn’t choose the number of threads. In particular, VECLIB_MAXIMUM_THREADS has no effect, and top shows only a single CPU in use. Rest assured that if you’ve configured R to use Accelerate (vecLib) BLAS, you should see very good performance.\n\n\n\n\nAll of the functionality discussed here applies only if the iterations/loops of your calculations can be done completely separately and do not depend on one another. This scenario is called an embarrassingly parallel computation. So coding up the evolution of a time series or a Markov chain is not possible using these tools. However, bootstrapping, random forests, simulation studies, cross-validation and many other statistical methods can be handled in this way.\nOne can easily parallelize lapply (or sapply) statements or parallelize for loops using the future package. Here’s we’ll just show the basic mechanics of using the future package. There’s much more detail in this SCF tutorial.\nIn Sections 3.1 and 3.2, we’ll parallelize across multiple cores on one machine. Section 3.3 shows how to use multiple machines.\n\n\nHere we’ll parallelize an lapply operation. We need to call plan to set up the workers that will carry out the individual tasks (one for each element of the input list or vector) in parallel.\nThe multisession “plan” simply starts worker processes on the machine you are working on. You could skip the workers argument and the number of workers will equal the number of cores on your machine. Later we’ll see the use of the multicore and cluster “plans”, which set up the workers in a different way.\nHere we parallelize leave-one-out cross-validation for a random forest model.\n\nsource('rf.R')  # loads in data (X and Y) and looFit()\n\nlibrary(future.apply)\n## Set up four workers to run calculations in parallel\nplan(multisession, workers = 4)\n\n## Run the cross-validation in parallel, four tasks at a time on the four workers\nsystem.time(\n  out &lt;- future_lapply(seq_along(Y), looFit, Y, X, future.seed = TRUE)\n)   \n#   user  system elapsed \n#  0.684   0.086  19.831 \n\n\n## Side note: seq_along(Y) is a safe equivalent of 1:length(Y)\n\nThe use of future.seed ensures safe parallel random number generation as discussed in Section 5.\nHere the low user time is because the time spent in the worker processes is not counted at the level of the overall master process that dispatches the workers.\nNote that one can use plan without specifying the number of workers, in which case it will call parallelly::availableCores() and in general set the number of workers to a sensible value based on your system (and your scheduler allocation if your code is running on a cluster under a scheduler such as Slurm).\n\n\n\nWe can use the future package in combination with the foreach command to run a for loop in parallel. Of course this will only be valid if the iterations can be computed independently.\nThe syntax for foreach is a bit different than a standard for loop. Also note that the output for each iteration is simply the result of the last line in the { } body of the foreach statement.\nHere’s the syntax when using newer (version 1.0.0 and later) versions of doFuture:\n\nsource('rf.R')  # loads in data (X and Y) and looFit()\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\nlibrary(doFuture, quietly = TRUE)\n\nplan(multisession, workers = 4)\n\n## Toy example of using foreach+future\nout &lt;- foreach(i = seq_len(30)) %dofuture% {\n    mean(1:i)\n}\nout[1:3]\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 1.5\n\n[[3]]\n[1] 2\n\n## Replicate our cross-validation from future_lapply.\n\n## Add option to ensure safe random number generation in parallel\n## (discussed in Section 5)\nout &lt;- foreach(i = seq_along(Y), .combine = c,\n       .options.future = list(seed = TRUE)) %dofuture% {\n       looFit(i, Y, X)\n}\nout[1:3]\n\n        1         2         3 \n0.3151692 1.0207755 1.8871612 \n\n\nPrior to version 1.0.0 of doFuture, you’d do this:\n\nsource('rf.R')  # loads in data (X and Y) and looFit()\nlibrary(future)\nplan(multisession, workers = 4)\n\nlibrary(doFuture, quietly = TRUE)\nregisterDoFuture()\n\n## Toy example of using foreach+future\nout &lt;- foreach(i = seq_len(30)) %dopar% {\n    mean(1:i)\n}\nout[1:3]\n\n## Replicate our cross-validation from future_lapply.\n\n## Use %dorng% instead of the standard %dopar% to safely\n## generate random numbers in parallel (Section 5)\nlibrary(doRNG)\nout &lt;- foreach(i = seq_along(Y), .combine = c) %dorng% {\n    looFit(i, Y, X)\n}\nout[1:3]\n\nNote that foreach also provides functionality for collecting and managing the results to avoid some of the bookkeeping you would need to do if writing your own standard for loop. The result of foreach will generally be a list, unless we request the results be combined in different way, as we do here using .combine = c to use c() to get a vector rather than a list.\nYou can debug by running serially using %do% rather than %dopar%. Note that you may need to load packages within the foreach construct to ensure a package is available to all of the calculations.\nIt is possible to use foreach to parallelize over nested loops. Suppose that the outer loop has too few tasks to effectively parallelize over and you also want to parallelize over the inner loop as well. Provided the calculations in each task (defined based on the pair of indexes from both loops) are independent of the other tasks, you can define two foreach loops, with the outer foreach using the %:% operator and the inner foreach using the usual %dopar% operator. More details can be found in this foreach vignette.\n\n\n\nThe future package automatically identifies the objects needed by your future-based code and makes copies of those objects once for each worker process (thankfully not once for each task).\nIf you’re working with large objects, making a copy of the objects for each of the worker processes can be a significant time cost and can greatly increase your memory use.\nThe multicore plan (not available on Windows or in RStudio) forks the main R process.\n\nplan(multicore, workers = 4)\n\nThis creates R worker processes with the same state as the original R process.\n\nImportantly, this means that global variables in the forked worker processes are just references to the objects in memory in the original R process.\nSo the additional processes do not use additional memory for those objects (despite what is shown in top as memory used by each process).\nAnd there is no time involved in making copies.\nHowever, if you modify objects in the worker processes then copies are made.\nYou can use these global variables in functions you call in parallel or pass the variables into functions as function arguments.\n\nSo, the take-home message is that using multicore on non-Windows machines can have a big advantage when working with large data objects.\n\n\n\nWe can use the cluster plan to run workers across multiple machines.\nIf we know the names of the machines and can access them via password-less SSH (e.g., using ssh keys), then we can simply provide the names of the machines to create a cluster and use the ‘cluster’ plan.\nHere we want to use two cores on one machine and two on another.\n\nlibrary(future.apply)\nworkers &lt;- c(rep('arwen.berkeley.edu', 2), rep('radagast.berkeley.edu', 2))\nplan(cluster, workers = workers)\n# Now use parallel_lapply, foreach, etc. as before\n\nIf you are using the Slurm scheduler on a Linux cluster and in your sbatch or srun command you use --ntasks, then the following will allow you to use as many workers as the value of ntasks. One caveat is that one still needs to be able to access the various machines via password-less SSH.\n\nplan(cluster)\n# Now use parallel_lapply, parallel_sapply, foreach, etc. as before\n\nAlternatively, you could set specify the workers manually. Here we use srun (note this is being done within our original sbatch or srun) to run hostname once per Slurm task, returning the name of the node the task is assigned to.\n\nworkers &lt;- system('srun hostname', intern = TRUE)\nplan(cluster, workers = workers)\n# Now use parallel_lapply, parallel_sapply, foreach, etc. as before\n\nIn all cases, we can verify that the workers are running on the various nodes by checking the nodename of each of the workers:\n\ntmp &lt;- future_sapply(seq_len(nbrOfWorkers()), \n              function(i)\n                cat(\"Worker running in process\", Sys.getpid(),\n                    \"on\", Sys.info()[['nodename']], \"\\n\"))\n\n\n\n\n\nThe future package allows you to do everything that one can do using older packages/functions such as mclapply, parLapply and foreach wit backends such as doParallel, doSNOW, doMPI. So my recommendation is just to use the future package. But here is some syntax for the older approaches.\nAs with calculations using the future package, all of the functionality discussed here applies only if the iterations/loops of your calculations can be done completely separately and do not depend on one another. This scenario is called an embarrassingly parallel computation. So coding up the evolution of a time series or a Markov chain is not possible using these tools. However, bootstrapping, random forests, simulation studies, cross-validation and many other statistical methods can be handled in this way.\n\n\nHere are a couple of the ways to do a parallel lapply:\n\nlibrary(parallel)\nnCores &lt;- 4  \ncl &lt;- makeCluster(nCores) \n\n# clusterExport(cl, c('x', 'y')) # if the processes need objects\n# from master's workspace (not needed here as no global vars used)\n\n# First approach: parLapply\nresult1 &lt;- parLapply(cl, seq_along(Y), looFit, Y, X)\n# Second approach: mclapply\nresult2 &lt;- mclapply(seq_along(Y), looFit, Y, X)\n\n\n\n\nAnd here’s how to use doParallel with foreach instead of doFuture.\n\nlibrary(doParallel)  # uses parallel package, a core R package\n\nnCores &lt;- 4  \nregisterDoParallel(nCores)\n\nout &lt;- foreach(i = seq_along(Y)) %dopar% {\n    looFit(i, Y, X)\n}\n\n\n\n\nWhether you need to explicitly load packages and export global variables from the main process to the parallelized worker processes depends on the details of how you are doing the parallelization.\nUnder several scenarios (but only on Linux and MacOS, not on Windows), packages and global variables in the main R process are automatically available to the worker tasks without any work on your part. These scenarios are\n\nforeach with the doParallel backend,\nparallel lapply (and related) statements when starting the cluster via makeForkCluster, instead of the usual makeCluster, and\nuse of mclapply.\n\nThis is because all of these approaches fork the original R process, thereby creating worker processes with the same state as the original R process. Interestingly, this means that global variables in the forked worker processes are just references to the objects in memory in the original R process. So the additional processes do not use additional memory for those objects (despite what is shown in top) and there is no time involved in making copies. However, if you modify objects in the worker processes then copies are made.\nCaveat: with mclapply you can use a global variable in functions you call in parallel or pass the global variable in as an argument, in both cases without copying. However with parLapply and makeForkCluster, passing the global variable as an argument results in copies being made for some reason.\nImportantly, because forking is not available on Windows, the above statements only apply on Linux and MacOS.\nIn contrast, with parallel lapply (and related) statements (but not foreach) when starting the cluster using the standard makeCluster (which sets up a so-called PSOCK cluster, starting the R worker processes via Rscript), one needs to load packages within the code that is executed in parallel. In addition one needs to use clusterExport to tell R which objects in the global environment should be available to the worker processes. This involves making as many copies of the objects as there are worker processes, so one can easily exceed the physical memory (RAM) on the machine if one has large objects, and the copying of large objects will take time.\n\n\n\nOne can set up a cluster of workers across multiple nodes using parallel::makeCluster. Then one can use parLapply and foreach with that cluster of workers.\n\nlibrary(parallel)\nmachines = c(rep(\"gandalf.berkeley.edu\", 2), rep(\"arwen.berkeley.edu\", 2))\n\ncl = makeCluster(machines, type = \"SOCK\")\n\n# With parLapply or parSapply:\n\nparSapply(cl, 1:5, function(i) return(mean(1:i)))\n\n[1] 1.0 1.5 2.0 2.5 3.0\n\n# With foreach:\nlibrary(doSNOW, quietly = TRUE)\n\n\nAttaching package: 'snow'\n\n\nThe following objects are masked from 'package:parallel':\n\n    closeNode, clusterApply, clusterApplyLB, clusterCall, clusterEvalQ,\n    clusterExport, clusterMap, clusterSplit, makeCluster, parApply,\n    parCapply, parLapply, parRapply, parSapply, recvData, recvOneData,\n    sendData, splitIndices, stopCluster\n\nregisterDoSNOW(cl)\n# Now use foreach as usual\n\nFor foreach, we used the doSNOW backend. The doSNOW backend has the advantage over doMPI that it doesn’t need to have MPI installed on the system.\n\n\n\n\nThe key thing when thinking about random numbers in a parallel context is that you want to avoid having the same ‘random’ numbers occur on multiple processes. On a computer, random numbers are not actually random but are generated as a sequence of pseudo-random numbers designed to mimic true random numbers. The sequence is finite (but very long) and eventually repeats itself. When one sets a seed, one is choosing a position in that sequence to start from. Subsequent random numbers are based on that subsequence. All random numbers can be generated from one or more random uniform numbers, so we can just think about a sequence of values between 0 and 1.\nThe worst thing that could happen is that one sets things up in such a way that every process is using the same sequence of random numbers. This could happen if you mistakenly set the same seed in each process, e.g., using set.seed(mySeed) in R on every process.\nThe naive approach is to use a different seed for each process. E.g., if your processes are numbered id = 1,2,...,p with a variable id that is unique to a process, setting the seed to be the value of id on each process. This is likely not to cause problems, but raises the danger that two (or more sequences) might overlap. For an algorithm with dependence on the full sequence, such as an MCMC, this probably won’t cause big problems (though you likely wouldn’t know if it did), but for something like simple simulation studies, some of your ‘independent’ samples could be exact replicates of a sample on another process. Given the period length of the default generators in R, this is actually quite unlikely, but it is a bit sloppy.\nTo avoid this problem, the key is to use an algorithm that ensures sequences that do not overlap.\nIn R, the rlecuyer package deals with this. The L’Ecuyer algorithm has a period of \\(2^{191}\\), which it divides into subsequences of length \\(2^{127}\\).\n\n\nThe future package integrates well with the L’Ecuyer parallel RNG approach, which guarantees non-overlapping random numbers. There is a good discussion about seeds for future_lapply and future_sapply in the help for those functions.\n\n\nHere we can set a single seed. Behind the scenes the L’Ecuyer-CMRG RNG is used so that the random numbers generated for each iteration are independent. Note there is some overhead here when the number of iterations is large.\n\nlibrary(future.apply)\nn &lt;- 40\nset.seed(1)\nout1 &lt;- future_sapply(1:n, function(i) rnorm(1), future.seed = TRUE)\nset.seed(1)\nout2 &lt;- future_sapply(1:n, function(i) rnorm(1), future.seed = TRUE)\nidentical(out1, out2)\n\n[1] TRUE\n\n\nBasically future_lapply pregenerates a seed for each iteration using parallel:::nextRNGStream, which uses the L’Ecuyer algorithm. See more details here.\nI could also have set future.seed to a numeric value, instead of setting the seed using set.seed, to make the generated results reproducible.\n\n\n\nWhen using %dofuture%, you can simply include .options.future = list(seed = TRUE) to ensure parallel RNG is done safely (shown in Section 3.2). If you forget and have RNG in your parallelized code, doFuture will warn you.\nBefore version 1.0.0 of doFuture, one would need to use the %doRNG% operator with foreach to ensure correct RNG with foreach (also seen in Section 3.2).\n\n\n\n\n\n\nHere’s how you initialize independent sequences on different processes when using the parallel package’s parallel lapply functionality.\n\nlibrary(parallel)\nlibrary(rlecuyer)\nnCores &lt;- 4\ncl &lt;- makeCluster(nCores)\niseed &lt;- 1\nclusterSetRNGStream(cl = cl, iseed = iseed)\n## Now proceed with your parLapply, using the `cl` object\n\nWith mclapply you can set the argument mc.set.seed = TRUE, which is the default. This will give different seeds for each process, but for safety, you should choose the L’Ecuyer algorithm via RNGkind(\"L'Ecuyer-CMRG\") before running mclapply.\n\n\n\nFor foreach, you can use registerDoRNG:\n\nlibrary(doRNG)\nlibrary(doParallel)\nregisterDoParallel(4)\nregisterDoRNG(seed = 1)\n## Now use foreach with %dopar%\n\n\n\n\nWhen using mclapply, you can use the mc.set.seed argument as follows (note that mc.set.seed is TRUE by default, so you should get different seeds for the different processes by default), but one needs to invoke RNGkind(\"L'Ecuyer-CMRG\") to get independent streams via the L’Ecuyer algorithm.\n\nlibrary(parallel)\nlibrary(rlecuyer)\nRNGkind(\"L'Ecuyer-CMRG\")\nres &lt;- mclapply(seq_len(Y), looFit, Y, X, mc.cores = 4, \n    mc.set.seed = TRUE) \n\n\n\n\n\n\npartools is a package developed by Norm Matloff at UC-Davis. He has the perspective that Spark/Hadoop are not the right tools in many cases when doing statistics-related work and has developed some simple tools for parallelizing computation across multiple nodes, also referred to as Snowdoop. The tools make use of the key idea in Spark/Hadoop of a distributed file system and distributed data objects but avoid the complications of trying to ensure fault tolerance, which is critical only on very large clusters of machines.\nI won’t go into details, but partools allows you to split up your data across multiple nodes and then read the data into R in parallel across R sessions running on those nodes, all controlled from a single master R session. You can then do operations on the subsets and gather results back to the master session as needed. One point that confused me in the partools vignette is that it shows how to split up a dataset that you can read into your R session, but it’s not clear what one does if the dataset is too big to read into a single R session.",
    "crumbs": [
      "Parallel R"
    ]
  },
  {
    "objectID": "parallel-R.html#overview",
    "href": "parallel-R.html#overview",
    "title": "Parallel processing in R",
    "section": "",
    "text": "R provides a variety of functionality for parallelization, including threaded operations (linear algebra), parallel for loops and lapply-type statements, and parallelization across multiple machines. This material focuses on R’s future package, a flexible and powerful approach to parallelization in R.",
    "crumbs": [
      "Parallel R"
    ]
  },
  {
    "objectID": "parallel-R.html#threading",
    "href": "parallel-R.html#threading",
    "title": "Parallel processing in R",
    "section": "",
    "text": "Threading in R is limited to linear algebra, provided R is linked against a threaded BLAS.\n\n\nThe BLAS is the library of basic linear algebra operations (written in Fortran or C). A fast BLAS can greatly speed up linear algebra relative to the default BLAS on a machine. Some fast BLAS libraries are\n\nIntel’s MKL; may be available for educational use for free\nOpenBLAS; open source and free\nvecLib for Macs; provided with your Mac (both newer Apple Silicon and older Intel Macs)\n\nIn addition to being fast when used on a single core, all of these BLAS libraries are threaded - if your computer has multiple cores and there are free resources, your linear algebra will use multiple cores, provided your installed R is linked against the threaded BLAS installed on your machine.\nYou can use an optimized BLAS on your own machine(s).\n\n\n\nHere’s some code that illustrates the speed of using a threaded BLAS:\n\nlibrary(RhpcBLASctl)  ## package that controls number of threads from within R\n\nx &lt;- matrix(rnorm(5000^2), 5000)\n\n## Control number of threads from within R. See next section for details.\nblas_set_num_threads(4)\nsystem.time({\n   x &lt;- crossprod(x)\n   U &lt;- chol(x)\n})\n\n#   user  system elapsed \n# 14.104   5.403   6.752 \n\nblas_set_num_threads(1)\nsystem.time({\n   x &lt;- crossprod(x)\n   U &lt;- chol(x)\n})\n\n#   user  system elapsed \n# 12.393   0.055  12.344 \n\nHere the elapsed time indicates that using four threads gave us a two times (2x) speedup in terms of real time, while the user time indicates that the threaded calculation took a bit more total processing time (combining time across all processors) because of the overhead of using multiple threads. So the threading helps, but it’s not the 4x linear speedup we would hope for.\n\n\n\nIn general, threaded code will detect the number of cores available on a machine and make use of them. However, you can also explicitly control the number of threads available to a process.\nFor most threaded code (that based on the openMP protocol), the number of threads can be set by setting the OMP_NUM_THREADS environment variable. Note that under some circumstances you may need to use VECLIB_MAXIMUM_THREADS if on an Intel (older) Mac or MKL_NUM_THREADS if R is linked against MKL (which can be seen by running sessionInfo). For information relevant for newer Apple Silicon (M1 and M2) based Macs see below.\nFor example, to set it for four threads in bash:\nexport OMP_NUM_THREADS=4\nDo this before starting your R or Python session or before running your compiled executable.\nAlternatively, you can set OMP_NUM_THREADS as you invoke your job, e.g., here with R:\nOMP_NUM_THREADS=4 R CMD BATCH --no-save job.R job.out\nFinally, the R package, RhpcBLASctl, allows you to control the number of threads from within R, as already seen in the example in the previous subsection.\n\nlibrary(RhpcBLASctl)\nblas_set_num_threads(4)\n# now run your linear algebra\n\n\n\n\nNote that newer Macs (Apple Silicon-based M1 and M2 Macs) also provide the Accelerate (vecLib) BLAS, but apparently they use the Mac’s AMX co-processor (details are hard to find online). This gives fast computation, but the calculations are not using the regular CPU cores and so one doesn’t choose the number of threads. In particular, VECLIB_MAXIMUM_THREADS has no effect, and top shows only a single CPU in use. Rest assured that if you’ve configured R to use Accelerate (vecLib) BLAS, you should see very good performance.",
    "crumbs": [
      "Parallel R"
    ]
  },
  {
    "objectID": "parallel-R.html#parallel-loops-including-parallel-lapply-via-the-future-package",
    "href": "parallel-R.html#parallel-loops-including-parallel-lapply-via-the-future-package",
    "title": "Parallel processing in R",
    "section": "",
    "text": "All of the functionality discussed here applies only if the iterations/loops of your calculations can be done completely separately and do not depend on one another. This scenario is called an embarrassingly parallel computation. So coding up the evolution of a time series or a Markov chain is not possible using these tools. However, bootstrapping, random forests, simulation studies, cross-validation and many other statistical methods can be handled in this way.\nOne can easily parallelize lapply (or sapply) statements or parallelize for loops using the future package. Here’s we’ll just show the basic mechanics of using the future package. There’s much more detail in this SCF tutorial.\nIn Sections 3.1 and 3.2, we’ll parallelize across multiple cores on one machine. Section 3.3 shows how to use multiple machines.\n\n\nHere we’ll parallelize an lapply operation. We need to call plan to set up the workers that will carry out the individual tasks (one for each element of the input list or vector) in parallel.\nThe multisession “plan” simply starts worker processes on the machine you are working on. You could skip the workers argument and the number of workers will equal the number of cores on your machine. Later we’ll see the use of the multicore and cluster “plans”, which set up the workers in a different way.\nHere we parallelize leave-one-out cross-validation for a random forest model.\n\nsource('rf.R')  # loads in data (X and Y) and looFit()\n\nlibrary(future.apply)\n## Set up four workers to run calculations in parallel\nplan(multisession, workers = 4)\n\n## Run the cross-validation in parallel, four tasks at a time on the four workers\nsystem.time(\n  out &lt;- future_lapply(seq_along(Y), looFit, Y, X, future.seed = TRUE)\n)   \n#   user  system elapsed \n#  0.684   0.086  19.831 \n\n\n## Side note: seq_along(Y) is a safe equivalent of 1:length(Y)\n\nThe use of future.seed ensures safe parallel random number generation as discussed in Section 5.\nHere the low user time is because the time spent in the worker processes is not counted at the level of the overall master process that dispatches the workers.\nNote that one can use plan without specifying the number of workers, in which case it will call parallelly::availableCores() and in general set the number of workers to a sensible value based on your system (and your scheduler allocation if your code is running on a cluster under a scheduler such as Slurm).\n\n\n\nWe can use the future package in combination with the foreach command to run a for loop in parallel. Of course this will only be valid if the iterations can be computed independently.\nThe syntax for foreach is a bit different than a standard for loop. Also note that the output for each iteration is simply the result of the last line in the { } body of the foreach statement.\nHere’s the syntax when using newer (version 1.0.0 and later) versions of doFuture:\n\nsource('rf.R')  # loads in data (X and Y) and looFit()\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\nlibrary(doFuture, quietly = TRUE)\n\nplan(multisession, workers = 4)\n\n## Toy example of using foreach+future\nout &lt;- foreach(i = seq_len(30)) %dofuture% {\n    mean(1:i)\n}\nout[1:3]\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 1.5\n\n[[3]]\n[1] 2\n\n## Replicate our cross-validation from future_lapply.\n\n## Add option to ensure safe random number generation in parallel\n## (discussed in Section 5)\nout &lt;- foreach(i = seq_along(Y), .combine = c,\n       .options.future = list(seed = TRUE)) %dofuture% {\n       looFit(i, Y, X)\n}\nout[1:3]\n\n        1         2         3 \n0.3151692 1.0207755 1.8871612 \n\n\nPrior to version 1.0.0 of doFuture, you’d do this:\n\nsource('rf.R')  # loads in data (X and Y) and looFit()\nlibrary(future)\nplan(multisession, workers = 4)\n\nlibrary(doFuture, quietly = TRUE)\nregisterDoFuture()\n\n## Toy example of using foreach+future\nout &lt;- foreach(i = seq_len(30)) %dopar% {\n    mean(1:i)\n}\nout[1:3]\n\n## Replicate our cross-validation from future_lapply.\n\n## Use %dorng% instead of the standard %dopar% to safely\n## generate random numbers in parallel (Section 5)\nlibrary(doRNG)\nout &lt;- foreach(i = seq_along(Y), .combine = c) %dorng% {\n    looFit(i, Y, X)\n}\nout[1:3]\n\nNote that foreach also provides functionality for collecting and managing the results to avoid some of the bookkeeping you would need to do if writing your own standard for loop. The result of foreach will generally be a list, unless we request the results be combined in different way, as we do here using .combine = c to use c() to get a vector rather than a list.\nYou can debug by running serially using %do% rather than %dopar%. Note that you may need to load packages within the foreach construct to ensure a package is available to all of the calculations.\nIt is possible to use foreach to parallelize over nested loops. Suppose that the outer loop has too few tasks to effectively parallelize over and you also want to parallelize over the inner loop as well. Provided the calculations in each task (defined based on the pair of indexes from both loops) are independent of the other tasks, you can define two foreach loops, with the outer foreach using the %:% operator and the inner foreach using the usual %dopar% operator. More details can be found in this foreach vignette.\n\n\n\nThe future package automatically identifies the objects needed by your future-based code and makes copies of those objects once for each worker process (thankfully not once for each task).\nIf you’re working with large objects, making a copy of the objects for each of the worker processes can be a significant time cost and can greatly increase your memory use.\nThe multicore plan (not available on Windows or in RStudio) forks the main R process.\n\nplan(multicore, workers = 4)\n\nThis creates R worker processes with the same state as the original R process.\n\nImportantly, this means that global variables in the forked worker processes are just references to the objects in memory in the original R process.\nSo the additional processes do not use additional memory for those objects (despite what is shown in top as memory used by each process).\nAnd there is no time involved in making copies.\nHowever, if you modify objects in the worker processes then copies are made.\nYou can use these global variables in functions you call in parallel or pass the variables into functions as function arguments.\n\nSo, the take-home message is that using multicore on non-Windows machines can have a big advantage when working with large data objects.\n\n\n\nWe can use the cluster plan to run workers across multiple machines.\nIf we know the names of the machines and can access them via password-less SSH (e.g., using ssh keys), then we can simply provide the names of the machines to create a cluster and use the ‘cluster’ plan.\nHere we want to use two cores on one machine and two on another.\n\nlibrary(future.apply)\nworkers &lt;- c(rep('arwen.berkeley.edu', 2), rep('radagast.berkeley.edu', 2))\nplan(cluster, workers = workers)\n# Now use parallel_lapply, foreach, etc. as before\n\nIf you are using the Slurm scheduler on a Linux cluster and in your sbatch or srun command you use --ntasks, then the following will allow you to use as many workers as the value of ntasks. One caveat is that one still needs to be able to access the various machines via password-less SSH.\n\nplan(cluster)\n# Now use parallel_lapply, parallel_sapply, foreach, etc. as before\n\nAlternatively, you could set specify the workers manually. Here we use srun (note this is being done within our original sbatch or srun) to run hostname once per Slurm task, returning the name of the node the task is assigned to.\n\nworkers &lt;- system('srun hostname', intern = TRUE)\nplan(cluster, workers = workers)\n# Now use parallel_lapply, parallel_sapply, foreach, etc. as before\n\nIn all cases, we can verify that the workers are running on the various nodes by checking the nodename of each of the workers:\n\ntmp &lt;- future_sapply(seq_len(nbrOfWorkers()), \n              function(i)\n                cat(\"Worker running in process\", Sys.getpid(),\n                    \"on\", Sys.info()[['nodename']], \"\\n\"))",
    "crumbs": [
      "Parallel R"
    ]
  },
  {
    "objectID": "parallel-R.html#older-alternatives-to-the-future-package-for-parallel-loopslapply",
    "href": "parallel-R.html#older-alternatives-to-the-future-package-for-parallel-loopslapply",
    "title": "Parallel processing in R",
    "section": "",
    "text": "The future package allows you to do everything that one can do using older packages/functions such as mclapply, parLapply and foreach wit backends such as doParallel, doSNOW, doMPI. So my recommendation is just to use the future package. But here is some syntax for the older approaches.\nAs with calculations using the future package, all of the functionality discussed here applies only if the iterations/loops of your calculations can be done completely separately and do not depend on one another. This scenario is called an embarrassingly parallel computation. So coding up the evolution of a time series or a Markov chain is not possible using these tools. However, bootstrapping, random forests, simulation studies, cross-validation and many other statistical methods can be handled in this way.\n\n\nHere are a couple of the ways to do a parallel lapply:\n\nlibrary(parallel)\nnCores &lt;- 4  \ncl &lt;- makeCluster(nCores) \n\n# clusterExport(cl, c('x', 'y')) # if the processes need objects\n# from master's workspace (not needed here as no global vars used)\n\n# First approach: parLapply\nresult1 &lt;- parLapply(cl, seq_along(Y), looFit, Y, X)\n# Second approach: mclapply\nresult2 &lt;- mclapply(seq_along(Y), looFit, Y, X)\n\n\n\n\nAnd here’s how to use doParallel with foreach instead of doFuture.\n\nlibrary(doParallel)  # uses parallel package, a core R package\n\nnCores &lt;- 4  \nregisterDoParallel(nCores)\n\nout &lt;- foreach(i = seq_along(Y)) %dopar% {\n    looFit(i, Y, X)\n}\n\n\n\n\nWhether you need to explicitly load packages and export global variables from the main process to the parallelized worker processes depends on the details of how you are doing the parallelization.\nUnder several scenarios (but only on Linux and MacOS, not on Windows), packages and global variables in the main R process are automatically available to the worker tasks without any work on your part. These scenarios are\n\nforeach with the doParallel backend,\nparallel lapply (and related) statements when starting the cluster via makeForkCluster, instead of the usual makeCluster, and\nuse of mclapply.\n\nThis is because all of these approaches fork the original R process, thereby creating worker processes with the same state as the original R process. Interestingly, this means that global variables in the forked worker processes are just references to the objects in memory in the original R process. So the additional processes do not use additional memory for those objects (despite what is shown in top) and there is no time involved in making copies. However, if you modify objects in the worker processes then copies are made.\nCaveat: with mclapply you can use a global variable in functions you call in parallel or pass the global variable in as an argument, in both cases without copying. However with parLapply and makeForkCluster, passing the global variable as an argument results in copies being made for some reason.\nImportantly, because forking is not available on Windows, the above statements only apply on Linux and MacOS.\nIn contrast, with parallel lapply (and related) statements (but not foreach) when starting the cluster using the standard makeCluster (which sets up a so-called PSOCK cluster, starting the R worker processes via Rscript), one needs to load packages within the code that is executed in parallel. In addition one needs to use clusterExport to tell R which objects in the global environment should be available to the worker processes. This involves making as many copies of the objects as there are worker processes, so one can easily exceed the physical memory (RAM) on the machine if one has large objects, and the copying of large objects will take time.\n\n\n\nOne can set up a cluster of workers across multiple nodes using parallel::makeCluster. Then one can use parLapply and foreach with that cluster of workers.\n\nlibrary(parallel)\nmachines = c(rep(\"gandalf.berkeley.edu\", 2), rep(\"arwen.berkeley.edu\", 2))\n\ncl = makeCluster(machines, type = \"SOCK\")\n\n# With parLapply or parSapply:\n\nparSapply(cl, 1:5, function(i) return(mean(1:i)))\n\n[1] 1.0 1.5 2.0 2.5 3.0\n\n# With foreach:\nlibrary(doSNOW, quietly = TRUE)\n\n\nAttaching package: 'snow'\n\n\nThe following objects are masked from 'package:parallel':\n\n    closeNode, clusterApply, clusterApplyLB, clusterCall, clusterEvalQ,\n    clusterExport, clusterMap, clusterSplit, makeCluster, parApply,\n    parCapply, parLapply, parRapply, parSapply, recvData, recvOneData,\n    sendData, splitIndices, stopCluster\n\nregisterDoSNOW(cl)\n# Now use foreach as usual\n\nFor foreach, we used the doSNOW backend. The doSNOW backend has the advantage over doMPI that it doesn’t need to have MPI installed on the system.",
    "crumbs": [
      "Parallel R"
    ]
  },
  {
    "objectID": "parallel-R.html#parallel-random-number-generation",
    "href": "parallel-R.html#parallel-random-number-generation",
    "title": "Parallel processing in R",
    "section": "",
    "text": "The key thing when thinking about random numbers in a parallel context is that you want to avoid having the same ‘random’ numbers occur on multiple processes. On a computer, random numbers are not actually random but are generated as a sequence of pseudo-random numbers designed to mimic true random numbers. The sequence is finite (but very long) and eventually repeats itself. When one sets a seed, one is choosing a position in that sequence to start from. Subsequent random numbers are based on that subsequence. All random numbers can be generated from one or more random uniform numbers, so we can just think about a sequence of values between 0 and 1.\nThe worst thing that could happen is that one sets things up in such a way that every process is using the same sequence of random numbers. This could happen if you mistakenly set the same seed in each process, e.g., using set.seed(mySeed) in R on every process.\nThe naive approach is to use a different seed for each process. E.g., if your processes are numbered id = 1,2,...,p with a variable id that is unique to a process, setting the seed to be the value of id on each process. This is likely not to cause problems, but raises the danger that two (or more sequences) might overlap. For an algorithm with dependence on the full sequence, such as an MCMC, this probably won’t cause big problems (though you likely wouldn’t know if it did), but for something like simple simulation studies, some of your ‘independent’ samples could be exact replicates of a sample on another process. Given the period length of the default generators in R, this is actually quite unlikely, but it is a bit sloppy.\nTo avoid this problem, the key is to use an algorithm that ensures sequences that do not overlap.\nIn R, the rlecuyer package deals with this. The L’Ecuyer algorithm has a period of \\(2^{191}\\), which it divides into subsequences of length \\(2^{127}\\).\n\n\nThe future package integrates well with the L’Ecuyer parallel RNG approach, which guarantees non-overlapping random numbers. There is a good discussion about seeds for future_lapply and future_sapply in the help for those functions.\n\n\nHere we can set a single seed. Behind the scenes the L’Ecuyer-CMRG RNG is used so that the random numbers generated for each iteration are independent. Note there is some overhead here when the number of iterations is large.\n\nlibrary(future.apply)\nn &lt;- 40\nset.seed(1)\nout1 &lt;- future_sapply(1:n, function(i) rnorm(1), future.seed = TRUE)\nset.seed(1)\nout2 &lt;- future_sapply(1:n, function(i) rnorm(1), future.seed = TRUE)\nidentical(out1, out2)\n\n[1] TRUE\n\n\nBasically future_lapply pregenerates a seed for each iteration using parallel:::nextRNGStream, which uses the L’Ecuyer algorithm. See more details here.\nI could also have set future.seed to a numeric value, instead of setting the seed using set.seed, to make the generated results reproducible.\n\n\n\nWhen using %dofuture%, you can simply include .options.future = list(seed = TRUE) to ensure parallel RNG is done safely (shown in Section 3.2). If you forget and have RNG in your parallelized code, doFuture will warn you.\nBefore version 1.0.0 of doFuture, one would need to use the %doRNG% operator with foreach to ensure correct RNG with foreach (also seen in Section 3.2).\n\n\n\n\n\n\nHere’s how you initialize independent sequences on different processes when using the parallel package’s parallel lapply functionality.\n\nlibrary(parallel)\nlibrary(rlecuyer)\nnCores &lt;- 4\ncl &lt;- makeCluster(nCores)\niseed &lt;- 1\nclusterSetRNGStream(cl = cl, iseed = iseed)\n## Now proceed with your parLapply, using the `cl` object\n\nWith mclapply you can set the argument mc.set.seed = TRUE, which is the default. This will give different seeds for each process, but for safety, you should choose the L’Ecuyer algorithm via RNGkind(\"L'Ecuyer-CMRG\") before running mclapply.\n\n\n\nFor foreach, you can use registerDoRNG:\n\nlibrary(doRNG)\nlibrary(doParallel)\nregisterDoParallel(4)\nregisterDoRNG(seed = 1)\n## Now use foreach with %dopar%\n\n\n\n\nWhen using mclapply, you can use the mc.set.seed argument as follows (note that mc.set.seed is TRUE by default, so you should get different seeds for the different processes by default), but one needs to invoke RNGkind(\"L'Ecuyer-CMRG\") to get independent streams via the L’Ecuyer algorithm.\n\nlibrary(parallel)\nlibrary(rlecuyer)\nRNGkind(\"L'Ecuyer-CMRG\")\nres &lt;- mclapply(seq_len(Y), looFit, Y, X, mc.cores = 4, \n    mc.set.seed = TRUE)",
    "crumbs": [
      "Parallel R"
    ]
  },
  {
    "objectID": "parallel-R.html#the-partools-package",
    "href": "parallel-R.html#the-partools-package",
    "title": "Parallel processing in R",
    "section": "",
    "text": "partools is a package developed by Norm Matloff at UC-Davis. He has the perspective that Spark/Hadoop are not the right tools in many cases when doing statistics-related work and has developed some simple tools for parallelizing computation across multiple nodes, also referred to as Snowdoop. The tools make use of the key idea in Spark/Hadoop of a distributed file system and distributed data objects but avoid the complications of trying to ensure fault tolerance, which is critical only on very large clusters of machines.\nI won’t go into details, but partools allows you to split up your data across multiple nodes and then read the data into R in parallel across R sessions running on those nodes, all controlled from a single master R session. You can then do operations on the subsets and gather results back to the master session as needed. One point that confused me in the partools vignette is that it shows how to split up a dataset that you can read into your R session, but it’s not clear what one does if the dataset is too big to read into a single R session.",
    "crumbs": [
      "Parallel R"
    ]
  },
  {
    "objectID": "parallel-C.html",
    "href": "parallel-C.html",
    "title": "Parallel processing in C/C++",
    "section": "",
    "text": "Some long-standing tools for parallelizing C, C++, and Fortran code are openMP for writing threaded code to run in parallel on one machine and MPI for writing code that passages message to run in parallel across (usually) multiple nodes.\n\n\n\nIt’s straightforward to write threaded code in C and C++ (as well as Fortran) to exploit multiple cores. The basic approach is to use the OpenMP protocol.\n\n\nHere’s how one would parallelize a loop in C/C++ using an OpenMP compiler directive. In this case we are parallelizing the outer loop; the iterations of the outer loop are done in parallel, while the iterations of the inner loop are done serially within a thread. As with foreach in R, you only want to do this if the iterations do not depend on each other. The code is available as a C++ program (but the core of the code is just C code) in testOpenMP.cpp.\n// see testOpenMP.cpp\n#include &lt;iostream&gt;\nusing namespace std;\n\n// compile with:  g++ -fopenmp -L/usr/local/lib  \n//                  testOpenMP.cpp -o testOpenMP \n\nint main(){\n  int nReps = 20;\n  double x[nReps];\n  #pragma omp parallel for\n  for (int i=0; i&lt;nReps; i++){\n    x[i] = 0.0;\n    for ( int j=0; j&lt;1000000000; j++){\n      x[i] = x[i] + 1.0;\n    }\n    cout &lt;&lt; x[i] &lt;&lt; endl;\n  }\n  return 0;\n}\nWe would compile this program as follows\n$ g++ -fopenmp testOpenMP.cpp -o testOpenMP\nThe main thing to be aware of in using OpenMP is not having different threads overwrite variables used by other threads. In the example above, variables declared within the #pragma directive will be recognized as variables that are private to each thread. In fact, you could declare int i before the compiler directive and things would be fine because OpenMP is smart enough to deal properly with the primary looping variable. But big problems would ensue if you had instead written the following code:\nint main(){\n  int nReps = 20;\n  int j;  // DON'T DO THIS !!!!!!!!!!!!!\n  double x[nReps];\n  #pragma omp parallel for\n  for (int i=0; i&lt;nReps; i++){\n    x[i] = 0.0;\n    for (j=0; j&lt;1000000000; j++){\n      x[i] = x[i] + 1.0;\n    }\n    cout &lt;&lt; x[i] &lt;&lt; endl;\n  }\n  return 0;\n}\nNote that we do want x declared before the compiler directive because we want all the threads to write to a common x (but, importantly, to different components of x). That’s the point!\nWe can also be explicit about what is shared and what is private to each thread:\nint main(){\n  int nReps = 20;\n  int i, j;\n  double x[nReps];\n  #pragma omp parallel for private(i,j) shared(x, nReps)\n  for (i=0; i&lt;nReps; i++){\n    x[i] = 0.0;\n    for (j=0; j&lt;1000000000; j++){\n      x[i] = x[i] + 1.0;\n    }\n    cout &lt;&lt; x[i] &lt;&lt; endl;\n  }\n  return 0;\n}\n\n\n\nThe easiest path here is to use the Rcpp package. In this case, you can write your C++ code with OpenMP pragma statements as in the previous subsection. You’ll need to make sure that the PKG_CXXFLAGS and PKG_LIBS environment variables are set to include -f openmp so the compilation is done correctly. More details/examples linked to from this Stack overflow post.\n\n\n\nThe goal here is just to give you a sense of what is possible with OpenMP.\nThe OpenMP API provides three components: compiler directives that parallelize your code (such as #pragma omp parallel for), library functions (such as omp_get_thread_num()), and environment variables (such as OMP_NUM_THREADS)\nOpenMP constructs apply to structured blocks of code. Blocks may be executed in parallel or sequentially, depending on how one uses the OpenMP pragma statements. One can also force execution of a block to wait until particular preceding blocks have finished, using a barrier.\nHere’s a basic “Hello, world” example that illustrates how it works (the full program is in helloWorldOpenMP.cpp):\n// see helloWorldOpenMP.cpp\n#include &lt;stdio.h&gt;\n#include &lt;omp.h&gt; // needed when using any openMP functions \n//                               such as omp_get_thread_num()\n\nvoid myFun(double *in, int id){\n// this is the function that would presumably do the heavy computational stuff\n}\n\nint main()\n{\n   int nthreads, myID;\n   double* input;\n   /* make the values of nthreads and myid private to each thread */\n   #pragma omp parallel private (nthreads, myID)\n   { // beginning of block\n      myID = omp_get_thread_num();\n      printf(\"Hello, I am thread %d\\n\", myID);\n      myFun(input, myID);  // do some computation on each thread\n      /* only main node print the number of threads */\n      if (myid == 0)\n      {\n         nthreads = omp_get_num_threads();\n         printf(\"I'm the boss and control %i threads. How come they're in front of me?\\n\", nThreads);\n      }\n   } // end of block\n   return 0;\n} \nThe parallel directive starts a team of threads, including the main thread, which is a member of the team and has thread number 0. The number of threads is determined in the following ways - here the first two options specify four threads:\n\n#pragma omp parallel NUM_THREADS (4) // set 4 threads for this parallel block\nomp_set_num_threads(4) // set four threads in general\nthe value of the OMP_NUM_THREADS environment variable\na default - usually the number of cores on the compute node\n\nNote that in #pragma omp parallel for, there are actually two instructions, parallel starts a team of threads, and for farms out the iterations to the team. In our parallel for invocation, we could have done it more explicitly as:\n#pragma omp parallel\n#pragma omp for\nWe can also explicitly distribute different chunks of code amongst different threads as seen here and in the full program in sectionsOpenMP.cpp.\n// see sectionsOpenMP.cpp\n#pragma omp parallel // starts a new team of threads\n{\n   Work0(); // this function would be run by all threads. \n   #pragma omp sections // divides the team into sections \n   { \n      // everything herein is run only once. \n      #pragma omp section \n      { Work1(); } \n      #pragma omp section \n      { \n         Work2(); \n         Work3(); \n      } \n      #pragma omp section \n      { Work4(); } \n   }\n} // implied barrier\nHere Work1, {Work2 + Work3} and Work4 are done in parallel, but Work2 and Work3 are done in sequence (on a single thread).\nIf one wants to make sure that all of a parallized calculation is complete before any further code is executed you can insert #pragma omp barrier.\nNote that a #pragma for statement includes an implicit barrier as does the end of any block specified with #pragma omp parallel.\nYou can use nowait if you explicitly want to prevent threads from waiting at an implicit barrier: e.g., #pragma omp parallel sections nowait or #pragma omp parallel for nowait\nOne should be careful about multiple threads writing to the same variable at the same time (this is an example of a race condition). In the example below, if one doesn’t have the #pragma omp critical directive two threads could read the current value of result at the same time and then sequentially write to result after incrementing their local copy, which would result in one of the increments being lost. A way to avoid this is with the critical directive (for single lines of code you can also use atomic instead of critical), as seen here and in the full program in criticalOpenMP.cpp:\n// see criticalOpenMP.cpp\ndouble result = 0.0;\ndouble tmp;\n#pragma omp parallel for private (tmp, i) shared (result)\nfor (int i=0; i&lt;n; i++){\n   tmp = myFun(i);\n   #pragma omp critical\n   result += tmp;\n}\nYou should also be able to use syntax like the following for the parallel for declaration (in which case you shouldn’t need the #pragma omp critical):\n#pragma omp parallel for reduction(+:result)\nI believe that doing this sort of calculation where multiple threads write to the same variable may be rather inefficient given time lost in waiting to have access to result, but presumably this would depend on how much time is spent in myFun() relative to the reduction operation.\n\n\n\n\n\n\nThere are multiple MPI implementations, of which openMPI and mpich are very common. openMPI is quite common, and we’ll use that.\nIn MPI programming, the same code runs on all the machines. This is called SPMD (single program, multiple data). As we saw a bit with the pbdR code, one invokes the same code (same program) multiple times, but the behavior of the code can be different based on querying the rank (ID) of the process. Since MPI operates in a distributed fashion, any transfer of information between processes must be done explicitly via send and receive calls (e.g., MPI_Send, MPI_Recv, MPI_Isend, and MPI_Irecv). (The ``MPI_’’ is for C code; C++ just has Send, Recv, etc.)\nThe latter two of these functions (MPI_Isend and MPI_Irecv) are so-called non-blocking calls. One important concept to understand is the difference between blocking and non-blocking calls. Blocking calls wait until the call finishes, while non-blocking calls return and allow the code to continue. Non-blocking calls can be more efficient, but can lead to problems with synchronization between processes.\nIn addition to send and receive calls to transfer to and from specific processes, there are calls that send out data to all processes (MPI_Scatter), gather data back (MPI_Gather) and perform reduction operations (MPI_Reduce).\nDebugging MPI code can be tricky because communication can hang, error messages from the workers may not be seen or readily accessible, and it can be difficult to assess the state of the worker processes.\n\n\n\nHere’s a basic hello world example The code is also in mpiHello.c.\n// see mpiHello.c\n#include &lt;stdio.h&gt; \n#include &lt;math.h&gt; \n#include &lt;mpi.h&gt;\n\nint main(int argc, char* argv) {     \n    int myrank, nprocs, namelen;     \n    char process_name[MPI_MAX_PROCESSOR_NAME];\n    MPI_Init(&argc, &argv);     \n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);   \n    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);          \n    MPI_Get_processor_name(process_name, &namelen);            \n    printf(\"Hello from process %d of %d on %s\\n\", \n        myrank, nprocs, process_name);\n    MPI_Finalize();     \n    return 0; \n} \nThere are C (mpicc) and C++ (mpic++) compilers for MPI programs (mpicxx and mpiCC are synonyms). I’ll use the MPI C++ compiler even though the code is all plain C code.\nThen we’ll run the executable via mpirun. Here the code will just run on my single machine, called arwen. See Section 3.3 for details on how to run on multiple machines.\nmpicxx mpiHello.c -o mpiHello\nmpirun -np 4 mpiHello\nHere’s the output we would expect:\n## Hello from processor 0 of 4 on arwen\n## Hello from processor 1 of 4 on arwen\n## Hello from processor 2 of 4 on arwen\n## Hello from processor 3 of 4 on arwen\nTo actually write real MPI code, you’ll need to go learn some of the MPI syntax. See quad_mpi.c and quad_mpi.cpp, which are example C and C++ programs (for approximating an integral via quadrature) that show some of the basic MPI functions. Compilation and running are as above:\nmpicxx quad_mpi.cpp -o quad_mpi\nmpirun -machinefile .hosts -np 4 quad_mpi\nAnd here’s the output we would expect:\n23 November 2021 03:28:25 PM\n\nQUAD_MPI\n  C++/MPI version\n  Estimate an integral of f(x) from A to B.\n  f(x) = 50 / (pi * ( 2500 * x * x + 1 ) )\n\n  A = 0\n  B = 10\n  N = 999999999\n  EXACT =       0.4993633810764567\n\n  Use MPI to divide the computation among 4 total processes,\n  of which one is the main process and does not do core computations.\n  Process 1 contributed MY_TOTAL = 0.49809\n  Process 2 contributed MY_TOTAL = 0.00095491\n  Process 3 contributed MY_TOTAL = 0.000318308\n\n  Estimate =       0.4993634591634721\n  Error = 7.808701535383378e-08\n  Time = 10.03146505355835\n  Process 2 contributed MY_TOTAL = 0.00095491\n\nQUAD_MPI:\n  Normal end of execution.\n\n23 November 2021 03:28:36 PM\n\n\n\nMPI-based executables require that you start your process(es) in a special way via the mpirun command. Note that mpirun, mpiexec and orterun are synonyms under openMPI.\nThe basic requirements for starting such a job are that you specify the number of processes you want to run and that you indicate what machines those processes should run on. Those machines should be networked together such that MPI can ssh to the various machines without any password required.\n\n\nThere are two ways to tell mpirun the machines on which to run the worker processes.\nFirst, we can pass the machine names directly, replicating the name if we want multiple processes on a single machine. In the example here, these are machines accessible to me, and you would need to replace those names with the names of machines you have access to. You’ll need to set up SSH keys so that you can access the machines without a password.\nmpirun --host gandalf,radagast,arwen,arwen -np 4 hostname\nAlternatively, we can create a file with the relevant information.\necho 'gandalf slots=1' &gt; .hosts\necho 'radagast slots=1' &gt;&gt; .hosts\necho 'arwen slots=2' &gt;&gt; .hosts\nmpirun -machinefile .hosts -np 4 hostname\nOne can also just duplicate a given machine name as many times as desired, rather than using slots.\n\n\n\nIf you are running your code as part of a job submitted to Slurm, you generally won’t need to pass the machinefile or np arguments as MPI will get that information from Slurm. So you can simply run your executable, in this case first checking which machines mpirun is using:\nmpirun hostname\nmpirun quad_mpi\n\n\n\nTo limit the number of threads for each process, we can tell mpirun to export the value of OMP_NUM_THREADS to the processes. E.g., calling a C program, quad_mpi:\nexport OMP_NUM_THREADS=2\nmpirun -machinefile .hosts -np 4 -x OMP_NUM_THREADS quad_mpi\nThere are additional details involved in carefully controlling how processes are allocated to nodes, but the default arguments for mpirun should do a reasonable job in many situations.",
    "crumbs": [
      "Parallel C/C++"
    ]
  },
  {
    "objectID": "parallel-C.html#overview",
    "href": "parallel-C.html#overview",
    "title": "Parallel processing in C/C++",
    "section": "",
    "text": "Some long-standing tools for parallelizing C, C++, and Fortran code are openMP for writing threaded code to run in parallel on one machine and MPI for writing code that passages message to run in parallel across (usually) multiple nodes.",
    "crumbs": [
      "Parallel C/C++"
    ]
  },
  {
    "objectID": "parallel-C.html#using-openmp-threads-for-basic-shared-memory-programming-in-c",
    "href": "parallel-C.html#using-openmp-threads-for-basic-shared-memory-programming-in-c",
    "title": "Parallel processing in C/C++",
    "section": "",
    "text": "It’s straightforward to write threaded code in C and C++ (as well as Fortran) to exploit multiple cores. The basic approach is to use the OpenMP protocol.\n\n\nHere’s how one would parallelize a loop in C/C++ using an OpenMP compiler directive. In this case we are parallelizing the outer loop; the iterations of the outer loop are done in parallel, while the iterations of the inner loop are done serially within a thread. As with foreach in R, you only want to do this if the iterations do not depend on each other. The code is available as a C++ program (but the core of the code is just C code) in testOpenMP.cpp.\n// see testOpenMP.cpp\n#include &lt;iostream&gt;\nusing namespace std;\n\n// compile with:  g++ -fopenmp -L/usr/local/lib  \n//                  testOpenMP.cpp -o testOpenMP \n\nint main(){\n  int nReps = 20;\n  double x[nReps];\n  #pragma omp parallel for\n  for (int i=0; i&lt;nReps; i++){\n    x[i] = 0.0;\n    for ( int j=0; j&lt;1000000000; j++){\n      x[i] = x[i] + 1.0;\n    }\n    cout &lt;&lt; x[i] &lt;&lt; endl;\n  }\n  return 0;\n}\nWe would compile this program as follows\n$ g++ -fopenmp testOpenMP.cpp -o testOpenMP\nThe main thing to be aware of in using OpenMP is not having different threads overwrite variables used by other threads. In the example above, variables declared within the #pragma directive will be recognized as variables that are private to each thread. In fact, you could declare int i before the compiler directive and things would be fine because OpenMP is smart enough to deal properly with the primary looping variable. But big problems would ensue if you had instead written the following code:\nint main(){\n  int nReps = 20;\n  int j;  // DON'T DO THIS !!!!!!!!!!!!!\n  double x[nReps];\n  #pragma omp parallel for\n  for (int i=0; i&lt;nReps; i++){\n    x[i] = 0.0;\n    for (j=0; j&lt;1000000000; j++){\n      x[i] = x[i] + 1.0;\n    }\n    cout &lt;&lt; x[i] &lt;&lt; endl;\n  }\n  return 0;\n}\nNote that we do want x declared before the compiler directive because we want all the threads to write to a common x (but, importantly, to different components of x). That’s the point!\nWe can also be explicit about what is shared and what is private to each thread:\nint main(){\n  int nReps = 20;\n  int i, j;\n  double x[nReps];\n  #pragma omp parallel for private(i,j) shared(x, nReps)\n  for (i=0; i&lt;nReps; i++){\n    x[i] = 0.0;\n    for (j=0; j&lt;1000000000; j++){\n      x[i] = x[i] + 1.0;\n    }\n    cout &lt;&lt; x[i] &lt;&lt; endl;\n  }\n  return 0;\n}\n\n\n\nThe easiest path here is to use the Rcpp package. In this case, you can write your C++ code with OpenMP pragma statements as in the previous subsection. You’ll need to make sure that the PKG_CXXFLAGS and PKG_LIBS environment variables are set to include -f openmp so the compilation is done correctly. More details/examples linked to from this Stack overflow post.\n\n\n\nThe goal here is just to give you a sense of what is possible with OpenMP.\nThe OpenMP API provides three components: compiler directives that parallelize your code (such as #pragma omp parallel for), library functions (such as omp_get_thread_num()), and environment variables (such as OMP_NUM_THREADS)\nOpenMP constructs apply to structured blocks of code. Blocks may be executed in parallel or sequentially, depending on how one uses the OpenMP pragma statements. One can also force execution of a block to wait until particular preceding blocks have finished, using a barrier.\nHere’s a basic “Hello, world” example that illustrates how it works (the full program is in helloWorldOpenMP.cpp):\n// see helloWorldOpenMP.cpp\n#include &lt;stdio.h&gt;\n#include &lt;omp.h&gt; // needed when using any openMP functions \n//                               such as omp_get_thread_num()\n\nvoid myFun(double *in, int id){\n// this is the function that would presumably do the heavy computational stuff\n}\n\nint main()\n{\n   int nthreads, myID;\n   double* input;\n   /* make the values of nthreads and myid private to each thread */\n   #pragma omp parallel private (nthreads, myID)\n   { // beginning of block\n      myID = omp_get_thread_num();\n      printf(\"Hello, I am thread %d\\n\", myID);\n      myFun(input, myID);  // do some computation on each thread\n      /* only main node print the number of threads */\n      if (myid == 0)\n      {\n         nthreads = omp_get_num_threads();\n         printf(\"I'm the boss and control %i threads. How come they're in front of me?\\n\", nThreads);\n      }\n   } // end of block\n   return 0;\n} \nThe parallel directive starts a team of threads, including the main thread, which is a member of the team and has thread number 0. The number of threads is determined in the following ways - here the first two options specify four threads:\n\n#pragma omp parallel NUM_THREADS (4) // set 4 threads for this parallel block\nomp_set_num_threads(4) // set four threads in general\nthe value of the OMP_NUM_THREADS environment variable\na default - usually the number of cores on the compute node\n\nNote that in #pragma omp parallel for, there are actually two instructions, parallel starts a team of threads, and for farms out the iterations to the team. In our parallel for invocation, we could have done it more explicitly as:\n#pragma omp parallel\n#pragma omp for\nWe can also explicitly distribute different chunks of code amongst different threads as seen here and in the full program in sectionsOpenMP.cpp.\n// see sectionsOpenMP.cpp\n#pragma omp parallel // starts a new team of threads\n{\n   Work0(); // this function would be run by all threads. \n   #pragma omp sections // divides the team into sections \n   { \n      // everything herein is run only once. \n      #pragma omp section \n      { Work1(); } \n      #pragma omp section \n      { \n         Work2(); \n         Work3(); \n      } \n      #pragma omp section \n      { Work4(); } \n   }\n} // implied barrier\nHere Work1, {Work2 + Work3} and Work4 are done in parallel, but Work2 and Work3 are done in sequence (on a single thread).\nIf one wants to make sure that all of a parallized calculation is complete before any further code is executed you can insert #pragma omp barrier.\nNote that a #pragma for statement includes an implicit barrier as does the end of any block specified with #pragma omp parallel.\nYou can use nowait if you explicitly want to prevent threads from waiting at an implicit barrier: e.g., #pragma omp parallel sections nowait or #pragma omp parallel for nowait\nOne should be careful about multiple threads writing to the same variable at the same time (this is an example of a race condition). In the example below, if one doesn’t have the #pragma omp critical directive two threads could read the current value of result at the same time and then sequentially write to result after incrementing their local copy, which would result in one of the increments being lost. A way to avoid this is with the critical directive (for single lines of code you can also use atomic instead of critical), as seen here and in the full program in criticalOpenMP.cpp:\n// see criticalOpenMP.cpp\ndouble result = 0.0;\ndouble tmp;\n#pragma omp parallel for private (tmp, i) shared (result)\nfor (int i=0; i&lt;n; i++){\n   tmp = myFun(i);\n   #pragma omp critical\n   result += tmp;\n}\nYou should also be able to use syntax like the following for the parallel for declaration (in which case you shouldn’t need the #pragma omp critical):\n#pragma omp parallel for reduction(+:result)\nI believe that doing this sort of calculation where multiple threads write to the same variable may be rather inefficient given time lost in waiting to have access to result, but presumably this would depend on how much time is spent in myFun() relative to the reduction operation.",
    "crumbs": [
      "Parallel C/C++"
    ]
  },
  {
    "objectID": "parallel-C.html#mpi",
    "href": "parallel-C.html#mpi",
    "title": "Parallel processing in C/C++",
    "section": "",
    "text": "There are multiple MPI implementations, of which openMPI and mpich are very common. openMPI is quite common, and we’ll use that.\nIn MPI programming, the same code runs on all the machines. This is called SPMD (single program, multiple data). As we saw a bit with the pbdR code, one invokes the same code (same program) multiple times, but the behavior of the code can be different based on querying the rank (ID) of the process. Since MPI operates in a distributed fashion, any transfer of information between processes must be done explicitly via send and receive calls (e.g., MPI_Send, MPI_Recv, MPI_Isend, and MPI_Irecv). (The ``MPI_’’ is for C code; C++ just has Send, Recv, etc.)\nThe latter two of these functions (MPI_Isend and MPI_Irecv) are so-called non-blocking calls. One important concept to understand is the difference between blocking and non-blocking calls. Blocking calls wait until the call finishes, while non-blocking calls return and allow the code to continue. Non-blocking calls can be more efficient, but can lead to problems with synchronization between processes.\nIn addition to send and receive calls to transfer to and from specific processes, there are calls that send out data to all processes (MPI_Scatter), gather data back (MPI_Gather) and perform reduction operations (MPI_Reduce).\nDebugging MPI code can be tricky because communication can hang, error messages from the workers may not be seen or readily accessible, and it can be difficult to assess the state of the worker processes.\n\n\n\nHere’s a basic hello world example The code is also in mpiHello.c.\n// see mpiHello.c\n#include &lt;stdio.h&gt; \n#include &lt;math.h&gt; \n#include &lt;mpi.h&gt;\n\nint main(int argc, char* argv) {     \n    int myrank, nprocs, namelen;     \n    char process_name[MPI_MAX_PROCESSOR_NAME];\n    MPI_Init(&argc, &argv);     \n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);   \n    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);          \n    MPI_Get_processor_name(process_name, &namelen);            \n    printf(\"Hello from process %d of %d on %s\\n\", \n        myrank, nprocs, process_name);\n    MPI_Finalize();     \n    return 0; \n} \nThere are C (mpicc) and C++ (mpic++) compilers for MPI programs (mpicxx and mpiCC are synonyms). I’ll use the MPI C++ compiler even though the code is all plain C code.\nThen we’ll run the executable via mpirun. Here the code will just run on my single machine, called arwen. See Section 3.3 for details on how to run on multiple machines.\nmpicxx mpiHello.c -o mpiHello\nmpirun -np 4 mpiHello\nHere’s the output we would expect:\n## Hello from processor 0 of 4 on arwen\n## Hello from processor 1 of 4 on arwen\n## Hello from processor 2 of 4 on arwen\n## Hello from processor 3 of 4 on arwen\nTo actually write real MPI code, you’ll need to go learn some of the MPI syntax. See quad_mpi.c and quad_mpi.cpp, which are example C and C++ programs (for approximating an integral via quadrature) that show some of the basic MPI functions. Compilation and running are as above:\nmpicxx quad_mpi.cpp -o quad_mpi\nmpirun -machinefile .hosts -np 4 quad_mpi\nAnd here’s the output we would expect:\n23 November 2021 03:28:25 PM\n\nQUAD_MPI\n  C++/MPI version\n  Estimate an integral of f(x) from A to B.\n  f(x) = 50 / (pi * ( 2500 * x * x + 1 ) )\n\n  A = 0\n  B = 10\n  N = 999999999\n  EXACT =       0.4993633810764567\n\n  Use MPI to divide the computation among 4 total processes,\n  of which one is the main process and does not do core computations.\n  Process 1 contributed MY_TOTAL = 0.49809\n  Process 2 contributed MY_TOTAL = 0.00095491\n  Process 3 contributed MY_TOTAL = 0.000318308\n\n  Estimate =       0.4993634591634721\n  Error = 7.808701535383378e-08\n  Time = 10.03146505355835\n  Process 2 contributed MY_TOTAL = 0.00095491\n\nQUAD_MPI:\n  Normal end of execution.\n\n23 November 2021 03:28:36 PM\n\n\n\nMPI-based executables require that you start your process(es) in a special way via the mpirun command. Note that mpirun, mpiexec and orterun are synonyms under openMPI.\nThe basic requirements for starting such a job are that you specify the number of processes you want to run and that you indicate what machines those processes should run on. Those machines should be networked together such that MPI can ssh to the various machines without any password required.\n\n\nThere are two ways to tell mpirun the machines on which to run the worker processes.\nFirst, we can pass the machine names directly, replicating the name if we want multiple processes on a single machine. In the example here, these are machines accessible to me, and you would need to replace those names with the names of machines you have access to. You’ll need to set up SSH keys so that you can access the machines without a password.\nmpirun --host gandalf,radagast,arwen,arwen -np 4 hostname\nAlternatively, we can create a file with the relevant information.\necho 'gandalf slots=1' &gt; .hosts\necho 'radagast slots=1' &gt;&gt; .hosts\necho 'arwen slots=2' &gt;&gt; .hosts\nmpirun -machinefile .hosts -np 4 hostname\nOne can also just duplicate a given machine name as many times as desired, rather than using slots.\n\n\n\nIf you are running your code as part of a job submitted to Slurm, you generally won’t need to pass the machinefile or np arguments as MPI will get that information from Slurm. So you can simply run your executable, in this case first checking which machines mpirun is using:\nmpirun hostname\nmpirun quad_mpi\n\n\n\nTo limit the number of threads for each process, we can tell mpirun to export the value of OMP_NUM_THREADS to the processes. E.g., calling a C program, quad_mpi:\nexport OMP_NUM_THREADS=2\nmpirun -machinefile .hosts -np 4 -x OMP_NUM_THREADS quad_mpi\nThere are additional details involved in carefully controlling how processes are allocated to nodes, but the default arguments for mpirun should do a reasonable job in many situations.",
    "crumbs": [
      "Parallel C/C++"
    ]
  }
]