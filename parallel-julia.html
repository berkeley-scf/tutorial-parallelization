<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Parallel processing in Julia</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-649e5f0678146f359c225094671654aa.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="assets/styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./parallel-julia.html">Parallel Julia</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="https://statistics.berkeley.edu" class="sidebar-logo-link">
      <img src="./assets/img/logo.svg" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">SCF Parallelization Tutorial</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/berkeley-scf/tutorial-parallelization" title="" class="quarto-navigation-tool px-1" aria-label="GitHub"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./parallel-python.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Parallel Python</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./parallel-R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Parallel R</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./parallel-julia.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Parallel Julia</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./parallel-matlab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Parallel MATLAB</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./parallel-C.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Parallel C/C++</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./license.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">License</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">1 Overview</a></li>
  <li><a href="#threading" id="toc-threading" class="nav-link" data-scroll-target="#threading">2 Threading</a>
  <ul class="collapse">
  <li><a href="#threaded-linear-algebra" id="toc-threaded-linear-algebra" class="nav-link" data-scroll-target="#threaded-linear-algebra">2.1 Threaded linear algebra</a></li>
  <li><a href="#threaded-for-loops" id="toc-threaded-for-loops" class="nav-link" data-scroll-target="#threaded-for-loops">2.2 Threaded for loops</a></li>
  <li><a href="#spawning-tasks-on-threads" id="toc-spawning-tasks-on-threads" class="nav-link" data-scroll-target="#spawning-tasks-on-threads">2.3 Spawning tasks on threads</a></li>
  <li><a href="#controlling-the-number-of-threads" id="toc-controlling-the-number-of-threads" class="nav-link" data-scroll-target="#controlling-the-number-of-threads">2.4 Controlling the number of threads</a></li>
  </ul></li>
  <li><a href="#multi-process-parallelization" id="toc-multi-process-parallelization" class="nav-link" data-scroll-target="#multi-process-parallelization">3 Multi-process parallelization</a>
  <ul class="collapse">
  <li><a href="#parallel-map-operations" id="toc-parallel-map-operations" class="nav-link" data-scroll-target="#parallel-map-operations">3.1 Parallel map operations</a></li>
  <li><a href="#parallel-for-loops" id="toc-parallel-for-loops" class="nav-link" data-scroll-target="#parallel-for-loops">3.2 Parallel for loops</a></li>
  <li><a href="#passing-data-to-the-workers" id="toc-passing-data-to-the-workers" class="nav-link" data-scroll-target="#passing-data-to-the-workers">3.3 Passing data to the workers</a></li>
  <li><a href="#spawning-tasks" id="toc-spawning-tasks" class="nav-link" data-scroll-target="#spawning-tasks">3.4 Spawning tasks</a></li>
  <li><a href="#using-multiple-machines" id="toc-using-multiple-machines" class="nav-link" data-scroll-target="#using-multiple-machines">3.5 Using multiple machines</a></li>
  </ul></li>
  <li><a href="#loops-and-fused-operations" id="toc-loops-and-fused-operations" class="nav-link" data-scroll-target="#loops-and-fused-operations">4 Loops and fused operations</a></li>
  <li><a href="#using-the-gpu---basic-offloading" id="toc-using-the-gpu---basic-offloading" class="nav-link" data-scroll-target="#using-the-gpu---basic-offloading">5 Using the GPU - basic offloading</a>
  <ul class="collapse">
  <li><a href="#matrix-multiplication" id="toc-matrix-multiplication" class="nav-link" data-scroll-target="#matrix-multiplication">5.1 Matrix multiplication</a></li>
  <li><a href="#vectorized-calculations" id="toc-vectorized-calculations" class="nav-link" data-scroll-target="#vectorized-calculations">5.2 Vectorized calculations</a></li>
  </ul></li>
  <li><a href="#using-the-gpu---writing-gpu-kernels" id="toc-using-the-gpu---writing-gpu-kernels" class="nav-link" data-scroll-target="#using-the-gpu---writing-gpu-kernels">6 Using the GPU - writing GPU kernels</a>
  <ul class="collapse">
  <li><a href="#basic-example" id="toc-basic-example" class="nav-link" data-scroll-target="#basic-example">6.1 Basic example</a></li>
  <li><a href="#efficient-memory-access" id="toc-efficient-memory-access" class="nav-link" data-scroll-target="#efficient-memory-access">6.2 Efficient memory access</a></li>
  <li><a href="#using-atomics-for-reduction-operations" id="toc-using-atomics-for-reduction-operations" class="nav-link" data-scroll-target="#using-atomics-for-reduction-operations">6.3 Using atomics for reduction operations</a></li>
  <li><a href="#debugging-kernel-code" id="toc-debugging-kernel-code" class="nav-link" data-scroll-target="#debugging-kernel-code">6.4 Debugging kernel code</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Parallel processing in Julia</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">1 Overview</h2>
<p>Julia provides built-in support for various kinds of parallel processing on one or more machines. This material focuses on some standard approaches that are (mostly) analogous to functionality in Python and R. However there is other functionality available, including the ability to control tasks and sending data between processes in a fine-grained way.</p>
<p>In addition to parallelization, the second to last section discusses some issues related to efficiency with for loops, in particular <em>fused</em> operations. This is not directly related to parallelization but given the focus on loops in this document, it’s useful and interesting to know about.</p>
<p>Finally, the last section discussed offloading computation to the GPU, i.e., massive parallelization on many GPU cores.</p>
</section>
<section id="threading" class="level2">
<h2 class="anchored" data-anchor-id="threading">2 Threading</h2>
<p>Threaded calculations are done in parallel on <a href="./#31-shared-memory">software threads</a>.</p>
<p>Threads share objects in memory with the parent process, which is useful for avoiding copies but raises the danger of a “race condition”, where different threads modify data that other threads are using and cause errors..</p>
<section id="threaded-linear-algebra" class="level3">
<h3 class="anchored" data-anchor-id="threaded-linear-algebra">2.1 Threaded linear algebra</h3>
<p>As with Python and R, Julia uses BLAS, a standard library of basic linear algebra operations (written in Fortran or C), for linear algebra operations. A fast BLAS can greatly speed up linear algebra relative to the default BLAS on a machine. Julia uses a fast, open source, free BLAS library called <em>OpenBLAS</em>. In addition to being fast when used on a single core, the openBLAS library is threaded - if your computer has multiple cores and there are free resources, your linear algebra will use multiple cores</p>
<p>Here’s an example.</p>
<div id="3f43eed1" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">BenchmarkTools</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">LinearAlgebra</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Distributions</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">7000</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="fu">rand</span>(<span class="fu">Uniform</span>(<span class="fl">0</span>,<span class="fl">1</span>), n,n);</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">println</span>(BLAS.<span class="fu">get_num_threads</span>())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>4</code></pre>
</div>
</div>
<div id="77ef3325" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">chol_xtx</span>(x)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> x<span class="op">'*</span>x   <span class="co">## z is positive definite</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    C <span class="op">=</span> <span class="fu">cholesky</span>(z) </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>BLAS.<span class="fu">set_num_threads</span>(<span class="fl">4</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> chol <span class="op">=</span> <span class="fu">chol_xtx</span>(x);  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  9.413 s (5 allocations: 747.68 MiB)</code></pre>
</div>
</div>
<div id="02929c85" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>BLAS.<span class="fu">set_num_threads</span>(<span class="fl">1</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> chol <span class="op">=</span> <span class="fu">chol_xtx</span>(x);  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  11.551 s (5 allocations: 747.68 MiB)</code></pre>
</div>
</div>
<p>We see that using four threads is faster than one, but in this case we don’t get a four-fold speedup.</p>
<section id="number-of-threads" class="level4">
<h4 class="anchored" data-anchor-id="number-of-threads">Number of threads</h4>
<p>By default, Julia will set the number of threads for linear algebra equal to the number of processors on your machine.</p>
<p>As seen above, you can check the number of threads being used with:</p>
<div id="518b4d9c" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>BLAS.<span class="fu">get_num_threads</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>1</code></pre>
</div>
</div>
<p>Other ways to control the number of threads used for linear algebra include:</p>
<ul>
<li>setting the <code>OMP_NUM_THREADS</code> environment variable in the shell before starting Julia, and</li>
<li>using <code>BLAS.set_num_threads(n)</code>.</li>
</ul>
</section>
</section>
<section id="threaded-for-loops" class="level3">
<h3 class="anchored" data-anchor-id="threaded-for-loops">2.2 Threaded for loops</h3>
<p>In Julia, you can directly set up <a href="https://docs.julialang.org/en/v1/manual/multi-threading">software threads to use for parallel processing</a>.</p>
<p>Here we’ll see some examples of running a for loop in parallel, both acting on a single object and used as a parallel map operation.</p>
<p>Here we can operate on a vector in parallel:</p>
<div id="ba0e27ac" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Base.Threads</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">50000000</span>;</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="fu">rand</span>(n);</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="pp">@threads</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu">length</span>(x)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    x[i] <span class="op">=</span> <span class="fu">exp</span>(x[i]) <span class="op">+</span> <span class="fu">sin</span>(x[i]);</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="cf">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We could also threads to carry out a parallel map operation, implemented as a for loop.</p>
<div id="0b949b76" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">1000</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">test</span>(n)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="fu">rand</span>(<span class="fu">Uniform</span>(<span class="fl">0</span>,<span class="fl">1</span>), n,n)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> x<span class="op">'*</span>x </span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    C <span class="op">=</span> <span class="fu">cholesky</span>(z)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(C.U[<span class="fl">1</span>,<span class="fl">1</span>])</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> <span class="fu">zeros</span>(<span class="fl">12</span>)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="pp">@threads</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">12</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    a[i] <span class="op">=</span> <span class="fu">test</span>(n)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="cf">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="spawning-tasks-on-threads" class="level3">
<h3 class="anchored" data-anchor-id="spawning-tasks-on-threads">2.3 Spawning tasks on threads</h3>
<p>You can also create (aka ‘spawn’) individual tasks on threads, with the tasks running in parallel.</p>
<p>Let’s see an example (taken from <a href="http://ferestrepoca.github.io/paradigmas-de-programacion/paralela/tutoriales/julia/notebooks/parallelProgrammingApplications.html">here</a> of sorting a vector in parallel, by sorting subsets of the vector in separate threads.</p>
<div id="f34282e3" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> <span class="bu">Base.Threads.@spawn</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># sort the elements of `v` in place, from indices `lo` to `hi` inclusive</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">psort!</span>(v, lo<span class="op">::</span><span class="dt">Int</span>=<span class="fl">1</span>, hi<span class="op">::</span><span class="dt">Int</span>=<span class="fu">length</span>(v))</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">println</span>(<span class="fu">current_task</span>(), <span class="ch">' '</span>, lo, <span class="ch">' '</span>, hi)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> lo <span class="op">&gt;=</span> hi                       <span class="co"># 1 or 0 elements; nothing to do</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> v</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> hi <span class="op">-</span> lo <span class="op">&lt;</span> <span class="fl">100000</span>               <span class="co"># below some cutoff, run in serial</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        <span class="fu">sort!</span>(<span class="fu">view</span>(v, lo<span class="op">:</span>hi), alg <span class="op">=</span> <span class="cn">MergeSort</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> v</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    mid <span class="op">=</span> (lo<span class="op">+</span>hi)<span class="op">&gt;&gt;&gt;</span><span class="fl">1</span>                 <span class="co"># find the midpoint</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">### Set up parallelization here </span><span class="al">###</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    <span class="co">## Sort two halves in parallel, one in current call and one in a new task</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    <span class="co">## in a separate thread:</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    half <span class="op">=</span> <span class="pp">@spawn</span> <span class="fu">psort!</span>(v, lo, mid)  <span class="co"># task to sort the lower half</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">psort!</span>(v, mid<span class="op">+</span><span class="fl">1</span>, hi)              <span class="co"># sort the upper half in the current call</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">wait</span>(half)                        <span class="co"># wait for the lower half to finish</span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    temp <span class="op">=</span> v[lo<span class="op">:</span>mid]                  <span class="co"># workspace for merging</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>    i, k, j <span class="op">=</span> <span class="fl">1</span>, lo, mid<span class="op">+</span><span class="fl">1</span>            <span class="co"># merge the two sorted sub-arrays</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>    <span class="pp">@inbounds</span> <span class="cf">while</span> k <span class="op">&lt;</span> j <span class="op">&lt;=</span> hi</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> v[j] <span class="op">&lt;</span> temp[i]</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>            v[k] <span class="op">=</span> v[j]</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>            j <span class="op">+=</span> <span class="fl">1</span></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>            v[k] <span class="op">=</span> temp[i]</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>            i <span class="op">+=</span> <span class="fl">1</span></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">end</span></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>        k <span class="op">+=</span> <span class="fl">1</span></span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>    <span class="pp">@inbounds</span> <span class="cf">while</span> k <span class="op">&lt;</span> j</span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>        v[k] <span class="op">=</span> temp[i]</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>        k <span class="op">+=</span> <span class="fl">1</span></span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>        i <span class="op">+=</span> <span class="fl">1</span></span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> v</span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>psort! (generic function with 3 methods)</code></pre>
</div>
</div>
<p>How does this work? Let’s consider an example where we sort a vector of length 250000.</p>
<p>The vector gets split into elements 1:125000 (run in task #1) and 125001:250000 (run in the main call). Then the elements 1:125000 are split into 1:62500 (run in task #2) and 62501:125000 (run in task #1), while the elements 125001:250000 are split into 125001:187500 (run in task #3) and 187501:250000 (run in the main call). No more splitting occurs because vectors of length less than 100000 are run in serial.</p>
<p>Assuming we have at least four threads (including the main process), each of the tasks will run in a separate thread, and all four sorts on the vector subsets will run in parallel.</p>
<div id="2f77dba7" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="fu">rand</span>(<span class="fl">250000</span>);</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">psort!</span>(x);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Task (runnable) @0x00007f91333bad60 1 250000
Task (runnable) @0x00007f91333bad60 125001 250000
Task (runnable) @0x00007f91333bad60 187501 250000
Task (runnable) @0x00007f91352101a0 1 125000
Task (runnable) @0x00007f91352101a0 62501 125000
Task (runnable) @0x00007f9135210330 125001 187500
Task (runnable) @0x00007f91352104c0 1 62500</code></pre>
</div>
</div>
<p>We see that the output from <code>current_task()</code> shows that the task labels correspond with what I stated above.</p>
<p>The number of tasks running in parallel will be at most the number of threads set in the Julia session.</p>
</section>
<section id="controlling-the-number-of-threads" class="level3">
<h3 class="anchored" data-anchor-id="controlling-the-number-of-threads">2.4 Controlling the number of threads</h3>
<p>You can see the number of threads available:</p>
<div id="bc457486" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="bu">Threads</span>.<span class="fu">nthreads</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>1</code></pre>
</div>
</div>
<p>You can control the number of threads used for threading in Julia (apart from linear algebra) either by:</p>
<ul>
<li>setting the <code>JULIA_NUM_THREADS</code> environment variable in the shell before starting Julia, or</li>
<li>starting Julia with the <code>-t</code> (or <code>--threads</code>) flag, e.g.: <code>julia -t 4</code>.</li>
</ul>
<p>Note that we can’t use <code>OMP_NUM_THREADS</code> as the Julia threading is not based on openMP.</p>
</section>
</section>
<section id="multi-process-parallelization" class="level2">
<h2 class="anchored" data-anchor-id="multi-process-parallelization">3 Multi-process parallelization</h2>
<section id="parallel-map-operations" class="level3">
<h3 class="anchored" data-anchor-id="parallel-map-operations">3.1 Parallel map operations</h3>
<p>We can use <code>pmap</code> to run a parallel map operation across multiple Julia processes (on one or more machines). <code>pmap</code> is good for cases where each task takes a non-negligible amount of time, as there is overhead (latency) in starting the tasks.</p>
<p>Here we’ll carry out multiple computationally expensive calculations in the map.</p>
<p>We need to import packages and create the function on each of the worker processes using <code>@everywhere</code>.</p>
<div id="e3974275" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Distributed</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="fu">nprocs</span>() <span class="op">==</span> <span class="fl">1</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">addprocs</span>(<span class="fl">4</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="cf">end</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="fu">nprocs</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING: using Distributed.@spawn in module Main conflicts with an existing identifier.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>5</code></pre>
</div>
</div>
<div id="db9d30f5" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="pp">@everywhere</span> <span class="cf">begin</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="im">using</span> <span class="bu">Distributions</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="im">using</span> <span class="bu">LinearAlgebra</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">function</span> <span class="fu">test</span>(n)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="fu">rand</span>(<span class="fu">Uniform</span>(<span class="fl">0</span>,<span class="fl">1</span>), n,n)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> <span class="fu">transpose</span>(x)<span class="op">*</span>x </span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>        C <span class="op">=</span> <span class="fu">cholesky</span>(z)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> C.U[<span class="fl">2</span>,<span class="fl">3</span>]</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> <span class="fu">pmap</span>(test, <span class="fu">repeat</span>([<span class="fl">5000</span>],<span class="fl">12</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>12-element Vector{Float64}:
 11.46796239567315
 10.98360399939223
 11.855528600147663
 11.809624824425185
 11.271693682820008
 11.191565722035467
 11.603740118960042
 11.540730849990373
 11.362238174937845
 11.789584816924307
 11.194481356957468
 11.393640476610342</code></pre>
</div>
</div>
<p>One can use <a href="./#4-parallelization-strategies">static allocation (prescheduling)</a> with the <code>batch_size</code> argument, thereby assigning that many tasks to each worker to reduce latentcy.</p>
</section>
<section id="parallel-for-loops" class="level3">
<h3 class="anchored" data-anchor-id="parallel-for-loops">3.2 Parallel for loops</h3>
<p>One can execute for loops in parallel across multiple worker processes as follows. This is particularly handy for cases where one uses a reduction operator (e.g., the <code>+</code> here) so that little data needs to be copied back to the main process. (And in this case we don’t copy any data to the workers either.)</p>
<p>Here we’ll sum over a large number of random numbers with chunks done on each of the workers, comparing the time to a basic for loop.</p>
<div id="47409f52" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">forfun</span>(n)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    sum <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span>n</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>        sum <span class="op">+=</span> <span class="fu">rand</span>(<span class="fl">1</span>)[<span class="fl">1</span>]</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(sum)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">pforfun</span>(n)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>   out <span class="op">=</span> <span class="pp">@sync</span> <span class="pp">@distributed</span> (<span class="op">+</span>) <span class="cf">for</span> i <span class="op">=</span> <span class="fl">1</span><span class="op">:</span>n</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>       <span class="fu">rand</span>(<span class="fl">1</span>)[<span class="fl">1</span>]</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>   <span class="cf">end</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>   <span class="cf">return</span>(out)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="fl">50000000</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a><span class="pp">@time</span> <span class="fu">forfun</span>(n);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  3.441998 seconds (50.01 M allocations: 2.981 GiB, 17.53% gc time, 0.59% compilation time)</code></pre>
</div>
</div>
<div id="12c70cd2" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="pp">@time</span> <span class="fu">pforfun</span>(n); </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  2.179350 seconds (498.54 k allocations: 33.226 MiB, 24.30% compilation time)</code></pre>
</div>
</div>
<p>The use of <code>@sync</code> causes the operation to block until the result is available so we can get the correct timing.</p>
<p>Without a reduction operation, one would generally end up passing a lot of data back to the main process, and this could take a lot of time. For such calculations, one would generally be better off using threaded for loops in order to take advantage of shared memory.</p>
<p>We’d have to look into how the random number seed is set on each worker to better understand any issues that might arise from parallel random number generation, but I believe that each worker has a different seed (but note that this does not explicitly ensure that the random number streams on the workers are distinct, as is the case if one uses the L’Ecuyer algorithm).</p>
</section>
<section id="passing-data-to-the-workers" class="level3">
<h3 class="anchored" data-anchor-id="passing-data-to-the-workers">3.3 Passing data to the workers</h3>
<p>With multiple workers, particularly on more than one machine, one generally wants to be careful about having to copy large data objects to each worker, as that could make up a substantial portion of the time involved in the computation.</p>
<p>One can explicitly copy a variable to the workers in an <code>@everywhere</code> block by using Julia’s interpolation syntax:</p>
<div id="f7fda9b7" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="pp">@everywhere</span> <span class="cf">begin</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="op">$</span>x  <span class="co"># copy to workers using interpolation syntax</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">println</span>(<span class="fu">pointer_from_objref</span>(x), <span class="ch">' '</span>, x[<span class="fl">1</span>])  </span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="cf">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Ptr{Nothing} @0x00007f91334cc2b0 3.0383289700841587e-6
      From worker 2:    Ptr{Nothing} @0x00007fb9f003c280 3.0383289700841587e-6
      From worker 3:    Ptr{Nothing} @0x00007f8345d34280 3.0383289700841587e-6
      From worker 4:    Ptr{Nothing} @0x00007f7cc4498280 3.0383289700841587e-6
      From worker 5:    Ptr{Nothing} @0x00007f607f094280 3.0383289700841587e-6</code></pre>
</div>
</div>
<p>We see based on <code>pointer_from_objref</code> that each copy of <code>x</code> is stored at a distinct location in memory, even when processes are on the same machine.</p>
<p>Also note that if one creates a variable within an <code>@everywhere</code> block, that variable is available to all tasks run on the worker, so it is global’ with respect to those tasks. Note the repeated values in the result here.</p>
<div id="3f211fef" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="pp">@everywhere</span> <span class="cf">begin</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="fu">rand</span>(<span class="fl">5</span>)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">function</span> <span class="fu">test</span>(i)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fu">sum</span>(x)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> <span class="fu">pmap</span>(test, <span class="fl">1</span><span class="op">:</span><span class="fl">12</span>, batch_size <span class="op">=</span> <span class="fl">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>12-element Vector{Float64}:
 0.9939036615065234
 1.4005465832334134
 1.7529749131894112
 3.2801398110921394
 0.9939036615065234
 1.4005465832334134
 1.7529749131894112
 3.2801398110921394
 0.9939036615065234
 1.4005465832334134
 1.7529749131894112
 3.2801398110921394</code></pre>
</div>
</div>
<p>If one wants to have multiple processes all work on the same object, without copying it, one can consider using Julia’s <a href="https://docs.julialang.org/en/v1/stdlib/SharedArrays/#SharedArrays.SharedArray">SharedArray</a> (one machine) or <a href="https://juliaparallel.org/DistributedArrays.jl/stable/">DArray from the DistributedArrays package</a> (multiple machines) types, which break up arrays into pieces, with different pieces stored locally on different processes.</p>
</section>
<section id="spawning-tasks" class="level3">
<h3 class="anchored" data-anchor-id="spawning-tasks">3.4 Spawning tasks</h3>
<p>One can use the <code>Distributed.@spawnat</code> macro to run tasks on processes, in a fashion similar to using <code>Threads.@spawn</code>. More details can be found <a href="https://docs.julialang.org/en/v1/stdlib/Distributed/#Distributed.@spawnat">here</a>.</p>
</section>
<section id="using-multiple-machines" class="level3">
<h3 class="anchored" data-anchor-id="using-multiple-machines">3.5 Using multiple machines</h3>
<p>In addition to using processes on one machine, one can use processes across multiple machines. One can either start the processes when you start the main Julia session or you can start them from within the Julia session. In both cases you’ll need to have the ability to ssh to the other machines without entering your password.</p>
<p>To start the processes when starting Julia, create a “machinefile” that lists the names of the machines and the number of worker processes to start on each machine.</p>
<p>Here’s an example machinefile:</p>
<pre><code>arwen.berkeley.edu
arwen.berkeley.edu
gandalf.berkeley.edu
gandalf.berkeley.edu</code></pre>
<p>Note that if you’re using Slurm on a Linux cluster, you could generate that file in the shell from within your Slurm allocation like this:</p>
<pre><code>srun hostname &gt; machines</code></pre>
<p>Then start Julia like this:</p>
<pre><code>julia --machine-file machines</code></pre>
<p>From within Julia, you can add processes like this (first we’ll remove the existing worker processes started using <code>addprocs()</code> previously):</p>
<div id="9884a1da" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rmprocs</span>(<span class="fu">workers</span>())</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="fu">addprocs</span>([(<span class="st">"arwen"</span>, <span class="fl">2</span>), (<span class="st">"gandalf"</span>, <span class="fl">2</span>)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>4-element Vector{Int64}:
 6
 7
 8
 9</code></pre>
</div>
</div>
<p>To check on the number of processes:</p>
<div id="eb59cdb4" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">nprocs</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>5</code></pre>
</div>
</div>
</section>
</section>
<section id="loops-and-fused-operations" class="level2">
<h2 class="anchored" data-anchor-id="loops-and-fused-operations">4 Loops and fused operations</h2>
<p>Consider the following vectorized code that you might run in a variety of languages (e.g., Julia, Python, R).</p>
<pre><code>x = tan(x) + 3*sin(x)</code></pre>
<p>If run as vectorized code, this has downsides. First, it will use additional memory (temporary arrays will be created to store <code>tan(x)</code>, <code>sin(x)</code>, <code>3*sin(x)</code>). Second, multiple for loops will have to get executed when the vectorized code is run, looping over the elements of <code>x</code> to calculate <code>tan(x)</code>, <code>sin(x)</code>, etc. (For example in R or Python/numpy, multiple for loops would get run in the underlying C code.)</p>
<p>Contrast that to running directly as a for loop (e.g., in Julia or in C/C++):</p>
<div id="0012824c" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu">length</span>(x)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>    x[i] <span class="op">=</span> <span class="fu">tan</span>(x[i]) <span class="op">+</span> <span class="fl">3</span><span class="fu">*sin</span>(x[i])</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="cf">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here temporary arrays don’t need to be allocated and there is only a single for loop.</p>
<p>Combining loops is called ‘fusing’ and is an <a href="https://docs.julialang.org/en/v1/manual/performance-tips/#More-dots:-Fuse-vectorized-operations">important optimization that Julia can do</a>. (It’s also a <a href="https://www.tensorflow.org/xla">key optimization done by XLA</a>, a compiler used with JAX and Tensorflow.)</p>
<p>Of course you might ask why use vectorized code at all given that Julia will <a href="https://en.wikipedia.org/wiki/Just-in-time_compilation">JIT compile</a> the for loop above and run it really quickly. That’s true, but reading and writing vectorized code is easier than writing for loops.</p>
<p>Let’s compare the speed of the following approaches. We’ll put everything into functions as generally <a href="https://www.juliabloggers.com/timing-in-julia">recommended when timing Julia code</a> to avoid <a href="https://julialang.org/blog/2022/08/julia-1.8-highlights/#typed_globals">global variables that incur a performance penalty because their type can change</a>.</p>
<p>First, let’s find the time when directly using a for loop, as a baseline.</p>
<div id="15ca292c" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">50000000</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="fu">Array</span><span class="dt">{Float64}</span>(<span class="cn">undef</span>, n);</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="fu">rand</span>(n);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="07b3a060" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">direct_for_loop_calc</span>(x, y)</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu">length</span>(x)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>        y[i] <span class="op">=</span> <span class="fu">exp</span>(x[i]) <span class="op">+</span> <span class="fu">sin</span>(x[i])</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">BenchmarkTools</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="pp">@benchmark</span> <span class="fu">direct_for_loop_calc</span>(x, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<div class="ansi-escaped-output">
<pre>BenchmarkTools.Trial: 8 samples with 1 evaluation.
 Range <span class="ansi-bright-black-fg">(</span><span class="ansi-cyan-fg ansi-bold">min</span> … <span class="ansi-magenta-fg">max</span><span class="ansi-bright-black-fg">):  </span><span class="ansi-cyan-fg ansi-bold">646.281 ms</span> … <span class="ansi-magenta-fg">693.152 ms</span>  <span class="ansi-bright-black-fg">┊</span> GC <span class="ansi-bright-black-fg">(</span>min … max<span class="ansi-bright-black-fg">): </span>0.00% … 0.00%
 Time  <span class="ansi-bright-black-fg">(</span><span class="ansi-blue-fg ansi-bold">median</span><span class="ansi-bright-black-fg">):     </span><span class="ansi-blue-fg ansi-bold">655.665 ms               </span><span class="ansi-bright-black-fg">┊</span> GC <span class="ansi-bright-black-fg">(</span>median<span class="ansi-bright-black-fg">):    </span>0.00%
 Time  <span class="ansi-bright-black-fg">(</span><span class="ansi-green-fg ansi-bold">mean</span> ± <span class="ansi-green-fg">σ</span><span class="ansi-bright-black-fg">):   </span><span class="ansi-green-fg ansi-bold">664.190 ms</span> ± <span class="ansi-green-fg"> 17.358 ms</span>  <span class="ansi-bright-black-fg">┊</span> GC <span class="ansi-bright-black-fg">(</span>mean ± σ<span class="ansi-bright-black-fg">):  </span>0.00% ± 0.00%
  █     ██ █<span class="ansi-blue-fg"> </span>   █        <span class="ansi-green-fg"> </span>                  █ █               █  
  █▁▁▁▁▁██▁█<span class="ansi-blue-fg">▁</span>▁▁▁█▁▁▁▁▁▁▁▁<span class="ansi-green-fg">▁</span>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁
  646 ms<span class="ansi-bright-black-fg">           Histogram: frequency by time</span>          693 ms <span class="ansi-bold">&lt;</span>
 Memory estimate<span class="ansi-bright-black-fg">: </span><span class="ansi-yellow-fg">0 bytes</span>, allocs estimate<span class="ansi-bright-black-fg">: </span><span class="ansi-yellow-fg">0</span>.</pre>
</div>
</div>
</div>
<p>Notice the lack of additional memory use.</p>
<p>Now let’s try a basic vectorized calculation (for which we need the various periods to get vectorization), without fusing. We’ll reassign the result to the allocated <code>y</code> vector for comparability to the for loop implementation above.</p>
<div id="29719d35" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">basic_vectorized_calc</span>(x, y)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>     y <span class="op">.=</span> <span class="fu">exp</span>.(x) <span class="op">+</span> <span class="fl">3</span> <span class="op">*</span> <span class="fu">sin</span>.(x)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">BenchmarkTools</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="pp">@benchmark</span> <span class="fu">basic_vectorized_calc</span>(x, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<div class="ansi-escaped-output">
<pre>BenchmarkTools.Trial: 4 samples with 1 evaluation.
 Range <span class="ansi-bright-black-fg">(</span><span class="ansi-cyan-fg ansi-bold">min</span> … <span class="ansi-magenta-fg">max</span><span class="ansi-bright-black-fg">):  </span><span class="ansi-cyan-fg ansi-bold">1.369 s</span> … <span class="ansi-magenta-fg">   1.631 s</span>  <span class="ansi-bright-black-fg">┊</span> GC <span class="ansi-bright-black-fg">(</span>min … max<span class="ansi-bright-black-fg">): </span> 4.95% … 16.17%
 Time  <span class="ansi-bright-black-fg">(</span><span class="ansi-blue-fg ansi-bold">median</span><span class="ansi-bright-black-fg">):     </span><span class="ansi-blue-fg ansi-bold">1.485 s               </span><span class="ansi-bright-black-fg">┊</span> GC <span class="ansi-bright-black-fg">(</span>median<span class="ansi-bright-black-fg">):    </span>10.83%
 Time  <span class="ansi-bright-black-fg">(</span><span class="ansi-green-fg ansi-bold">mean</span> ± <span class="ansi-green-fg">σ</span><span class="ansi-bright-black-fg">):   </span><span class="ansi-green-fg ansi-bold">1.493 s</span> ± <span class="ansi-green-fg">121.252 ms</span>  <span class="ansi-bright-black-fg">┊</span> GC <span class="ansi-bright-black-fg">(</span>mean ± σ<span class="ansi-bright-black-fg">):  </span>10.78% ±  6.77%
  █         <span class="ansi-blue-fg">█</span>                <span class="ansi-green-fg"> </span>             █               █  
  █▁▁▁▁▁▁▁▁▁<span class="ansi-blue-fg">█</span>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁<span class="ansi-green-fg">▁</span>▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁
  1.37 s<span class="ansi-bright-black-fg">         Histogram: frequency by time</span>         1.63 s <span class="ansi-bold">&lt;</span>
 Memory estimate<span class="ansi-bright-black-fg">: </span><span class="ansi-yellow-fg">1.49 GiB</span>, allocs estimate<span class="ansi-bright-black-fg">: </span><span class="ansi-yellow-fg">8</span>.</pre>
</div>
</div>
</div>
<p>The original <code>x</code> array is 400 MB; notice the additional memory allocation and that this takes almost twice as long as the original for loop.</p>
<p>Here’s a fused version of the vectorized calculation, where the <code>@.</code> causes the loops to be fused.</p>
<div id="66fe86bc" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">fused_vectorized_calc</span>(x, y)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    y <span class="op">.=</span> @. <span class="fu">tan</span>(x) <span class="op">+</span> <span class="fl">3</span> <span class="op">*</span> <span class="fu">sin</span>(x)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="pp">@benchmark</span> <span class="fu">fused_vectorized_calc</span>(x, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<div class="ansi-escaped-output">
<pre>BenchmarkTools.Trial: 5 samples with 1 evaluation.
 Range <span class="ansi-bright-black-fg">(</span><span class="ansi-cyan-fg ansi-bold">min</span> … <span class="ansi-magenta-fg">max</span><span class="ansi-bright-black-fg">):  </span><span class="ansi-cyan-fg ansi-bold">1.088 s</span> … <span class="ansi-magenta-fg">  1.144 s</span>  <span class="ansi-bright-black-fg">┊</span> GC <span class="ansi-bright-black-fg">(</span>min … max<span class="ansi-bright-black-fg">): </span>0.00% … 0.00%
 Time  <span class="ansi-bright-black-fg">(</span><span class="ansi-blue-fg ansi-bold">median</span><span class="ansi-bright-black-fg">):     </span><span class="ansi-blue-fg ansi-bold">1.105 s              </span><span class="ansi-bright-black-fg">┊</span> GC <span class="ansi-bright-black-fg">(</span>median<span class="ansi-bright-black-fg">):    </span>0.00%
 Time  <span class="ansi-bright-black-fg">(</span><span class="ansi-green-fg ansi-bold">mean</span> ± <span class="ansi-green-fg">σ</span><span class="ansi-bright-black-fg">):   </span><span class="ansi-green-fg ansi-bold">1.112 s</span> ± <span class="ansi-green-fg">25.876 ms</span>  <span class="ansi-bright-black-fg">┊</span> GC <span class="ansi-bright-black-fg">(</span>mean ± σ<span class="ansi-bright-black-fg">):  </span>0.00% ± 0.00%
  █<span class="ansi-blue-fg">█</span>               █       <span class="ansi-green-fg"> </span>                      █       █  
  █<span class="ansi-blue-fg">█</span>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁<span class="ansi-green-fg">▁</span>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁█ ▁
  1.09 s<span class="ansi-bright-black-fg">         Histogram: frequency by time</span>        1.14 s <span class="ansi-bold">&lt;</span>
 Memory estimate<span class="ansi-bright-black-fg">: </span><span class="ansi-yellow-fg">0 bytes</span>, allocs estimate<span class="ansi-bright-black-fg">: </span><span class="ansi-yellow-fg">0</span>.</pre>
</div>
</div>
</div>
<p>We see that the time and (lack of) memory allocation are essentially the same as the original basic for loop.</p>
<p>Finally one can achieve the same fusion by having the function just compute scalar quantities and then using the vectorized version of the function (by using <code>scalar_calc.()</code> instead of <code>scalar_calc()</code>), which also does the fusion.</p>
<div id="12dc1669" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">scalar_calc</span>(x)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(<span class="fu">tan</span>(x) <span class="op">+</span> <span class="fl">3</span> <span class="op">*</span> <span class="fu">sin</span>(x))</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="pp">@benchmark</span> y <span class="op">.=</span> <span class="fu">scalar_calc</span>.(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<div class="ansi-escaped-output">
<pre>BenchmarkTools.Trial: 5 samples with 1 evaluation.
 Range <span class="ansi-bright-black-fg">(</span><span class="ansi-cyan-fg ansi-bold">min</span> … <span class="ansi-magenta-fg">max</span><span class="ansi-bright-black-fg">):  </span><span class="ansi-cyan-fg ansi-bold">1.040 s</span> … <span class="ansi-magenta-fg">  1.081 s</span>  <span class="ansi-bright-black-fg">┊</span> GC <span class="ansi-bright-black-fg">(</span>min … max<span class="ansi-bright-black-fg">): </span>0.00% … 0.00%
 Time  <span class="ansi-bright-black-fg">(</span><span class="ansi-blue-fg ansi-bold">median</span><span class="ansi-bright-black-fg">):     </span><span class="ansi-blue-fg ansi-bold">1.065 s              </span><span class="ansi-bright-black-fg">┊</span> GC <span class="ansi-bright-black-fg">(</span>median<span class="ansi-bright-black-fg">):    </span>0.00%
 Time  <span class="ansi-bright-black-fg">(</span><span class="ansi-green-fg ansi-bold">mean</span> ± <span class="ansi-green-fg">σ</span><span class="ansi-bright-black-fg">):   </span><span class="ansi-green-fg ansi-bold">1.063 s</span> ± <span class="ansi-green-fg">15.025 ms</span>  <span class="ansi-bright-black-fg">┊</span> GC <span class="ansi-bright-black-fg">(</span>mean ± σ<span class="ansi-bright-black-fg">):  </span>0.00% ± 0.00%
  █                          █<span class="ansi-blue-fg"> </span>   <span class="ansi-green-fg"> </span> █      █              █  
  █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█<span class="ansi-blue-fg">▁</span>▁▁▁<span class="ansi-green-fg">▁</span>▁█▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁
  1.04 s<span class="ansi-bright-black-fg">         Histogram: frequency by time</span>        1.08 s <span class="ansi-bold">&lt;</span>
 Memory estimate<span class="ansi-bright-black-fg">: </span><span class="ansi-yellow-fg">16 bytes</span>, allocs estimate<span class="ansi-bright-black-fg">: </span><span class="ansi-yellow-fg">1</span>.</pre>
</div>
</div>
</div>
</section>
<section id="using-the-gpu---basic-offloading" class="level2">
<h2 class="anchored" data-anchor-id="using-the-gpu---basic-offloading">5 Using the GPU - basic offloading</h2>
<p>We can use <code>CUDA.jl</code> to offload computations to the GPU. Here we’ll explore matrix multiplication and vectorized calculations. For this, Julia will take care, behind the scenes, of converting our Julia code to code that can run on the GPU.</p>
<p>There are a couple key things to remember about using a GPU:</p>
<ul>
<li>The GPU memory is separate from CPU memory, and transferring data from the CPU to GPU (or back) is often more costly than doing the computation on the GPU.
<ul>
<li>If possible, generate the data on the GPU or keep the data on the GPU when carrying out a sequence of operations.</li>
</ul></li>
<li>By default GPU calculations are often doing using 32-bit (4-byte) floating point numbers rather than the standard of 64-bit (8-byte) when on the CPU.
<ul>
<li>This can affect speed comparisons between CPU and GPU.</li>
</ul></li>
</ul>
<p>Note that for this section, I’m pasting in the output when running the code separately on a machine with a GPU because this document is generated on a machine without a GPU.</p>
<section id="matrix-multiplication" class="level3">
<h3 class="anchored" data-anchor-id="matrix-multiplication">5.1 Matrix multiplication</h3>
<p>Let’s first consider basic matrix multiplication. In this case since we generate the matrices on the CPU, they are 64-bit.</p>
<div id="4c219f71" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">BenchmarkTools</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">CUDA</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">LinearAlgebra</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">matmult</span>(x, y)</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> x <span class="op">*</span> y</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> z</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">7000</span></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="fu">randn</span>(n, n);</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="fu">randn</span>(n, n);</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>x_gpu <span class="op">=</span> <span class="fu">CuArray</span>(x);</span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>y_gpu <span class="op">=</span> <span class="fu">CuArray</span>(y);</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a><span class="co">## These use 64-bit numbers:</span></span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a><span class="fu">typeof</span>(x)</span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Matrix{Float64} (alias for Array{Float64, 2})</span></span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a><span class="fu">typeof</span>(x_gpu)</span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a><span class="co"># CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}</span></span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a><span class="bu">LinearAlgebra</span>.BLAS.<span class="fu">set_num_threads</span>(<span class="fl">1</span>);</span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a><span class="pp">@benchmark</span> z <span class="op">=</span> <span class="fu">matmult</span>(x, y) </span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a><span class="co"># BenchmarkTools.Trial: 1 sample with 1 evaluation.</span></span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a><span class="co">#  Single result which took 17.271 s (0.00% GC) to evaluate,</span></span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a><span class="co">#  with a memory estimate of 373.84 MiB, over 2 allocations.</span></span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a><span class="pp">@benchmark</span> CUDA.<span class="pp">@sync</span> z_gpu <span class="op">=</span> <span class="fu">matmult</span>(x_gpu, y_gpu)</span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a><span class="co"># BenchmarkTools.Trial: 65 samples with 1 evaluation.</span></span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a><span class="co">#  Range (min … max):  53.172 ms … 90.679 ms  ┊ GC (min … max): 0.00% … 0.00%</span></span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a><span class="co">#  Time  (median):     76.419 ms              ┊ GC (median):    0.00%</span></span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a><span class="co">#  Time  (mean ± σ):   77.404 ms ±  4.092 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Clearly the the GPU calculation is much faster, taking about 75 milliseconds, compared to 17 seconds on the CPU (albeit using a single thread).</p>
<p>Let’s compare that to the time of copying the data to the GPU:</p>
<div id="b9675675" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="pp">@benchmark</span> CUDA.<span class="pp">@sync</span> tmp <span class="op">=</span> <span class="fu">CuArray</span>(x)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="co"># BenchmarkTools.Trial: 59 samples with 1 evaluation.</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="co">#  Range (min … max):  83.766 ms … 137.849 ms  ┊ GC (min … max): 0.00% … 0.00%</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="co">#  Time  (median):     84.684 ms               ┊ GC (median):    0.00%</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="co">#  Time  (mean ± σ):   85.696 ms ±   7.011 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This suggests that the time in copying the data is similar to that for doing the computation.</p>
<p>If we count the time of transferring the data to and from the GPU, that ends up being a substantial part of the time, compared to the 75 ms for simply doing the matrix multiplication.</p>
<div id="b85c5977" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">matmult_with_transfer</span>(x, y)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>    xc <span class="op">=</span> <span class="fu">CuArray</span>(x)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>    yc <span class="op">=</span> <span class="fu">CuArray</span>(y)</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> xc <span class="op">*</span> yc</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fu">Array</span>(z)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a><span class="pp">@benchmark</span> CUDA.<span class="pp">@sync</span> z <span class="op">=</span> <span class="fu">matmult_with_transfer</span>(x, y) </span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a><span class="co"># BenchmarkTools.Trial: 20 samples with 1 evaluation.</span></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a><span class="co">#  Range (min … max):  251.578 ms … 258.017 ms  ┊ GC (min … max): 0.00% … 0.57%</span></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a><span class="co">#  Time  (median):     253.886 ms               ┊ GC (median):    0.00%</span></span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a><span class="co">#  Time  (mean ± σ):   254.228 ms ±   1.708 ms  ┊ GC (mean ± σ):  4.33% ± 6.78%</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As a sidenote, we can force use of 64-bit numbers on the GPU (in this case when generating values on the GPU) like this.</p>
<div id="5fa5a82d" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>x_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(<span class="dt">Float64</span>, n, n);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, let’s consider whether the matrix multiplication is faster using 32-bit numbers.</p>
<div id="4832f4d0" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="fu">randn</span>(<span class="dt">Float32</span>, n, n);</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="fu">randn</span>(<span class="dt">Float32</span>, n, n);</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>x_gpu <span class="op">=</span> <span class="fu">CuArray</span>(x);</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>y_gpu <span class="op">=</span> <span class="fu">CuArray</span>(y);</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="fu">typeof</span>(x_gpu)</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a><span class="pp">@benchmark</span> z <span class="op">=</span> <span class="fu">matmult</span>(x, y) </span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a><span class="co"># BenchmarkTools.Trial: 1 sample with 1 evaluation.</span></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a><span class="co">#  Single result which took 8.671 s (0.00% GC) to evaluate,</span></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a><span class="pp">@benchmark</span> CUDA.<span class="pp">@sync</span> z_gpu <span class="op">=</span> <span class="fu">matmult</span>(x_gpu, y_gpu)</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a><span class="co"># BenchmarkTools.Trial: 91 samples with 1 evaluation.</span></span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a><span class="co">#  Range (min … max):  41.174 ms … 70.491 ms  ┊ GC (min … max): 0.00% … 0.00%</span></span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a><span class="co">#  Time  (median):     54.420 ms              ┊ GC (median):    0.00%</span></span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a><span class="co">#  Time  (mean ± σ):   55.363 ms ±  2.912 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>So that’s faster, though I’m not sure why the CPU implementation is about twice as fast (which makes sense in that it is working with numbers taking up half as much space) while the GPU implementation does not achieve that speedup (54 ms. with 32-bit compared to 75 ms. with 64-bit).</p>
</section>
<section id="vectorized-calculations" class="level3">
<h3 class="anchored" data-anchor-id="vectorized-calculations">5.2 Vectorized calculations</h3>
<p>Here we’ll consider using the GPU for vectorized calculations, noting that <a href="./#4-loops-and-fused-operations">earlier</a> we talked about using <code>.</code> to vectorize and <code>@</code> to fuse loops in the context of CPU-based calculations.</p>
<div id="396bba84" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Scalar function to do something.</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">myfun</span>(x)</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> <span class="fu">tan</span>(x) <span class="op">+</span> <span class="fl">3</span> <span class="op">*</span> <span class="fu">sin</span>(x)</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Vectorized version that modifies `y` in place.</span></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">myfun_vec</span>(x, y)</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>    y <span class="op">.=</span> <span class="fu">myfun</span>.(x)</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> </span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">250000000</span>; </span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="fu">Vector</span><span class="dt">{Float64}</span>(<span class="cn">undef</span>, n);</span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="fu">rand</span>(n);</span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a>x_gpu <span class="op">=</span> <span class="fu">CuArray</span>(x);</span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a>y_gpu <span class="op">=</span> <span class="fu">CuArray</span>(y);</span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-20"><a href="#cb49-20" aria-hidden="true" tabindex="-1"></a><span class="pp">@benchmark</span> <span class="fu">myfun_vec</span>(x, y)   <span class="co"># 3.5 sec.</span></span>
<span id="cb49-21"><a href="#cb49-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-22"><a href="#cb49-22" aria-hidden="true" tabindex="-1"></a><span class="pp">@benchmark</span> CUDA.<span class="pp">@sync</span> <span class="fu">myfun_vec</span>(x_gpu, y_gpu)  <span class="co"># 6 ms.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here we have a massive 500x speedup of 6 ms. compared to 3.5 seconds.</p>
<p>Of course, as in the matrix multiplication example above, if you need to copy the data to and from the GPU, that will add substantial time.</p>
</section>
</section>
<section id="using-the-gpu---writing-gpu-kernels" class="level2">
<h2 class="anchored" data-anchor-id="using-the-gpu---writing-gpu-kernels">6 Using the GPU - writing GPU kernels</h2>
<p>Next we’ll explore <a href="https://cuda.juliagpu.org/stable/development/kernel">writing our own GPU kernels</a>. Kernels are functions that encode the core computational operations that are executed in parallel.</p>
<p>In other languages, the basic mode of operation with a GPU when you are writing your own GPU code is to write a kernel using CUDA (basically C) code and then call the kernel in parallel via C, R, or Python code. In Julia, we can write the kernel using Julia syntax (though many operations (particularly non-numerical ones) will not run on the GPU…).</p>
<section id="basic-example" class="level3">
<h3 class="anchored" data-anchor-id="basic-example">6.1 Basic example</h3>
<p>Here’s a basic example in which we’ll do a calculation in place. We run 1000 scalar calculations using 1000 threads.</p>
<p>We use <code>@cuda</code> to compile and run the kernel.</p>
<div id="330672c3" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">my_kernel</span>(x)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>  idx <span class="op">=</span> <span class="fu">threadIdx</span>().x;   <span class="co"># What thread am I?</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> idx <span class="op">&lt;=</span> <span class="fu">length</span>(x)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>    x[idx] <span class="op">=</span> <span class="fu">tan</span>(x[idx]) <span class="op">+</span> <span class="fl">3</span><span class="fu">*sin</span>(x[idx]);</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">end</span></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">1000</span>;</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>x_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(n);</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a><span class="fu">Array</span>(x_gpu)[n]</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a><span class="co"># -1.5321726f0</span></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a><span class="pp">@cuda</span> threads<span class="op">=</span>n <span class="fu">my_kernel</span>(x_gpu);</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a><span class="fu">Array</span>(x_gpu)[n]   <span class="co"># Check the computation was done by checking last element.</span></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a><span class="co"># -28.875708f0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>There are limits on the number of threads we can use.</p>
<div id="8ee36aa2" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">2000</span>;</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>x_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(n);</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="pp">@cuda</span> threads<span class="op">=</span>n <span class="fu">my_kernel</span>(x_gpu);</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="co"># ERROR: Number of threads in x-dimension exceeds device limit (2000 &gt; 1024).</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="multiple-blocks" class="level4">
<h4 class="anchored" data-anchor-id="multiple-blocks">6.1.1 Multiple blocks</h4>
<p>We need to use at least as many threads as computations, and in addition to only being able to use 1024 threads in the x dimension, we can have at most 1024 threads in a block on the A100 GPU we’re using. So we’ll need multiple blocks.</p>
<div id="279617a1" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">my_kernel</span>(x)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>  i <span class="op">=</span> <span class="fu">threadIdx</span>().x;  <span class="co"># What thread am I within the block?</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>  j <span class="op">=</span> <span class="fu">blockIdx</span>().x;   <span class="co"># What block am I in?</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>  idx <span class="op">=</span>  (j<span class="op">-</span><span class="fl">1</span>)<span class="fu">*blockDim</span>().x <span class="op">+</span> i;</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> idx <span class="op">&lt;=</span> <span class="fu">length</span>(x)</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>    x[idx] <span class="op">=</span> <span class="fu">tan</span>(x[idx]) <span class="op">+</span> <span class="fl">3</span><span class="fu">*sin</span>(x[idx]);</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">end</span></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span></span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">2000</span>;</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a>nthreads <span class="op">=</span> <span class="fl">1024</span>;</span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a>x_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(n);</span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a>initial <span class="op">=</span> <span class="fu">Array</span>(x_gpu)[n]</span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a>nblocks <span class="op">=</span> <span class="fu">Int</span>(<span class="fu">ceil</span>(n<span class="op">/</span>nthreads));</span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a><span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks <span class="fu">my_kernel</span>(x_gpu);</span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a>(initial, <span class="fu">Array</span>(x_gpu)[n])  <span class="co"># Check that calculation was done.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s do a smaller test run in which we can check on the thread and block indexing.</p>
<div id="df421097" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">my_kernel_print</span>(x)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>  i <span class="op">=</span> <span class="fu">threadIdx</span>().x;  <span class="co"># What thread am I within the block?</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>  j <span class="op">=</span> <span class="fu">blockIdx</span>().x;   <span class="co"># What block am I in?</span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>  idx <span class="op">=</span>  (j<span class="op">-</span><span class="fl">1</span>)<span class="fu">*blockDim</span>().x <span class="op">+</span> i;</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> idx <span class="op">&lt;=</span> <span class="fu">length</span>(x)</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>    x[idx] <span class="op">=</span> <span class="fu">tan</span>(x[idx]) <span class="op">+</span> <span class="fl">3</span><span class="fu">*sin</span>(x[idx]);</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>    <span class="pp">@cuprintln</span> idx, i, j, <span class="fu">blockDim</span>().x, <span class="fu">blockDim</span>().y;</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">end</span></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">200</span>;</span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>x_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(n);</span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a>nthreads <span class="op">=</span> <span class="fl">100</span>;</span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a>nblocks <span class="op">=</span> <span class="fu">Int</span>(<span class="fu">ceil</span>(n<span class="op">/</span>nthreads));</span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a><span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks <span class="fu">my_kernel_print</span>(x_gpu);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>When we run this, notice the output seems to be grouped based on warps of 32 threads (apart from the last set since <code>n=200</code> is not a multiple of 32).</p>
</section>
<section id="larger-computations" class="level4">
<h4 class="anchored" data-anchor-id="larger-computations">6.1.2 Larger computations</h4>
<p>In many cases we’ll have more tasks than the total number of GPU cores. As long as we don’t exceed the maximum size of a block or grid, we can just ask for as many threads as we have tasks and rely on the GPU to manage assigning the tasks to the GPU cores.</p>
<p>We’d want to check that the number/dimension of the block here does not exceed the maximum block size. I didn’t do that, but it ran, so it must have been ok!</p>
<p>Here we’ll run the computation we ran earlier when we did not write our own kernel and just relied on Julia to offload to the GPU behind the scene.</p>
<div id="801fafa0" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">250000000</span>;</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>x_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(n);</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>nthreads <span class="op">=</span> <span class="fl">1024</span>;</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>nblocks <span class="op">=</span> <span class="fu">Int</span>(<span class="fu">ceil</span>(n<span class="op">/</span>nthreads));</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="fu">Array</span>(x_gpu)[n]</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Run it once to flush out any compilation/transformation time.</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>y_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(<span class="fl">5</span>);</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>CUDA.<span class="pp">@sync</span> <span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks <span class="fu">my_kernel</span>(y_gpu);</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>CUDA.<span class="pp">@time</span> CUDA.<span class="pp">@sync</span> <span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks <span class="fu">my_kernel</span>(x_gpu);</span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.002003 seconds (45 CPU allocations: 2.719 KiB)</span></span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a><span class="fu">Array</span>(x_gpu)[n]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The 2.0 ms is reasonably comparable to the 3.7 ms when we just had Julia run the <a href="./#vectorized-example">vectorized computation on the GPU</a> (from the last time we ran it). That used 64-bit floats. When I reran the code above using 64-bit floats, the time was 5.2 ms.</p>
</section>
</section>
<section id="efficient-memory-access" class="level3">
<h3 class="anchored" data-anchor-id="efficient-memory-access">6.2 Efficient memory access</h3>
<p>We’ll explore two final topics related to efficiently accessing data in memory: first accessing global GPU memory efficiently and second making use of shared GPU memory.</p>
<section id="coalesced-access-to-global-memory" class="level4">
<h4 class="anchored" data-anchor-id="coalesced-access-to-global-memory">6.2.1 Coalesced access to global memory</h4>
<p>If adjacent threads in a block access adjacent memory locations, a chunk of data can be obtained in a single access to global memory.</p>
<p>We’ll implement element-wise summing of two matrices. Obviously one can just do this directly with <code>CuArray</code>s in Julia, but if we implement it ourselves, it illustrates that reading a matrix by column is much more efficient than reading by row. Here a thread block either handles part of a column (good) or part of a row (bad). The x-dimension of the blocks in the grid then handles multiple thread blocks within each column (or row; bad) and the y-dimension of the blocks in the grid handles the different columns (or rows; bad).</p>
<div id="d5363c59" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">10000</span>;</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>X_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(n,n);</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>Y_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(n,n);</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>out_gpu <span class="op">=</span> CUDA.<span class="fu">zeros</span>(n,n);</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>X_gpu_small <span class="op">=</span> CUDA.<span class="fu">randn</span>(<span class="fl">5</span>,<span class="fl">5</span>);</span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>Y_gpu_small <span class="op">=</span> CUDA.<span class="fu">randn</span>(<span class="fl">5</span>,<span class="fl">5</span>);</span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>out_gpu_small <span class="op">=</span> CUDA.<span class="fu">zeros</span>(<span class="fl">5</span>,<span class="fl">5</span>);</span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Good: Adjacent threads process elements in a column.</span></span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">kernel_sum_bycol!</span>(X, Y, output)</span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a>    row_idx <span class="op">=</span> <span class="fu">threadIdx</span>().x <span class="op">+</span> (<span class="fu">blockIdx</span>().x <span class="op">-</span> <span class="fl">1</span>)<span class="fu">*blockDim</span>().x;</span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a>    col_idx <span class="op">=</span> <span class="fu">blockIdx</span>().y;</span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> row_idx <span class="op">&lt;=</span> <span class="fu">size</span>(X, <span class="fl">1</span>) <span class="op">&amp;&amp;</span> col_idx <span class="op">&lt;=</span> <span class="fu">size</span>(Y, <span class="fl">2</span>)</span>
<span id="cb55-16"><a href="#cb55-16" aria-hidden="true" tabindex="-1"></a>        output[row_idx, col_idx] <span class="op">=</span> X[row_idx, col_idx] <span class="op">+</span> Y[row_idx, col_idx]</span>
<span id="cb55-17"><a href="#cb55-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb55-18"><a href="#cb55-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="cn">nothing</span></span>
<span id="cb55-19"><a href="#cb55-19" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb55-20"><a href="#cb55-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-21"><a href="#cb55-21" aria-hidden="true" tabindex="-1"></a>nthreads <span class="op">=</span> <span class="fl">1024</span>;</span>
<span id="cb55-22"><a href="#cb55-22" aria-hidden="true" tabindex="-1"></a><span class="co"># x dim of grid is number of thread blocks in a column.</span></span>
<span id="cb55-23"><a href="#cb55-23" aria-hidden="true" tabindex="-1"></a><span class="co"># y dim of grid is number of columns.</span></span>
<span id="cb55-24"><a href="#cb55-24" aria-hidden="true" tabindex="-1"></a>nblocks <span class="op">=</span> (<span class="fu">Int</span>(<span class="fu">ceil</span>(n<span class="op">/</span>nthreads)), n);</span>
<span id="cb55-25"><a href="#cb55-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-26"><a href="#cb55-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-27"><a href="#cb55-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Flush out any compilation time.</span></span>
<span id="cb55-28"><a href="#cb55-28" aria-hidden="true" tabindex="-1"></a>CUDA.<span class="pp">@sync</span> <span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks <span class="fu">kernel_sum_bycol!</span>(X_gpu_small, Y_gpu_small, out_gpu_small);</span>
<span id="cb55-29"><a href="#cb55-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-30"><a href="#cb55-30" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> CUDA.<span class="pp">@sync</span> <span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks <span class="fu">kernel_sum_bycol!</span>(X_gpu, Y_gpu, out_gpu);</span>
<span id="cb55-31"><a href="#cb55-31" aria-hidden="true" tabindex="-1"></a><span class="co"># 2.153 ms (47 allocations: 1.30 KiB)</span></span>
<span id="cb55-32"><a href="#cb55-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-33"><a href="#cb55-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Bad: Adjacent threads process elements in a row.</span></span>
<span id="cb55-34"><a href="#cb55-34" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">kernel_sum_byrow!</span>(X, Y, output)</span>
<span id="cb55-35"><a href="#cb55-35" aria-hidden="true" tabindex="-1"></a>    row_idx <span class="op">=</span> <span class="fu">blockIdx</span>().y;</span>
<span id="cb55-36"><a href="#cb55-36" aria-hidden="true" tabindex="-1"></a>    col_idx <span class="op">=</span> <span class="fu">threadIdx</span>().x <span class="op">+</span> (<span class="fu">blockIdx</span>().x <span class="op">-</span> <span class="fl">1</span>)<span class="fu">*blockDim</span>().x;</span>
<span id="cb55-37"><a href="#cb55-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb55-38"><a href="#cb55-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> row_idx <span class="op">&lt;=</span> <span class="fu">size</span>(X, <span class="fl">1</span>) <span class="op">&amp;&amp;</span> col_idx <span class="op">&lt;=</span> <span class="fu">size</span>(Y, <span class="fl">2</span>)</span>
<span id="cb55-39"><a href="#cb55-39" aria-hidden="true" tabindex="-1"></a>        output[row_idx, col_idx] <span class="op">=</span> X[row_idx, col_idx] <span class="op">+</span> Y[row_idx, col_idx]</span>
<span id="cb55-40"><a href="#cb55-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb55-41"><a href="#cb55-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="cn">nothing</span></span>
<span id="cb55-42"><a href="#cb55-42" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb55-43"><a href="#cb55-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-44"><a href="#cb55-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-45"><a href="#cb55-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Flush out any compilation time.</span></span>
<span id="cb55-46"><a href="#cb55-46" aria-hidden="true" tabindex="-1"></a>CUDA.<span class="pp">@sync</span> <span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks <span class="fu">kernel_sum_byrow!</span>(X_gpu_small, Y_gpu_small, out_gpu_small);</span>
<span id="cb55-47"><a href="#cb55-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-48"><a href="#cb55-48" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> CUDA.<span class="pp">@sync</span> <span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks <span class="fu">kernel_sum_byrow!</span>(X_gpu, Y_gpu, out_gpu);</span>
<span id="cb55-49"><a href="#cb55-49" aria-hidden="true" tabindex="-1"></a><span class="co"># 10.500 ms (47 allocations: 1.30 KiB)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="using-shared-memory" class="level4">
<h4 class="anchored" data-anchor-id="using-shared-memory">6.2.2 Using shared memory</h4>
<p>Accessing global GPU memory is much slower than doing computation on the GPU. So we’d like to avoid repeated access to global memory (e.g., a bad scenario would be a ratio of one arithmetic calculation per retrieval from global memory). One strategy is for multiple threads in a block to cooperate to load data from global memory into shared memory accessible by all the threads in the block. The computation can then be done on the data in shared memory.</p>
<p>Here’s a simplified example that shows how to load the data into shared memory. There’s no actual computation coded here, but one could imagine that each thread would then each do a computation that uses the entire chunk of data in shared memory.</p>
<div id="9ea57a60" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">kernel_reader_bycol</span>(x<span class="op">::</span><span class="dt">CuDeviceArray{T}</span>) <span class="kw">where</span> T</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>  i <span class="op">=</span> <span class="fu">threadIdx</span>().x;  <span class="co"># What thread am I within the block?</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>  j <span class="op">=</span> <span class="fu">blockIdx</span>().x;   <span class="co"># What block am I in?</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>  idx <span class="op">=</span>  (j<span class="op">-</span><span class="fl">1</span>)<span class="fu">*blockDim</span>().x <span class="op">+</span> i;</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>  dims <span class="op">=</span> <span class="fu">size</span>(x);</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Setup shared memory for the subset of data.</span></span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>  shared_data <span class="op">=</span> <span class="fu">CuDynamicSharedArray</span>(T, (<span class="fu">blockDim</span>().x, dims[<span class="fl">2</span>]));</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> chunk_start <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fu">blockDim</span>().x<span class="op">:</span>dims[<span class="fl">1</span>]</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>    chunk_size <span class="op">=</span> <span class="fu">min</span>(<span class="fu">blockDim</span>().x, dims[<span class="fl">1</span>] <span class="op">-</span> chunk_start <span class="op">+</span> <span class="fl">1</span>);</span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">## Transfer a chunk of rows in parallel, one row per thread.</span></span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;=</span> chunk_size</span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> col <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span>dims[<span class="fl">2</span>]</span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a>        shared_data[i, col] <span class="op">=</span> x[chunk_start <span class="op">+</span> i <span class="op">-</span> <span class="fl">1</span>, col];</span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a>      <span class="cf">end</span></span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sync_threads</span>()</span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># At this point we'd insert code to do the actual computation, based on `idx`.</span></span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Each thread now has the opportunity to compute on all the data in the chunk in</span></span>
<span id="cb56-22"><a href="#cb56-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># `shared_data`.</span></span>
<span id="cb56-23"><a href="#cb56-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb56-24"><a href="#cb56-24" aria-hidden="true" tabindex="-1"></a>  <span class="cf">end</span></span>
<span id="cb56-25"><a href="#cb56-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-26"><a href="#cb56-26" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span></span>
<span id="cb56-27"><a href="#cb56-27" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb56-28"><a href="#cb56-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-29"><a href="#cb56-29" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">10000000</span>;</span>
<span id="cb56-30"><a href="#cb56-30" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="fl">10</span>;</span>
<span id="cb56-31"><a href="#cb56-31" aria-hidden="true" tabindex="-1"></a>x_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(n, m);</span>
<span id="cb56-32"><a href="#cb56-32" aria-hidden="true" tabindex="-1"></a>x_gpu_small <span class="op">=</span> CUDA.<span class="fu">randn</span>(<span class="fl">5</span>, m);</span>
<span id="cb56-33"><a href="#cb56-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-34"><a href="#cb56-34" aria-hidden="true" tabindex="-1"></a>nthreads <span class="op">=</span> <span class="fl">1024</span>;</span>
<span id="cb56-35"><a href="#cb56-35" aria-hidden="true" tabindex="-1"></a>nblocks <span class="op">=</span> <span class="fl">100</span>;  <span class="co"># This is arbitrary in this example as we are not doing an actual computation.</span></span>
<span id="cb56-36"><a href="#cb56-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-37"><a href="#cb56-37" aria-hidden="true" tabindex="-1"></a>memsize <span class="op">=</span> nthreads <span class="op">*</span> m <span class="op">*</span> <span class="fl">4</span>;</span>
<span id="cb56-38"><a href="#cb56-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-39"><a href="#cb56-39" aria-hidden="true" tabindex="-1"></a>CUDA.<span class="pp">@sync</span> <span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks shmem<span class="op">=</span>memsize <span class="fu">kernel_reader_bycol</span>(x_gpu_small);</span>
<span id="cb56-40"><a href="#cb56-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-41"><a href="#cb56-41" aria-hidden="true" tabindex="-1"></a>CUDA.<span class="pp">@time</span> <span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks shmem<span class="op">=</span>memsize <span class="fu">kernel_reader_bycol</span>(x_gpu);</span>
<span id="cb56-42"><a href="#cb56-42" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.138480 seconds (24 CPU allocations: 752 bytes)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If <code>m</code> gets much bigger, we get an error “ERROR: Amount of dynamic shared memory exceeds device limit (400.000 KiB &gt; 48.000 KiB).” So for larger <code>m</code> we’d need to rework how we manipulate the data.</p>
<p>Let’s close by seeing if the memory access patterns make a difference in this example. Instead of accessing by column, we’ll access by row, but with the matrix transposed so it is very wide instead of very long.</p>
<p>My initial thought was that accessing by row would be slower because adjacent threads are not reading from adjacent locations in global memory.</p>
<div id="153fbe2c" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">kernel_reader_byrow</span>(x<span class="op">::</span><span class="dt">CuDeviceArray{T}</span>) <span class="kw">where</span> T</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>  i <span class="op">=</span> <span class="fu">threadIdx</span>().x;  <span class="co"># What thread am I within the block?</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>  j <span class="op">=</span> <span class="fu">blockIdx</span>().x;   <span class="co"># What block am I in?</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>  idx <span class="op">=</span>  (j<span class="op">-</span><span class="fl">1</span>)<span class="fu">*blockDim</span>().x <span class="op">+</span> i;</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>  dims <span class="op">=</span> <span class="fu">size</span>(x);</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Setup shared memory for the subset of data.</span></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>  shared_data <span class="op">=</span> <span class="fu">CuDynamicSharedArray</span>(T, (dims[<span class="fl">1</span>], <span class="fu">blockDim</span>().x));</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> chunk_start <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fu">blockDim</span>().x<span class="op">:</span>dims[<span class="fl">2</span>]</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>    chunk_size <span class="op">=</span> <span class="fu">min</span>(<span class="fu">blockDim</span>().x, dims[<span class="fl">2</span>] <span class="op">-</span> chunk_start <span class="op">+</span> <span class="fl">1</span>);</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">## Transfer a chunk of rows in parallel, one column per thread.</span></span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;=</span> chunk_size</span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> row <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span>dims[<span class="fl">1</span>]</span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a>        shared_data[row, i] <span class="op">=</span> x[row, chunk_start <span class="op">+</span> i <span class="op">-</span> <span class="fl">1</span>];</span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a>      <span class="cf">end</span></span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sync_threads</span>()</span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># At this point we'd insert code to do the actual computation, based on `idx`.</span></span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Each thread now has the opportunity to compute on all the data in the chunk in</span></span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># `shared_data`.</span></span>
<span id="cb57-23"><a href="#cb57-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb57-24"><a href="#cb57-24" aria-hidden="true" tabindex="-1"></a>  <span class="cf">end</span></span>
<span id="cb57-25"><a href="#cb57-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-26"><a href="#cb57-26" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span></span>
<span id="cb57-27"><a href="#cb57-27" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb57-28"><a href="#cb57-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-29"><a href="#cb57-29" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">10000000</span>;</span>
<span id="cb57-30"><a href="#cb57-30" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="fl">10</span>;</span>
<span id="cb57-31"><a href="#cb57-31" aria-hidden="true" tabindex="-1"></a>x_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(m, n);</span>
<span id="cb57-32"><a href="#cb57-32" aria-hidden="true" tabindex="-1"></a>x_gpu_small <span class="op">=</span> CUDA.<span class="fu">randn</span>(m, <span class="fl">5</span>);</span>
<span id="cb57-33"><a href="#cb57-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-34"><a href="#cb57-34" aria-hidden="true" tabindex="-1"></a>nthreads <span class="op">=</span> <span class="fl">1024</span>;</span>
<span id="cb57-35"><a href="#cb57-35" aria-hidden="true" tabindex="-1"></a>nblocks <span class="op">=</span> <span class="fl">100</span>;  <span class="co"># This is arbitrary in this example as we are not doing an actual computation.</span></span>
<span id="cb57-36"><a href="#cb57-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-37"><a href="#cb57-37" aria-hidden="true" tabindex="-1"></a>memsize <span class="op">=</span> nthreads <span class="op">*</span> m <span class="op">*</span> <span class="fl">4</span>;</span>
<span id="cb57-38"><a href="#cb57-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-39"><a href="#cb57-39" aria-hidden="true" tabindex="-1"></a>CUDA.<span class="pp">@sync</span> <span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks shmem<span class="op">=</span>memsize <span class="fu">kernel_reader_byrow</span>(x_gpu);</span>
<span id="cb57-40"><a href="#cb57-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-41"><a href="#cb57-41" aria-hidden="true" tabindex="-1"></a>CUDA.<span class="pp">@time</span> CUDA.<span class="pp">@sync</span> <span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks shmem<span class="op">=</span>memsize <span class="fu">kernel_reader_byrow</span>(x_gpu);</span>
<span id="cb57-42"><a href="#cb57-42" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.105434 seconds (25 CPU allocations: 1008 bytes)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We see that the access by row here is (a bit) faster. I think this is because the entire chunk of data in the wide matrix lives in a small area of global memory, while in the long matrix, each column in the chunk has adjacent values but separate columns are very far apart because the matrix is so long.</p>
<p>We might be able to improve efficiency with the wide matrix by operating by column within the wide matrix. This would involve more work to manage the indexing because we wouldn’t just have each thread manage a column (unless we used very few threads, which would presumably reduce efficiency).</p>
</section>
</section>
<section id="using-atomics-for-reduction-operations" class="level3">
<h3 class="anchored" data-anchor-id="using-atomics-for-reduction-operations">6.3 Using atomics for reduction operations</h3>
<p>One thing we haven’t seen so far is being able to have different threads write to the same memory location (e.g., to a scalar or to an element of an array). One can easily imagine needing to do this to carry out reduction operations (e.g., calculating a sum or a max or min).</p>
<p>The obvious danger is that two threads might write to the memory location at the same time and somehow cause the location not to be properly updated.</p>
<p>Suppose we want to calculate the log-likelihood (or some other loss function) across independent observations. We’d like to do the summation on the GPU to avoid passing all the log-likelihood values from GPU to CPU and then having to do the sum on the CPU.</p>
<div id="3a572b2b" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">BenchmarkTools</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Distributions</span></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">100_000_000</span>;   <span class="co"># Formatted for readability.</span></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>norm_dist <span class="op">=</span> <span class="fu">Normal</span>(<span class="fl">0</span>,<span class="fl">1</span>)</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> <span class="fu">rand</span>(norm_dist, n);</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">loglik_kernel</span>(x, result)</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>  i <span class="op">=</span> <span class="fu">threadIdx</span>().x;  <span class="co"># What thread am I within the block?</span></span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>  j <span class="op">=</span> <span class="fu">blockIdx</span>().x;   <span class="co"># What block am I in?</span></span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>  idx <span class="op">=</span>  (j<span class="op">-</span><span class="fl">1</span>)<span class="fu">*blockDim</span>().x <span class="op">+</span> i;</span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> idx <span class="op">&lt;=</span> <span class="fu">length</span>(x)</span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># logpdf(norm_dist, x[idx]) # Doesn't compile.</span></span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true" tabindex="-1"></a>    CUDA.<span class="pp">@atomic</span> result[<span class="fl">1</span>] <span class="op">+=</span> <span class="op">-</span><span class="fl">0.5</span><span class="op">*</span>x[idx]<span class="op">^</span><span class="fl">2</span>;            <span class="co"># Experimental, but nicer interface.</span></span>
<span id="cb58-15"><a href="#cb58-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">#CUDA.atomic_add!(pointer(result), -0.5*x[idx]^2);  # Stable low-level API.</span></span>
<span id="cb58-16"><a href="#cb58-16" aria-hidden="true" tabindex="-1"></a>  <span class="cf">end</span></span>
<span id="cb58-17"><a href="#cb58-17" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span></span>
<span id="cb58-18"><a href="#cb58-18" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb58-19"><a href="#cb58-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-20"><a href="#cb58-20" aria-hidden="true" tabindex="-1"></a>nthreads <span class="op">=</span> <span class="fl">1024</span>;</span>
<span id="cb58-21"><a href="#cb58-21" aria-hidden="true" tabindex="-1"></a>nblocks <span class="op">=</span> <span class="fu">Int</span>(<span class="fu">ceil</span>(n<span class="op">/</span>nthreads));</span>
<span id="cb58-22"><a href="#cb58-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-23"><a href="#cb58-23" aria-hidden="true" tabindex="-1"></a>samples_gpu <span class="op">=</span> <span class="fu">CuArray</span>(samples);</span>
<span id="cb58-24"><a href="#cb58-24" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> CUDA.<span class="fu">zeros</span>(<span class="fu">typeof</span>(samples[<span class="fl">1</span>]), <span class="fl">1</span>);</span>
<span id="cb58-25"><a href="#cb58-25" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> CUDA.<span class="pp">@sync</span> <span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks <span class="fu">loglik_kernel</span>(samples_gpu, result);</span>
<span id="cb58-26"><a href="#cb58-26" aria-hidden="true" tabindex="-1"></a><span class="co"># 161.352 ms (34 allocations: 944 bytes)</span></span>
<span id="cb58-27"><a href="#cb58-27" aria-hidden="true" tabindex="-1"></a><span class="fu">Array</span>(result)[<span class="fl">1</span>] <span class="fu">-n*log</span>(<span class="fl">2</span><span class="op">*</span><span class="cn">pi</span>)<span class="op">/</span><span class="fl">2</span>  <span class="co"># Adjust for normalizing constant as scalar computation, not on GPU.</span></span>
<span id="cb58-28"><a href="#cb58-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-29"><a href="#cb58-29" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> <span class="fu">sum</span>(<span class="fu">logpdf</span>.(norm_dist, samples))</span>
<span id="cb58-30"><a href="#cb58-30" aria-hidden="true" tabindex="-1"></a><span class="co"># 1.410 s (5 allocations: 762.94 MiB)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>So we got about a 12-fold speedup, which is less than we’ve been getting for some of our other comparisons.</p>
<p>I was curious how much time is spent handling the reduction operation (presumably there is some loss in efficiency from having all the threads write to the same memory location). When I changed <code>result</code> to be a vector of length equal to that of <code>samples</code> and just assign the individual PDF evaluations to the corresponding elements of <code>result</code> without the atomic operation, the time was 3 milliseconds (compared to 161 above), so there is a performance degradation from the atomic operation.</p>
<section id="using-shared-memory-to-reduce-the-cost-of-atomic-operations" class="level4">
<h4 class="anchored" data-anchor-id="using-shared-memory-to-reduce-the-cost-of-atomic-operations">6.3.1 Using shared memory to reduce the cost of atomic operations</h4>
<p>One solution to the performance degradation is to not have all of the summing make use of the same location in memory to accumulate the result.</p>
<p>Instead we can use shared memory to more efficiently do the reduction within each thread block before doing the final reduction across blocks. Here’s an approach using a tree-like operation (as suggested by a ChatBot, but requiring some debugging on my part) to compute the partial sum within each thread block before using the atomic operation to compute the sum of the partial sums:</p>
<div id="cd57f362" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">loglik_kernel_shmem</span>(x<span class="op">::</span><span class="dt">CuDeviceArray{T}</span>, result<span class="op">::</span><span class="dt">CuDeviceArray{T}</span>) <span class="kw">where</span> T</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>  i <span class="op">=</span> <span class="fu">threadIdx</span>().x;  <span class="co"># What thread am I within the block?</span></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>  j <span class="op">=</span> <span class="fu">blockIdx</span>().x;   <span class="co"># What block am I in?</span></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>  idx <span class="op">=</span>  (j<span class="op">-</span><span class="fl">1</span>)<span class="fu">*blockDim</span>().x <span class="op">+</span> i;</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>  shared_data <span class="op">=</span> <span class="fu">CuDynamicSharedArray</span>(T, (<span class="fu">blockDim</span>().x));</span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># First do the core calculation and store in shared memory.</span></span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> idx <span class="op">&lt;=</span> <span class="fu">length</span>(x)</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>     shared_data[i] <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span><span class="op">*</span>x[idx]<span class="op">^</span><span class="fl">2</span>;</span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span></span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>     shared_data[i] <span class="op">=</span> <span class="fl">0.0</span>; </span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">end</span></span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Tree-like partial sum within the thread block,</span></span>
<span id="cb59-15"><a href="#cb59-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># summing pairs until the sum within the block</span></span>
<span id="cb59-16"><a href="#cb59-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># is contained in `shared_data[1]`.</span></span>
<span id="cb59-17"><a href="#cb59-17" aria-hidden="true" tabindex="-1"></a>  s <span class="op">=</span> <span class="fu">blockDim</span>().x <span class="op">÷</span> <span class="fl">2</span>;   <span class="co"># `÷` ensures `s` is Int.</span></span>
<span id="cb59-18"><a href="#cb59-18" aria-hidden="true" tabindex="-1"></a>  <span class="cf">while</span> s <span class="op">&gt;=</span> <span class="fl">1</span></span>
<span id="cb59-19"><a href="#cb59-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;=</span> s</span>
<span id="cb59-20"><a href="#cb59-20" aria-hidden="true" tabindex="-1"></a>      shared_data[i] <span class="op">+=</span> shared_data[i <span class="op">+</span> s];</span>
<span id="cb59-21"><a href="#cb59-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb59-22"><a href="#cb59-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sync_threads</span>()</span>
<span id="cb59-23"><a href="#cb59-23" aria-hidden="true" tabindex="-1"></a>    s <span class="op">÷=</span> <span class="fl">2</span>;</span>
<span id="cb59-24"><a href="#cb59-24" aria-hidden="true" tabindex="-1"></a>  <span class="cf">end</span></span>
<span id="cb59-25"><a href="#cb59-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb59-26"><a href="#cb59-26" aria-hidden="true" tabindex="-1"></a>  <span class="co"># The first thread in the block writes the partial sum to global memory.</span></span>
<span id="cb59-27"><a href="#cb59-27" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> i <span class="op">==</span> <span class="fl">1</span></span>
<span id="cb59-28"><a href="#cb59-28" aria-hidden="true" tabindex="-1"></a>    CUDA.<span class="pp">@atomic</span> result[<span class="fl">1</span>] <span class="op">+=</span> shared_data[<span class="fl">1</span>];</span>
<span id="cb59-29"><a href="#cb59-29" aria-hidden="true" tabindex="-1"></a>  <span class="cf">end</span></span>
<span id="cb59-30"><a href="#cb59-30" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span></span>
<span id="cb59-31"><a href="#cb59-31" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb59-32"><a href="#cb59-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-33"><a href="#cb59-33" aria-hidden="true" tabindex="-1"></a>memsize <span class="op">=</span> nthreads <span class="op">*</span> <span class="fu">sizeof</span>(samples[<span class="fl">1</span>]);</span>
<span id="cb59-34"><a href="#cb59-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-35"><a href="#cb59-35" aria-hidden="true" tabindex="-1"></a>result2 <span class="op">=</span> CUDA.<span class="fu">zeros</span>(<span class="fu">typeof</span>(samples[<span class="fl">1</span>]), <span class="fl">1</span>);</span>
<span id="cb59-36"><a href="#cb59-36" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> CUDA.<span class="pp">@sync</span> <span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks shmem<span class="op">=</span>memsize <span class="fu">loglik_kernel_shmem</span>(samples_gpu, result2);</span>
<span id="cb59-37"><a href="#cb59-37" aria-hidden="true" tabindex="-1"></a><span class="co"># 6.317 ms (34 allocations: 944 bytes)</span></span>
<span id="cb59-38"><a href="#cb59-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-39"><a href="#cb59-39" aria-hidden="true" tabindex="-1"></a><span class="fu">Array</span>(result2)[<span class="fl">1</span>] <span class="fu">-n*log</span>(<span class="fl">2</span><span class="op">*</span><span class="cn">pi</span>)<span class="op">/</span><span class="fl">2</span> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="debugging-kernel-code" class="level3">
<h3 class="anchored" data-anchor-id="debugging-kernel-code">6.4 Debugging kernel code</h3>
<p>It can be much harder to debug kernel code than regular Julia code. If the syntax doesn’t produce valid compiled code that can run on the GPU, it may not be obvious from the error messsage what the problem is.</p>
<p>As an example in the code in the previous section, I originally had <code>s = blockDim().x / 2;</code> and <code>s /= 2;</code>. I didn’t realize that even with integer inputs, that this produced a float output type for <code>s</code> and that as a result using <code>s</code> for indexing in <code>shared_data[i + s];</code> wouldn’t work. The error message said there was a problem with the LLVM/IR code produced from the kernel, but didn’t say where and it took a binary search on my part to figure out that <code>shared_data[i + s];</code> was the problematic piece of code and that was caused by <code>s</code> being a float.</p>
<p>On an only somewhat related point, the ChatBot originally gave me while <code>s &gt;= 0</code>, which is a bug that doesn’t prevent the code from running, but does give incorrect numerical results, so we still need to be careful with what we get from ChatBots.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/computing\.stat\.berkeley\.edu\/tutorial-parallelization");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>